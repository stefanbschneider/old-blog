{
  
    
        "post0": {
            "title": "Dealing with Partial Observability In Reinforcement Learning",
            "content": ". Important: This blog post is still work in progress. In reinforcement learning (RL), the RL agent typically selects a suitable action based on the last observation. In many practical environments, the full state can only be observed partially, such that important information may be missing when just considering the last observation. This blog post covers options for dealing with missing and only partially observed state, e.g., considering a sequence of last observations and applying self-attention to this sequence. . Note: This blog post is based on and very related to Anyscale&#8217;s blog post on attention nets with RLlib. In comparison, I focus less on RLlib&#8217;s trajectory API and more on providing a practical, end-to-end tutorial. . Example: The CartPole Gym Environment . As an example, consider the popular OpenAI Gym CartPole environment. Here, the task is to move a cart left or right in order to balance a pole on the cart as long as possible. . . In the normal CartPole-v1 environment, the RL agent observes four scalar values (defined here): . The cart position, i.e., where the cart currently is. | The cart velocity, i.e., how fast the cart is currently moving and in which direction (can be positive or negative). | The pole angle, i.e., how tilted the pole currently is and in which direction. | The pole angular velocity, i.e., how fast the pole is currently moving and in which direction. | . All four observations are important to decide whether the cart should move left or right. . Now, assume the RL agent only has access to an instant snapshot of the cart and the pole (e.g., through a photo/raw pixels) and can neither observe cart velocity nor pole angular velocity. In this case, the RL agent only has partial observations and does not know whether and how fast the pole is currently swinging. As a result, standard RL agents cannot solve the problem and do not learn to balance the pole. How to deal with this problem of partial observations, i.e., missing state (here, cart and pole velocity)? . Options for Dealing With Partial Observations . There are different options for dealing with partial observations/missing state, e.g., missing velocity in the CartPole example: . Add the missing state explicitly, e.g., measure and observe velocity. Note that this may require installing extra sensors or may even be infeasible in some scenarios. | Ignore the missing state, i.e., just rely on the available, partial observations. Depending on the missing state, this may be problematic and keep the agent from learning. | Keep track of a sequence of the last observations. By observing the cart position and pole angle over time, the agent can implicitly derive their velocity. There are different ways to deal with this sequence: Just use the sequence as is for a standard multi-layer perceptron (MLP)/dense feedforward neural network. | Feed the sequence into a recurrent neural network (RNN), e.g., with long short-term memory (LSTM). | Feed the sequence into a neural network with self-attention. | | In the following, I go through each option in more detail and illustrate them using simple example code. . Setup . For the examples, I use a PPO RL agent from Ray RLlib with the CartPole environment, described above. . To install these dependencies, run the following code (tested with Python 3.8 on Windows): . !pip install ray[rllib]==1.8.0 !pip install tensorflow==2.7.0 !pip install seaborn==0.11.2 !pip install gym==0.21.0 !pip install pyglet==1.5.21 . Requirement already satisfied: ray[rllib]==1.8.0 in c: users stefan git-repos private blog venv lib site-packages (1.8.0) Requirement already satisfied: grpcio&gt;=1.28.1 in c: users stefan git-repos private blog venv lib site-packages (from ray[rllib]==1.8.0) (1.41.1) Requirement already satisfied: click&gt;=7.0 in c: users stefan git-repos private blog venv lib site-packages (from ray[rllib]==1.8.0) (8.0.3) Requirement already satisfied: protobuf&gt;=3.15.3 in c: users stefan git-repos private blog venv lib site-packages (from ray[rllib]==1.8.0) (3.19.1) Requirement already satisfied: redis&gt;=3.5.0 in c: users stefan git-repos private blog venv lib site-packages (from ray[rllib]==1.8.0) (4.0.0) Requirement already satisfied: jsonschema in c: users stefan git-repos private blog venv lib site-packages (from ray[rllib]==1.8.0) (3.2.0) Requirement already satisfied: msgpack&lt;2.0.0,&gt;=1.0.0 in c: users stefan git-repos private blog venv lib site-packages (from ray[rllib]==1.8.0) (1.0.2) Requirement already satisfied: pyyaml in c: users stefan git-repos private blog venv lib site-packages (from ray[rllib]==1.8.0) (6.0) Requirement already satisfied: numpy&gt;=1.16 in c: users stefan git-repos private blog venv lib site-packages (from ray[rllib]==1.8.0) (1.21.4) Requirement already satisfied: filelock in c: users stefan git-repos private blog venv lib site-packages (from ray[rllib]==1.8.0) (3.4.0) Requirement already satisfied: attrs in c: users stefan git-repos private blog venv lib site-packages (from ray[rllib]==1.8.0) (20.3.0) Requirement already satisfied: requests in c: users stefan git-repos private blog venv lib site-packages (from ray[rllib]==1.8.0) (2.25.1) Requirement already satisfied: matplotlib!=3.4.3 in c: users stefan git-repos private blog venv lib site-packages (from ray[rllib]==1.8.0) (3.5.0) Requirement already satisfied: tabulate in c: users stefan git-repos private blog venv lib site-packages (from ray[rllib]==1.8.0) (0.8.9) Requirement already satisfied: scikit-image in c: users stefan git-repos private blog venv lib site-packages (from ray[rllib]==1.8.0) (0.18.3) Requirement already satisfied: gym in c: users stefan git-repos private blog venv lib site-packages (from ray[rllib]==1.8.0) (0.21.0) Requirement already satisfied: scipy in c: users stefan git-repos private blog venv lib site-packages (from ray[rllib]==1.8.0) (1.7.2) Requirement already satisfied: tensorboardX&gt;=1.9 in c: users stefan git-repos private blog venv lib site-packages (from ray[rllib]==1.8.0) (2.4) Requirement already satisfied: dm-tree in c: users stefan git-repos private blog venv lib site-packages (from ray[rllib]==1.8.0) (0.1.6) Requirement already satisfied: pandas in c: users stefan git-repos private blog venv lib site-packages (from ray[rllib]==1.8.0) (1.3.4) Requirement already satisfied: lz4 in c: users stefan git-repos private blog venv lib site-packages (from ray[rllib]==1.8.0) (3.1.3) Requirement already satisfied: colorama in c: users stefan git-repos private blog venv lib site-packages (from click&gt;=7.0-&gt;ray[rllib]==1.8.0) (0.4.4) Requirement already satisfied: six&gt;=1.5.2 in c: users stefan git-repos private blog venv lib site-packages (from grpcio&gt;=1.28.1-&gt;ray[rllib]==1.8.0) (1.15.0) Requirement already satisfied: setuptools-scm&gt;=4 in c: users stefan git-repos private blog venv lib site-packages (from matplotlib!=3.4.3-&gt;ray[rllib]==1.8.0) (6.3.2) Requirement already satisfied: python-dateutil&gt;=2.7 in c: users stefan git-repos private blog venv lib site-packages (from matplotlib!=3.4.3-&gt;ray[rllib]==1.8.0) (2.8.1) Requirement already satisfied: fonttools&gt;=4.22.0 in c: users stefan git-repos private blog venv lib site-packages (from matplotlib!=3.4.3-&gt;ray[rllib]==1.8.0) (4.28.1) Requirement already satisfied: packaging&gt;=20.0 in c: users stefan git-repos private blog venv lib site-packages (from matplotlib!=3.4.3-&gt;ray[rllib]==1.8.0) (20.8) Requirement already satisfied: pillow&gt;=6.2.0 in c: users stefan git-repos private blog venv lib site-packages (from matplotlib!=3.4.3-&gt;ray[rllib]==1.8.0) (8.4.0) Requirement already satisfied: cycler&gt;=0.10 in c: users stefan git-repos private blog venv lib site-packages (from matplotlib!=3.4.3-&gt;ray[rllib]==1.8.0) (0.11.0) Requirement already satisfied: pyparsing&gt;=2.2.1 in c: users stefan git-repos private blog venv lib site-packages (from matplotlib!=3.4.3-&gt;ray[rllib]==1.8.0) (2.4.7) Requirement already satisfied: kiwisolver&gt;=1.0.1 in c: users stefan git-repos private blog venv lib site-packages (from matplotlib!=3.4.3-&gt;ray[rllib]==1.8.0) (1.3.2) Requirement already satisfied: deprecated in c: users stefan git-repos private blog venv lib site-packages (from redis&gt;=3.5.0-&gt;ray[rllib]==1.8.0) (1.2.13) Requirement already satisfied: cloudpickle&gt;=1.2.0 in c: users stefan git-repos private blog venv lib site-packages (from gym-&gt;ray[rllib]==1.8.0) (2.0.0) Requirement already satisfied: pyrsistent&gt;=0.14.0 in c: users stefan git-repos private blog venv lib site-packages (from jsonschema-&gt;ray[rllib]==1.8.0) (0.17.3) Requirement already satisfied: setuptools in c: users stefan git-repos private blog venv lib site-packages (from jsonschema-&gt;ray[rllib]==1.8.0) (46.1.3) Requirement already satisfied: pytz&gt;=2017.3 in c: users stefan git-repos private blog venv lib site-packages (from pandas-&gt;ray[rllib]==1.8.0) (2020.5) Requirement already satisfied: certifi&gt;=2017.4.17 in c: users stefan git-repos private blog venv lib site-packages (from requests-&gt;ray[rllib]==1.8.0) (2020.12.5) Requirement already satisfied: chardet&lt;5,&gt;=3.0.2 in c: users stefan git-repos private blog venv lib site-packages (from requests-&gt;ray[rllib]==1.8.0) (4.0.0) Requirement already satisfied: idna&lt;3,&gt;=2.5 in c: users stefan git-repos private blog venv lib site-packages (from requests-&gt;ray[rllib]==1.8.0) (2.10) Requirement already satisfied: urllib3&lt;1.27,&gt;=1.21.1 in c: users stefan git-repos private blog venv lib site-packages (from requests-&gt;ray[rllib]==1.8.0) (1.26.2) Requirement already satisfied: imageio&gt;=2.3.0 in c: users stefan git-repos private blog venv lib site-packages (from scikit-image-&gt;ray[rllib]==1.8.0) (2.10.4) Requirement already satisfied: networkx&gt;=2.0 in c: users stefan git-repos private blog venv lib site-packages (from scikit-image-&gt;ray[rllib]==1.8.0) (2.6.3) Requirement already satisfied: PyWavelets&gt;=1.1.1 in c: users stefan git-repos private blog venv lib site-packages (from scikit-image-&gt;ray[rllib]==1.8.0) (1.2.0) Requirement already satisfied: tifffile&gt;=2019.7.26 in c: users stefan git-repos private blog venv lib site-packages (from scikit-image-&gt;ray[rllib]==1.8.0) (2021.11.2) Requirement already satisfied: tomli&gt;=1.0.0 in c: users stefan git-repos private blog venv lib site-packages (from setuptools-scm&gt;=4-&gt;matplotlib!=3.4.3-&gt;ray[rllib]==1.8.0) (1.2.2) Requirement already satisfied: wrapt&lt;2,&gt;=1.10 in c: users stefan git-repos private blog venv lib site-packages (from deprecated-&gt;redis&gt;=3.5.0-&gt;ray[rllib]==1.8.0) (1.13.3) Requirement already satisfied: tensorflow==2.7.0 in c: users stefan git-repos private blog venv lib site-packages (2.7.0) Requirement already satisfied: gast&lt;0.5.0,&gt;=0.2.1 in c: users stefan git-repos private blog venv lib site-packages (from tensorflow==2.7.0) (0.4.0) Requirement already satisfied: wheel&lt;1.0,&gt;=0.32.0 in c: users stefan git-repos private blog venv lib site-packages (from tensorflow==2.7.0) (0.34.2) Requirement already satisfied: tensorflow-io-gcs-filesystem&gt;=0.21.0 in c: users stefan git-repos private blog venv lib site-packages (from tensorflow==2.7.0) (0.22.0) Requirement already satisfied: grpcio&lt;2.0,&gt;=1.24.3 in c: users stefan git-repos private blog venv lib site-packages (from tensorflow==2.7.0) (1.41.1) Requirement already satisfied: numpy&gt;=1.14.5 in c: users stefan git-repos private blog venv lib site-packages (from tensorflow==2.7.0) (1.21.4) Requirement already satisfied: wrapt&gt;=1.11.0 in c: users stefan git-repos private blog venv lib site-packages (from tensorflow==2.7.0) (1.13.3) Requirement already satisfied: h5py&gt;=2.9.0 in c: users stefan git-repos private blog venv lib site-packages (from tensorflow==2.7.0) (3.6.0) Requirement already satisfied: opt-einsum&gt;=2.3.2 in c: users stefan git-repos private blog venv lib site-packages (from tensorflow==2.7.0) (3.3.0) Requirement already satisfied: tensorflow-estimator&lt;2.8,~=2.7.0rc0 in c: users stefan git-repos private blog venv lib site-packages (from tensorflow==2.7.0) (2.7.0) Requirement already satisfied: keras&lt;2.8,&gt;=2.7.0rc0 in c: users stefan git-repos private blog venv lib site-packages (from tensorflow==2.7.0) (2.7.0) Requirement already satisfied: libclang&gt;=9.0.1 in c: users stefan git-repos private blog venv lib site-packages (from tensorflow==2.7.0) (12.0.0) Requirement already satisfied: keras-preprocessing&gt;=1.1.1 in c: users stefan git-repos private blog venv lib site-packages (from tensorflow==2.7.0) (1.1.2) Requirement already satisfied: absl-py&gt;=0.4.0 in c: users stefan git-repos private blog venv lib site-packages (from tensorflow==2.7.0) (1.0.0) Requirement already satisfied: termcolor&gt;=1.1.0 in c: users stefan git-repos private blog venv lib site-packages (from tensorflow==2.7.0) (1.1.0) Requirement already satisfied: tensorboard~=2.6 in c: users stefan git-repos private blog venv lib site-packages (from tensorflow==2.7.0) (2.7.0) Requirement already satisfied: astunparse&gt;=1.6.0 in c: users stefan git-repos private blog venv lib site-packages (from tensorflow==2.7.0) (1.6.3) Requirement already satisfied: six&gt;=1.12.0 in c: users stefan git-repos private blog venv lib site-packages (from tensorflow==2.7.0) (1.15.0) Requirement already satisfied: google-pasta&gt;=0.1.1 in c: users stefan git-repos private blog venv lib site-packages (from tensorflow==2.7.0) (0.2.0) Requirement already satisfied: flatbuffers&lt;3.0,&gt;=1.12 in c: users stefan git-repos private blog venv lib site-packages (from tensorflow==2.7.0) (2.0) Requirement already satisfied: protobuf&gt;=3.9.2 in c: users stefan git-repos private blog venv lib site-packages (from tensorflow==2.7.0) (3.19.1) Requirement already satisfied: typing-extensions&gt;=3.6.6 in c: users stefan git-repos private blog venv lib site-packages (from tensorflow==2.7.0) (4.0.0) Requirement already satisfied: google-auth-oauthlib&lt;0.5,&gt;=0.4.1 in c: users stefan git-repos private blog venv lib site-packages (from tensorboard~=2.6-&gt;tensorflow==2.7.0) (0.4.6) Requirement already satisfied: markdown&gt;=2.6.8 in c: users stefan git-repos private blog venv lib site-packages (from tensorboard~=2.6-&gt;tensorflow==2.7.0) (3.3.5) Requirement already satisfied: requests&lt;3,&gt;=2.21.0 in c: users stefan git-repos private blog venv lib site-packages (from tensorboard~=2.6-&gt;tensorflow==2.7.0) (2.25.1) Requirement already satisfied: tensorboard-data-server&lt;0.7.0,&gt;=0.6.0 in c: users stefan git-repos private blog venv lib site-packages (from tensorboard~=2.6-&gt;tensorflow==2.7.0) (0.6.1) Requirement already satisfied: google-auth&lt;3,&gt;=1.6.3 in c: users stefan git-repos private blog venv lib site-packages (from tensorboard~=2.6-&gt;tensorflow==2.7.0) (2.3.3) Requirement already satisfied: tensorboard-plugin-wit&gt;=1.6.0 in c: users stefan git-repos private blog venv lib site-packages (from tensorboard~=2.6-&gt;tensorflow==2.7.0) (1.8.0) Requirement already satisfied: werkzeug&gt;=0.11.15 in c: users stefan git-repos private blog venv lib site-packages (from tensorboard~=2.6-&gt;tensorflow==2.7.0) (2.0.2) Requirement already satisfied: setuptools&gt;=41.0.0 in c: users stefan git-repos private blog venv lib site-packages (from tensorboard~=2.6-&gt;tensorflow==2.7.0) (46.1.3) Requirement already satisfied: rsa&lt;5,&gt;=3.1.4 in c: users stefan git-repos private blog venv lib site-packages (from google-auth&lt;3,&gt;=1.6.3-&gt;tensorboard~=2.6-&gt;tensorflow==2.7.0) (4.7.2) Requirement already satisfied: pyasn1-modules&gt;=0.2.1 in c: users stefan git-repos private blog venv lib site-packages (from google-auth&lt;3,&gt;=1.6.3-&gt;tensorboard~=2.6-&gt;tensorflow==2.7.0) (0.2.8) Requirement already satisfied: cachetools&lt;5.0,&gt;=2.0.0 in c: users stefan git-repos private blog venv lib site-packages (from google-auth&lt;3,&gt;=1.6.3-&gt;tensorboard~=2.6-&gt;tensorflow==2.7.0) (4.2.4) Requirement already satisfied: requests-oauthlib&gt;=0.7.0 in c: users stefan git-repos private blog venv lib site-packages (from google-auth-oauthlib&lt;0.5,&gt;=0.4.1-&gt;tensorboard~=2.6-&gt;tensorflow==2.7.0) (1.3.0) Requirement already satisfied: importlib-metadata&gt;=&#39;4.4&#39; in c: users stefan git-repos private blog venv lib site-packages (from markdown&gt;=2.6.8-&gt;tensorboard~=2.6-&gt;tensorflow==2.7.0) (4.8.2) Requirement already satisfied: idna&lt;3,&gt;=2.5 in c: users stefan git-repos private blog venv lib site-packages (from requests&lt;3,&gt;=2.21.0-&gt;tensorboard~=2.6-&gt;tensorflow==2.7.0) (2.10) Requirement already satisfied: chardet&lt;5,&gt;=3.0.2 in c: users stefan git-repos private blog venv lib site-packages (from requests&lt;3,&gt;=2.21.0-&gt;tensorboard~=2.6-&gt;tensorflow==2.7.0) (4.0.0) Requirement already satisfied: certifi&gt;=2017.4.17 in c: users stefan git-repos private blog venv lib site-packages (from requests&lt;3,&gt;=2.21.0-&gt;tensorboard~=2.6-&gt;tensorflow==2.7.0) (2020.12.5) Requirement already satisfied: urllib3&lt;1.27,&gt;=1.21.1 in c: users stefan git-repos private blog venv lib site-packages (from requests&lt;3,&gt;=2.21.0-&gt;tensorboard~=2.6-&gt;tensorflow==2.7.0) (1.26.2) Requirement already satisfied: zipp&gt;=0.5 in c: users stefan git-repos private blog venv lib site-packages (from importlib-metadata&gt;=&#39;4.4&#39;-&gt;markdown&gt;=2.6.8-&gt;tensorboard~=2.6-&gt;tensorflow==2.7.0) (3.6.0) Requirement already satisfied: pyasn1&lt;0.5.0,&gt;=0.4.6 in c: users stefan git-repos private blog venv lib site-packages (from pyasn1-modules&gt;=0.2.1-&gt;google-auth&lt;3,&gt;=1.6.3-&gt;tensorboard~=2.6-&gt;tensorflow==2.7.0) (0.4.8) Requirement already satisfied: oauthlib&gt;=3.0.0 in c: users stefan git-repos private blog venv lib site-packages (from requests-oauthlib&gt;=0.7.0-&gt;google-auth-oauthlib&lt;0.5,&gt;=0.4.1-&gt;tensorboard~=2.6-&gt;tensorflow==2.7.0) (3.1.1) Requirement already satisfied: seaborn==0.11.2 in c: users stefan git-repos private blog venv lib site-packages (0.11.2) Requirement already satisfied: scipy&gt;=1.0 in c: users stefan git-repos private blog venv lib site-packages (from seaborn==0.11.2) (1.7.2) Requirement already satisfied: numpy&gt;=1.15 in c: users stefan git-repos private blog venv lib site-packages (from seaborn==0.11.2) (1.21.4) Requirement already satisfied: matplotlib&gt;=2.2 in c: users stefan git-repos private blog venv lib site-packages (from seaborn==0.11.2) (3.5.0) Requirement already satisfied: pandas&gt;=0.23 in c: users stefan git-repos private blog venv lib site-packages (from seaborn==0.11.2) (1.3.4) Requirement already satisfied: python-dateutil&gt;=2.7 in c: users stefan git-repos private blog venv lib site-packages (from matplotlib&gt;=2.2-&gt;seaborn==0.11.2) (2.8.1) Requirement already satisfied: kiwisolver&gt;=1.0.1 in c: users stefan git-repos private blog venv lib site-packages (from matplotlib&gt;=2.2-&gt;seaborn==0.11.2) (1.3.2) Requirement already satisfied: cycler&gt;=0.10 in c: users stefan git-repos private blog venv lib site-packages (from matplotlib&gt;=2.2-&gt;seaborn==0.11.2) (0.11.0) Requirement already satisfied: packaging&gt;=20.0 in c: users stefan git-repos private blog venv lib site-packages (from matplotlib&gt;=2.2-&gt;seaborn==0.11.2) (20.8) Requirement already satisfied: pillow&gt;=6.2.0 in c: users stefan git-repos private blog venv lib site-packages (from matplotlib&gt;=2.2-&gt;seaborn==0.11.2) (8.4.0) Requirement already satisfied: fonttools&gt;=4.22.0 in c: users stefan git-repos private blog venv lib site-packages (from matplotlib&gt;=2.2-&gt;seaborn==0.11.2) (4.28.1) Requirement already satisfied: pyparsing&gt;=2.2.1 in c: users stefan git-repos private blog venv lib site-packages (from matplotlib&gt;=2.2-&gt;seaborn==0.11.2) (2.4.7) Requirement already satisfied: setuptools-scm&gt;=4 in c: users stefan git-repos private blog venv lib site-packages (from matplotlib&gt;=2.2-&gt;seaborn==0.11.2) (6.3.2) Requirement already satisfied: pytz&gt;=2017.3 in c: users stefan git-repos private blog venv lib site-packages (from pandas&gt;=0.23-&gt;seaborn==0.11.2) (2020.5) Requirement already satisfied: six&gt;=1.5 in c: users stefan git-repos private blog venv lib site-packages (from python-dateutil&gt;=2.7-&gt;matplotlib&gt;=2.2-&gt;seaborn==0.11.2) (1.15.0) Requirement already satisfied: setuptools in c: users stefan git-repos private blog venv lib site-packages (from setuptools-scm&gt;=4-&gt;matplotlib&gt;=2.2-&gt;seaborn==0.11.2) (46.1.3) Requirement already satisfied: tomli&gt;=1.0.0 in c: users stefan git-repos private blog venv lib site-packages (from setuptools-scm&gt;=4-&gt;matplotlib&gt;=2.2-&gt;seaborn==0.11.2) (1.2.2) Requirement already satisfied: gym==0.21.0 in c: users stefan git-repos private blog venv lib site-packages (0.21.0) Requirement already satisfied: cloudpickle&gt;=1.2.0 in c: users stefan git-repos private blog venv lib site-packages (from gym==0.21.0) (2.0.0) Requirement already satisfied: numpy&gt;=1.18.0 in c: users stefan git-repos private blog venv lib site-packages (from gym==0.21.0) (1.21.4) Requirement already satisfied: pyglet==1.5.21 in c: users stefan git-repos private blog venv lib site-packages (1.5.21) . . Start up ray, load the default PPO config, and determine the number of training iterations, which is the same for all options (for comparability). . import ray from ray.rllib.agents import ppo # adjust num_cpus and num_gpus to your system # for some reason, num_cpus=2 gets stuck on my system (when trying to train) ray.init(num_cpus=3, ignore_reinit_error=True) # stop conditions based on training iterations (each with 4000 train steps) stop = {&quot;training_iteration&quot;: 10} . 2021-11-29 17:43:44,346 INFO worker.py:832 -- Calling ray.init() again after it has already been called. . Option 1: Explicitly Add Missing State . Sometimes, it is possible to extend the observations and explicitly add important state that was previously unobserved. In the CartPole example, the cart and pole velocity can simply be &quot;added&quot; by using the default CartPole-v1 environment. Here, the cart velocity and pole velocity are already included in the observations. . Note that in many practical scenarios such &quot;missing&quot; state cannot be added and observed simply. Instead, it may require installing additional sensors or may even be completely infeasible. . Let&#39;s start with the best case, i.e., explicitly including the missing state. . import gym # the default CartPole env has all 4 observations: position and velocity of both cart and pole env = gym.make(&quot;CartPole-v1&quot;) env.observation_space.shape . (4,) . # run PPO on the default CartPole-v1 env config1 = ppo.DEFAULT_CONFIG.copy() config1[&quot;env&quot;] = &quot;CartPole-v1&quot; # training takes a while results1 = ray.tune.run(&quot;PPO&quot;, config=config1, stop=stop) print(&quot;Option 1: Training finished successfully&quot;) . == Status ==Current time: 2021-11-23 19:46:59 (running for 00:00:00.16)Memory usage on this node: 9.3/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 0/3 CPUs, 0/0 GPUs, 0.0/1.4 GiB heap, 0.0/0.7 GiB objectsResult logdir: C: Users Stefan ray_results PPONumber of trials: 1/1 (1 PENDING) Trial name status loc . PPO_CartPole-v1_bd6e2_00000 | PENDING | | . (pid=None) c: users stefan git-repos private blog venv lib site-packages redis connection.py:77: UserWarning: redis-py works best with hiredis. Please consider installing (pid=None) warnings.warn(msg) . == Status ==Current time: 2021-11-23 19:47:04 (running for 00:00:05.17)Memory usage on this node: 9.3/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 0/3 CPUs, 0/0 GPUs, 0.0/1.4 GiB heap, 0.0/0.7 GiB objectsResult logdir: C: Users Stefan ray_results PPONumber of trials: 1/1 (1 PENDING) Trial name status loc . PPO_CartPole-v1_bd6e2_00000 | PENDING | | . (pid=9332) 2021-11-23 19:47:10,868 INFO trainer.py:753 -- Tip: set framework=tfe or the --eager flag to enable TensorFlow eager execution (pid=9332) 2021-11-23 19:47:10,868 INFO ppo.py:166 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn&#39;t work for you. (pid=9332) 2021-11-23 19:47:10,868 INFO trainer.py:770 -- Current log_level is WARN. For more information, set &#39;log_level&#39;: &#39;INFO&#39; / &#39;DEBUG&#39; or use the -v and -vv flags. (pid=None) c: users stefan git-repos private blog venv lib site-packages redis connection.py:77: UserWarning: redis-py works best with hiredis. Please consider installing (pid=None) warnings.warn(msg) (pid=None) c: users stefan git-repos private blog venv lib site-packages redis connection.py:77: UserWarning: redis-py works best with hiredis. Please consider installing (pid=None) warnings.warn(msg) (pid=6508) 2021-11-23 19:47:22,968 WARNING deprecation.py:38 -- DeprecationWarning: `SampleBatch[&#39;is_training&#39;]` has been deprecated. Use `SampleBatch.is_training` instead. This will raise an error in the future! (pid=9332) 2021-11-23 19:47:24,418 WARNING deprecation.py:38 -- DeprecationWarning: `SampleBatch[&#39;is_training&#39;]` has been deprecated. Use `SampleBatch.is_training` instead. This will raise an error in the future! . == Status ==Current time: 2021-11-23 19:47:25 (running for 00:00:26.02)Memory usage on this node: 10.0/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.4 GiB heap, 0.0/0.7 GiB objectsResult logdir: C: Users Stefan ray_results PPONumber of trials: 1/1 (1 RUNNING) Trial name status loc . PPO_CartPole-v1_bd6e2_00000 | RUNNING | 127.0.0.1:9332 | . (pid=9332) 2021-11-23 19:47:25,401 WARNING trainer_template.py:185 -- `execution_plan` functions should accept `trainer`, `workers`, and `config` as args! (pid=9332) 2021-11-23 19:47:25,401 INFO trainable.py:110 -- Trainable.setup took 14.534 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads. (pid=9332) 2021-11-23 19:47:25,401 WARNING util.py:57 -- Install gputil for GPU system monitoring. . == Status ==Current time: 2021-11-23 19:47:26 (running for 00:00:27.03)Memory usage on this node: 10.0/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.4 GiB heap, 0.0/0.7 GiB objectsResult logdir: C: Users Stefan ray_results PPONumber of trials: 1/1 (1 RUNNING) Trial name status loc . PPO_CartPole-v1_bd6e2_00000 | RUNNING | 127.0.0.1:9332 | . (pid=9332) 2021-11-23 19:47:29,301 WARNING deprecation.py:38 -- DeprecationWarning: `slice` has been deprecated. Use `SampleBatch[start:stop]` instead. This will raise an error in the future! . == Status ==Current time: 2021-11-23 19:47:31 (running for 00:00:32.10)Memory usage on this node: 10.1/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.4 GiB heap, 0.0/0.7 GiB objectsResult logdir: C: Users Stefan ray_results PPONumber of trials: 1/1 (1 RUNNING) Trial name status loc . PPO_CartPole-v1_bd6e2_00000 | RUNNING | 127.0.0.1:9332 | . Result for PPO_CartPole-v1_bd6e2_00000: agent_timesteps_total: 4000 custom_metrics: {} date: 2021-11-23_19-47-32 done: false episode_len_mean: 23.201183431952664 episode_media: {} episode_reward_max: 64.0 episode_reward_mean: 23.201183431952664 episode_reward_min: 8.0 episodes_this_iter: 169 episodes_total: 169 experiment_id: ada6434c43d44cc4854441944ca25a66 hostname: nb-stschn info: learner: default_policy: custom_metrics: {} learner_stats: cur_kl_coeff: 0.20000000298023224 cur_lr: 4.999999873689376e-05 entropy: 0.6663767099380493 entropy_coeff: 0.0 kl: 0.02739105187356472 model: {} policy_loss: -0.04134032875299454 total_loss: 200.2156219482422 vf_explained_var: 0.026224354282021523 vf_loss: 200.25149536132812 num_agent_steps_sampled: 4000 num_agent_steps_trained: 4000 num_steps_sampled: 4000 num_steps_trained: 4000 iterations_since_restore: 1 node_ip: 127.0.0.1 num_healthy_workers: 2 off_policy_estimator: {} perf: cpu_util_percent: 68.96363636363635 ram_util_percent: 84.45454545454545 pid: 9332 policy_reward_max: {} policy_reward_mean: {} policy_reward_min: {} sampler_perf: mean_action_processing_ms: 0.08780811829245165 mean_env_render_ms: 0.0 mean_env_wait_ms: 0.09023848270573184 mean_inference_ms: 1.5741673442388602 mean_raw_obs_processing_ms: 0.1437277133823059 time_since_restore: 7.516907691955566 time_this_iter_s: 7.516907691955566 time_total_s: 7.516907691955566 timers: learn_throughput: 1111.186 learn_time_ms: 3599.756 load_throughput: 0.0 load_time_ms: 0.0 sample_throughput: 1024.425 sample_time_ms: 3904.629 update_time_ms: 0.0 timestamp: 1637693252 timesteps_since_restore: 0 timesteps_this_iter: 0 timesteps_total: 4000 training_iteration: 1 trial_id: bd6e2_00000 . == Status ==Current time: 2021-11-23 19:47:37 (running for 00:00:37.62)Memory usage on this node: 10.1/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.4 GiB heap, 0.0/0.7 GiB objectsResult logdir: C: Users Stefan ray_results PPONumber of trials: 1/1 (1 RUNNING) Trial name status loc iter total time (s) ts reward episode_reward_max episode_reward_min episode_len_mean . PPO_CartPole-v1_bd6e2_00000 | RUNNING | 127.0.0.1:9332 | 1 | 7.51691 | 4000 | 23.2012 | 64 | 8 | 23.2012 | . Result for PPO_CartPole-v1_bd6e2_00000: agent_timesteps_total: 8000 custom_metrics: {} date: 2021-11-23_19-47-39 done: false episode_len_mean: 42.95 episode_media: {} episode_reward_max: 150.0 episode_reward_mean: 42.95 episode_reward_min: 10.0 episodes_this_iter: 87 episodes_total: 256 experiment_id: ada6434c43d44cc4854441944ca25a66 hostname: nb-stschn info: learner: default_policy: custom_metrics: {} learner_stats: cur_kl_coeff: 0.30000001192092896 cur_lr: 4.999999873689376e-05 entropy: 0.6131702065467834 entropy_coeff: 0.0 kl: 0.015614509582519531 model: {} policy_loss: -0.030681779608130455 total_loss: 312.3175964355469 vf_explained_var: 0.07965515553951263 vf_loss: 312.3436279296875 num_agent_steps_sampled: 8000 num_agent_steps_trained: 8000 num_steps_sampled: 8000 num_steps_trained: 8000 num_steps_trained_this_iter: 0 iterations_since_restore: 2 node_ip: 127.0.0.1 num_healthy_workers: 2 off_policy_estimator: {} perf: cpu_util_percent: 70.44 ram_util_percent: 84.61 pid: 9332 policy_reward_max: {} policy_reward_mean: {} policy_reward_min: {} sampler_perf: mean_action_processing_ms: 0.08674016599786703 mean_env_render_ms: 0.0 mean_env_wait_ms: 0.10221243666315946 mean_inference_ms: 1.4379395671503181 mean_raw_obs_processing_ms: 0.15194804311583987 time_since_restore: 14.383461952209473 time_this_iter_s: 6.866554260253906 time_total_s: 14.383461952209473 timers: learn_throughput: 1118.369 learn_time_ms: 3576.637 load_throughput: 0.0 load_time_ms: 0.0 sample_throughput: 735.569 sample_time_ms: 5437.97 update_time_ms: 0.0 timestamp: 1637693259 timesteps_since_restore: 0 timesteps_this_iter: 0 timesteps_total: 8000 training_iteration: 2 trial_id: bd6e2_00000 . == Status ==Current time: 2021-11-23 19:47:42 (running for 00:00:43.52)Memory usage on this node: 10.1/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.4 GiB heap, 0.0/0.7 GiB objectsResult logdir: C: Users Stefan ray_results PPONumber of trials: 1/1 (1 RUNNING) Trial name status loc iter total time (s) ts reward episode_reward_max episode_reward_min episode_len_mean . PPO_CartPole-v1_bd6e2_00000 | RUNNING | 127.0.0.1:9332 | 2 | 14.3835 | 8000 | 42.95 | 150 | 10 | 42.95 | . Result for PPO_CartPole-v1_bd6e2_00000: agent_timesteps_total: 12000 custom_metrics: {} date: 2021-11-23_19-47-46 done: false episode_len_mean: 72.01 episode_media: {} episode_reward_max: 367.0 episode_reward_mean: 72.01 episode_reward_min: 10.0 episodes_this_iter: 31 episodes_total: 287 experiment_id: ada6434c43d44cc4854441944ca25a66 hostname: nb-stschn info: learner: default_policy: custom_metrics: {} learner_stats: cur_kl_coeff: 0.30000001192092896 cur_lr: 4.999999873689376e-05 entropy: 0.5750772953033447 entropy_coeff: 0.0 kl: 0.010552152059972286 model: {} policy_loss: -0.02355009876191616 total_loss: 682.247802734375 vf_explained_var: 0.08913025259971619 vf_loss: 682.2681274414062 num_agent_steps_sampled: 12000 num_agent_steps_trained: 12000 num_steps_sampled: 12000 num_steps_trained: 12000 num_steps_trained_this_iter: 0 iterations_since_restore: 3 node_ip: 127.0.0.1 num_healthy_workers: 2 off_policy_estimator: {} perf: cpu_util_percent: 65.9 ram_util_percent: 84.63333333333334 pid: 9332 policy_reward_max: {} policy_reward_mean: {} policy_reward_min: {} sampler_perf: mean_action_processing_ms: 0.08345135189333704 mean_env_render_ms: 0.0 mean_env_wait_ms: 0.10287899069530118 mean_inference_ms: 1.4127022652323893 mean_raw_obs_processing_ms: 0.146412975705891 time_since_restore: 20.96682572364807 time_this_iter_s: 6.583363771438599 time_total_s: 20.96682572364807 timers: learn_throughput: 1149.968 learn_time_ms: 3478.358 load_throughput: 11955260.808 load_time_ms: 0.335 sample_throughput: 676.221 sample_time_ms: 5915.228 update_time_ms: 0.0 timestamp: 1637693266 timesteps_since_restore: 0 timesteps_this_iter: 0 timesteps_total: 12000 training_iteration: 3 trial_id: bd6e2_00000 . == Status ==Current time: 2021-11-23 19:47:48 (running for 00:00:49.12)Memory usage on this node: 10.1/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.4 GiB heap, 0.0/0.7 GiB objectsResult logdir: C: Users Stefan ray_results PPONumber of trials: 1/1 (1 RUNNING) Trial name status loc iter total time (s) ts reward episode_reward_max episode_reward_min episode_len_mean . PPO_CartPole-v1_bd6e2_00000 | RUNNING | 127.0.0.1:9332 | 3 | 20.9668 | 12000 | 72.01 | 367 | 10 | 72.01 | . Result for PPO_CartPole-v1_bd6e2_00000: agent_timesteps_total: 16000 custom_metrics: {} date: 2021-11-23_19-47-53 done: false episode_len_mean: 99.53 episode_media: {} episode_reward_max: 367.0 episode_reward_mean: 99.53 episode_reward_min: 10.0 episodes_this_iter: 24 episodes_total: 311 experiment_id: ada6434c43d44cc4854441944ca25a66 hostname: nb-stschn info: learner: default_policy: custom_metrics: {} learner_stats: cur_kl_coeff: 0.30000001192092896 cur_lr: 4.999999873689376e-05 entropy: 0.5543258190155029 entropy_coeff: 0.0 kl: 0.005451680161058903 model: {} policy_loss: -0.013567883521318436 total_loss: 657.56103515625 vf_explained_var: 0.31792160868644714 vf_loss: 657.572998046875 num_agent_steps_sampled: 16000 num_agent_steps_trained: 16000 num_steps_sampled: 16000 num_steps_trained: 16000 num_steps_trained_this_iter: 0 iterations_since_restore: 4 node_ip: 127.0.0.1 num_healthy_workers: 2 off_policy_estimator: {} perf: cpu_util_percent: 67.78888888888889 ram_util_percent: 84.67777777777779 pid: 9332 policy_reward_max: {} policy_reward_mean: {} policy_reward_min: {} sampler_perf: mean_action_processing_ms: 0.07607290116067764 mean_env_render_ms: 0.0 mean_env_wait_ms: 0.10106716069791831 mean_inference_ms: 1.413093391686641 mean_raw_obs_processing_ms: 0.13886860480277202 time_since_restore: 27.66690731048584 time_this_iter_s: 6.7000815868377686 time_total_s: 27.66690731048584 timers: learn_throughput: 1164.888 learn_time_ms: 3433.808 load_throughput: 15940347.743 load_time_ms: 0.251 sample_throughput: 654.005 sample_time_ms: 6116.164 update_time_ms: 3.91 timestamp: 1637693273 timesteps_since_restore: 0 timesteps_this_iter: 0 timesteps_total: 16000 training_iteration: 4 trial_id: bd6e2_00000 . == Status ==Current time: 2021-11-23 19:47:54 (running for 00:00:54.83)Memory usage on this node: 10.1/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.4 GiB heap, 0.0/0.7 GiB objectsResult logdir: C: Users Stefan ray_results PPONumber of trials: 1/1 (1 RUNNING) Trial name status loc iter total time (s) ts reward episode_reward_max episode_reward_min episode_len_mean . PPO_CartPole-v1_bd6e2_00000 | RUNNING | 127.0.0.1:9332 | 4 | 27.6669 | 16000 | 99.53 | 367 | 10 | 99.53 | . == Status ==Current time: 2021-11-23 19:47:59 (running for 00:00:59.92)Memory usage on this node: 10.1/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.4 GiB heap, 0.0/0.7 GiB objectsResult logdir: C: Users Stefan ray_results PPONumber of trials: 1/1 (1 RUNNING) Trial name status loc iter total time (s) ts reward episode_reward_max episode_reward_min episode_len_mean . PPO_CartPole-v1_bd6e2_00000 | RUNNING | 127.0.0.1:9332 | 4 | 27.6669 | 16000 | 99.53 | 367 | 10 | 99.53 | . Result for PPO_CartPole-v1_bd6e2_00000: agent_timesteps_total: 20000 custom_metrics: {} date: 2021-11-23_19-47-59 done: false episode_len_mean: 131.85 episode_media: {} episode_reward_max: 472.0 episode_reward_mean: 131.85 episode_reward_min: 11.0 episodes_this_iter: 16 episodes_total: 327 experiment_id: ada6434c43d44cc4854441944ca25a66 hostname: nb-stschn info: learner: default_policy: custom_metrics: {} learner_stats: cur_kl_coeff: 0.30000001192092896 cur_lr: 4.999999873689376e-05 entropy: 0.5512375831604004 entropy_coeff: 0.0 kl: 0.0024686120450496674 model: {} policy_loss: -0.010818097740411758 total_loss: 538.4854125976562 vf_explained_var: 0.321399986743927 vf_loss: 538.4955444335938 num_agent_steps_sampled: 20000 num_agent_steps_trained: 20000 num_steps_sampled: 20000 num_steps_trained: 20000 num_steps_trained_this_iter: 0 iterations_since_restore: 5 node_ip: 127.0.0.1 num_healthy_workers: 2 off_policy_estimator: {} perf: cpu_util_percent: 66.27000000000001 ram_util_percent: 84.69000000000001 pid: 9332 policy_reward_max: {} policy_reward_mean: {} policy_reward_min: {} sampler_perf: mean_action_processing_ms: 0.07838994072084958 mean_env_render_ms: 0.0 mean_env_wait_ms: 0.09994187932727808 mean_inference_ms: 1.4035711330826053 mean_raw_obs_processing_ms: 0.13563655209121872 time_since_restore: 34.28356695175171 time_this_iter_s: 6.616659641265869 time_total_s: 34.28356695175171 timers: learn_throughput: 1174.033 learn_time_ms: 3407.059 load_throughput: 19925434.679 load_time_ms: 0.201 sample_throughput: 642.973 sample_time_ms: 6221.1 update_time_ms: 6.459 timestamp: 1637693279 timesteps_since_restore: 0 timesteps_this_iter: 0 timesteps_total: 20000 training_iteration: 5 trial_id: bd6e2_00000 . == Status ==Current time: 2021-11-23 19:48:04 (running for 00:01:05.53)Memory usage on this node: 10.1/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.4 GiB heap, 0.0/0.7 GiB objectsResult logdir: C: Users Stefan ray_results PPONumber of trials: 1/1 (1 RUNNING) Trial name status loc iter total time (s) ts reward episode_reward_max episode_reward_min episode_len_mean . PPO_CartPole-v1_bd6e2_00000 | RUNNING | 127.0.0.1:9332 | 5 | 34.2836 | 20000 | 131.85 | 472 | 11 | 131.85 | . Result for PPO_CartPole-v1_bd6e2_00000: agent_timesteps_total: 24000 custom_metrics: {} date: 2021-11-23_19-48-06 done: false episode_len_mean: 165.3 episode_media: {} episode_reward_max: 500.0 episode_reward_mean: 165.3 episode_reward_min: 14.0 episodes_this_iter: 14 episodes_total: 341 experiment_id: ada6434c43d44cc4854441944ca25a66 hostname: nb-stschn info: learner: default_policy: custom_metrics: {} learner_stats: cur_kl_coeff: 0.15000000596046448 cur_lr: 4.999999873689376e-05 entropy: 0.5289359092712402 entropy_coeff: 0.0 kl: 0.007540307007730007 model: {} policy_loss: -0.010302959010004997 total_loss: 430.6041564941406 vf_explained_var: 0.3625507056713104 vf_loss: 430.6133117675781 num_agent_steps_sampled: 24000 num_agent_steps_trained: 24000 num_steps_sampled: 24000 num_steps_trained: 24000 num_steps_trained_this_iter: 0 iterations_since_restore: 6 node_ip: 127.0.0.1 num_healthy_workers: 2 off_policy_estimator: {} perf: cpu_util_percent: 66.44444444444444 ram_util_percent: 84.6888888888889 pid: 9332 policy_reward_max: {} policy_reward_mean: {} policy_reward_min: {} sampler_perf: mean_action_processing_ms: 0.08137795136967195 mean_env_render_ms: 0.0 mean_env_wait_ms: 0.09919168272984091 mean_inference_ms: 1.3928938913992197 mean_raw_obs_processing_ms: 0.13256708957866883 time_since_restore: 40.86706018447876 time_this_iter_s: 6.583493232727051 time_total_s: 40.86706018447876 timers: learn_throughput: 1181.754 learn_time_ms: 3384.8 load_throughput: 23910521.615 load_time_ms: 0.167 sample_throughput: 635.607 sample_time_ms: 6293.196 update_time_ms: 5.383 timestamp: 1637693286 timesteps_since_restore: 0 timesteps_this_iter: 0 timesteps_total: 24000 training_iteration: 6 trial_id: bd6e2_00000 . == Status ==Current time: 2021-11-23 19:48:10 (running for 00:01:11.15)Memory usage on this node: 10.1/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.4 GiB heap, 0.0/0.7 GiB objectsResult logdir: C: Users Stefan ray_results PPONumber of trials: 1/1 (1 RUNNING) Trial name status loc iter total time (s) ts reward episode_reward_max episode_reward_min episode_len_mean . PPO_CartPole-v1_bd6e2_00000 | RUNNING | 127.0.0.1:9332 | 6 | 40.8671 | 24000 | 165.3 | 500 | 14 | 165.3 | . Result for PPO_CartPole-v1_bd6e2_00000: agent_timesteps_total: 28000 custom_metrics: {} date: 2021-11-23_19-48-13 done: false episode_len_mean: 200.73 episode_media: {} episode_reward_max: 500.0 episode_reward_mean: 200.73 episode_reward_min: 14.0 episodes_this_iter: 10 episodes_total: 351 experiment_id: ada6434c43d44cc4854441944ca25a66 hostname: nb-stschn info: learner: default_policy: custom_metrics: {} learner_stats: cur_kl_coeff: 0.15000000596046448 cur_lr: 4.999999873689376e-05 entropy: 0.5529451966285706 entropy_coeff: 0.0 kl: 0.004082479979842901 model: {} policy_loss: -0.007522972766309977 total_loss: 385.4500427246094 vf_explained_var: 0.34760990738868713 vf_loss: 385.45697021484375 num_agent_steps_sampled: 28000 num_agent_steps_trained: 28000 num_steps_sampled: 28000 num_steps_trained: 28000 num_steps_trained_this_iter: 0 iterations_since_restore: 7 node_ip: 127.0.0.1 num_healthy_workers: 2 off_policy_estimator: {} perf: cpu_util_percent: 65.51111111111112 ram_util_percent: 84.62222222222222 pid: 9332 policy_reward_max: {} policy_reward_mean: {} policy_reward_min: {} sampler_perf: mean_action_processing_ms: 0.08292146908394506 mean_env_render_ms: 0.0 mean_env_wait_ms: 0.09879974143372734 mean_inference_ms: 1.385670263519395 mean_raw_obs_processing_ms: 0.13008639493272484 time_since_restore: 47.68342137336731 time_this_iter_s: 6.81636118888855 time_total_s: 47.68342137336731 timers: learn_throughput: 1178.46 learn_time_ms: 3394.26 load_throughput: 27895608.551 load_time_ms: 0.143 sample_throughput: 630.221 sample_time_ms: 6346.978 update_time_ms: 4.614 timestamp: 1637693293 timesteps_since_restore: 0 timesteps_this_iter: 0 timesteps_total: 28000 training_iteration: 7 trial_id: bd6e2_00000 . == Status ==Current time: 2021-11-23 19:48:16 (running for 00:01:17.00)Memory usage on this node: 9.8/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.4 GiB heap, 0.0/0.7 GiB objectsResult logdir: C: Users Stefan ray_results PPONumber of trials: 1/1 (1 RUNNING) Trial name status loc iter total time (s) ts reward episode_reward_max episode_reward_min episode_len_mean . PPO_CartPole-v1_bd6e2_00000 | RUNNING | 127.0.0.1:9332 | 7 | 47.6834 | 28000 | 200.73 | 500 | 14 | 200.73 | . Result for PPO_CartPole-v1_bd6e2_00000: agent_timesteps_total: 32000 custom_metrics: {} date: 2021-11-23_19-48-20 done: false episode_len_mean: 231.96 episode_media: {} episode_reward_max: 500.0 episode_reward_mean: 231.96 episode_reward_min: 22.0 episodes_this_iter: 8 episodes_total: 359 experiment_id: ada6434c43d44cc4854441944ca25a66 hostname: nb-stschn info: learner: default_policy: custom_metrics: {} learner_stats: cur_kl_coeff: 0.07500000298023224 cur_lr: 4.999999873689376e-05 entropy: 0.5215219259262085 entropy_coeff: 0.0 kl: 0.003867241321131587 model: {} policy_loss: -0.006445886567234993 total_loss: 463.6306457519531 vf_explained_var: 0.039233140647411346 vf_loss: 463.6368408203125 num_agent_steps_sampled: 32000 num_agent_steps_trained: 32000 num_steps_sampled: 32000 num_steps_trained: 32000 num_steps_trained_this_iter: 0 iterations_since_restore: 8 node_ip: 127.0.0.1 num_healthy_workers: 2 off_policy_estimator: {} perf: cpu_util_percent: 74.09 ram_util_percent: 82.85 pid: 9332 policy_reward_max: {} policy_reward_mean: {} policy_reward_min: {} sampler_perf: mean_action_processing_ms: 0.08278395268749097 mean_env_render_ms: 0.0 mean_env_wait_ms: 0.09865615802656738 mean_inference_ms: 1.3818915499780655 mean_raw_obs_processing_ms: 0.12845417041433646 time_since_restore: 54.49989891052246 time_this_iter_s: 6.816477537155151 time_total_s: 54.49989891052246 timers: learn_throughput: 1177.487 learn_time_ms: 3397.064 load_throughput: 31880695.487 load_time_ms: 0.125 sample_throughput: 623.274 sample_time_ms: 6417.719 update_time_ms: 4.037 timestamp: 1637693300 timesteps_since_restore: 0 timesteps_this_iter: 0 timesteps_total: 32000 training_iteration: 8 trial_id: bd6e2_00000 . == Status ==Current time: 2021-11-23 19:48:23 (running for 00:01:23.82)Memory usage on this node: 9.9/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.4 GiB heap, 0.0/0.7 GiB objectsResult logdir: C: Users Stefan ray_results PPONumber of trials: 1/1 (1 RUNNING) Trial name status loc iter total time (s) ts reward episode_reward_max episode_reward_min episode_len_mean . PPO_CartPole-v1_bd6e2_00000 | RUNNING | 127.0.0.1:9332 | 8 | 54.4999 | 32000 | 231.96 | 500 | 22 | 231.96 | . Result for PPO_CartPole-v1_bd6e2_00000: agent_timesteps_total: 36000 custom_metrics: {} date: 2021-11-23_19-48-26 done: false episode_len_mean: 262.68 episode_media: {} episode_reward_max: 500.0 episode_reward_mean: 262.68 episode_reward_min: 23.0 episodes_this_iter: 9 episodes_total: 368 experiment_id: ada6434c43d44cc4854441944ca25a66 hostname: nb-stschn info: learner: default_policy: custom_metrics: {} learner_stats: cur_kl_coeff: 0.03750000149011612 cur_lr: 4.999999873689376e-05 entropy: 0.5338630676269531 entropy_coeff: 0.0 kl: 0.007856571115553379 model: {} policy_loss: -0.023872433230280876 total_loss: 322.21954345703125 vf_explained_var: 0.245315819978714 vf_loss: 322.2431335449219 num_agent_steps_sampled: 36000 num_agent_steps_trained: 36000 num_steps_sampled: 36000 num_steps_trained: 36000 num_steps_trained_this_iter: 0 iterations_since_restore: 9 node_ip: 127.0.0.1 num_healthy_workers: 2 off_policy_estimator: {} perf: cpu_util_percent: 68.84444444444443 ram_util_percent: 82.88888888888889 pid: 9332 policy_reward_max: {} policy_reward_mean: {} policy_reward_min: {} sampler_perf: mean_action_processing_ms: 0.0811432695248291 mean_env_render_ms: 0.0 mean_env_wait_ms: 0.09879980490397404 mean_inference_ms: 1.380473521421992 mean_raw_obs_processing_ms: 0.1275865818305654 time_since_restore: 61.216761350631714 time_this_iter_s: 6.716862440109253 time_total_s: 61.216761350631714 timers: learn_throughput: 1182.524 learn_time_ms: 3382.596 load_throughput: 35865782.423 load_time_ms: 0.112 sample_throughput: 618.128 sample_time_ms: 6471.147 update_time_ms: 3.589 timestamp: 1637693306 timesteps_since_restore: 0 timesteps_this_iter: 0 timesteps_total: 36000 training_iteration: 9 trial_id: bd6e2_00000 . == Status ==Current time: 2021-11-23 19:48:28 (running for 00:01:29.58)Memory usage on this node: 9.9/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.4 GiB heap, 0.0/0.7 GiB objectsResult logdir: C: Users Stefan ray_results PPONumber of trials: 1/1 (1 RUNNING) Trial name status loc iter total time (s) ts reward episode_reward_max episode_reward_min episode_len_mean . PPO_CartPole-v1_bd6e2_00000 | RUNNING | 127.0.0.1:9332 | 9 | 61.2168 | 36000 | 262.68 | 500 | 23 | 262.68 | . Result for PPO_CartPole-v1_bd6e2_00000: agent_timesteps_total: 40000 custom_metrics: {} date: 2021-11-23_19-48-33 done: true episode_len_mean: 293.67 episode_media: {} episode_reward_max: 500.0 episode_reward_mean: 293.67 episode_reward_min: 23.0 episodes_this_iter: 8 episodes_total: 376 experiment_id: ada6434c43d44cc4854441944ca25a66 hostname: nb-stschn info: learner: default_policy: custom_metrics: {} learner_stats: cur_kl_coeff: 0.03750000149011612 cur_lr: 4.999999873689376e-05 entropy: 0.5301569700241089 entropy_coeff: 0.0 kl: 0.003627386409789324 model: {} policy_loss: -0.014947282150387764 total_loss: 501.4374694824219 vf_explained_var: -0.0290857944637537 vf_loss: 501.4522705078125 num_agent_steps_sampled: 40000 num_agent_steps_trained: 40000 num_steps_sampled: 40000 num_steps_trained: 40000 num_steps_trained_this_iter: 0 iterations_since_restore: 10 node_ip: 127.0.0.1 num_healthy_workers: 2 off_policy_estimator: {} perf: cpu_util_percent: 67.72999999999999 ram_util_percent: 82.9 pid: 9332 policy_reward_max: {} policy_reward_mean: {} policy_reward_min: {} sampler_perf: mean_action_processing_ms: 0.08066479833841124 mean_env_render_ms: 0.0 mean_env_wait_ms: 0.09868370561224364 mean_inference_ms: 1.3787463257944534 mean_raw_obs_processing_ms: 0.12667667753095402 time_since_restore: 67.84992170333862 time_this_iter_s: 6.633160352706909 time_total_s: 67.84992170333862 timers: learn_throughput: 1186.015 learn_time_ms: 3372.637 load_throughput: 39850869.359 load_time_ms: 0.1 sample_throughput: 616.427 sample_time_ms: 6489.005 update_time_ms: 3.23 timestamp: 1637693313 timesteps_since_restore: 0 timesteps_this_iter: 0 timesteps_total: 40000 training_iteration: 10 trial_id: bd6e2_00000 . == Status ==Current time: 2021-11-23 19:48:33 (running for 00:01:34.28)Memory usage on this node: 9.9/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 0/3 CPUs, 0/0 GPUs, 0.0/1.4 GiB heap, 0.0/0.7 GiB objectsResult logdir: C: Users Stefan ray_results PPONumber of trials: 1/1 (1 TERMINATED) Trial name status loc iter total time (s) ts reward episode_reward_max episode_reward_min episode_len_mean . PPO_CartPole-v1_bd6e2_00000 | TERMINATED | 127.0.0.1:9332 | 10 | 67.8499 | 40000 | 293.67 | 500 | 23 | 293.67 | . (pid=9332) [2021-11-23 19:48:34,182 E 9332 1660] raylet_client.cc:159: IOError: Unknown error [RayletClient] Failed to disconnect from raylet. (pid=9332) Windows fatal exception: access violation (pid=9332) (pid=5972) [2021-11-23 19:48:34,205 C 5972 17072] core_worker.cc:796: Check failed: _s.ok() Bad status: IOError: Unknown error (pid=5972) *** StackTrace Information *** (pid=5972) PyInit__raylet (pid=5972) PyInit__raylet (pid=5972) PyInit__raylet (pid=5972) PyInit__raylet (pid=5972) PyInit__raylet (pid=5972) PyInit__raylet (pid=5972) PyInit__raylet (pid=5972) PyInit__raylet (pid=5972) PyInit__raylet (pid=5972) PyInit__raylet (pid=5972) PyInit__raylet (pid=5972) PyInit__raylet (pid=5972) PyInit__raylet (pid=5972) PyInit__raylet (pid=5972) PyInit__raylet (pid=5972) PyInit__raylet (pid=5972) PyNumber_InPlaceLshift (pid=5972) Py_CheckFunctionResult (pid=5972) PyEval_EvalFrameDefault (pid=5972) Py_CheckFunctionResult (pid=5972) PyEval_EvalFrameDefault (pid=5972) PyEval_EvalCodeWithName (pid=5972) PyEval_EvalCodeEx (pid=5972) PyEval_EvalCode (pid=5972) PyArena_New (pid=5972) PyArena_New (pid=5972) PyRun_FileExFlags (pid=5972) PyRun_SimpleFileExFlags (pid=5972) PyRun_AnyFileExFlags (pid=5972) Py_FatalError (pid=5972) Py_RunMain (pid=5972) Py_RunMain (pid=5972) Py_Main (pid=5972) BaseThreadInitThunk (pid=5972) RtlUserThreadStart (pid=5972) (pid=6508) [2021-11-23 19:48:34,220 C 6508 9932] core_worker.cc:796: Check failed: _s.ok() Bad status: IOError: Unknown error (pid=6508) *** StackTrace Information *** (pid=6508) PyInit__raylet (pid=6508) PyInit__raylet (pid=6508) PyInit__raylet (pid=6508) PyInit__raylet (pid=6508) PyInit__raylet (pid=6508) PyInit__raylet (pid=6508) PyInit__raylet (pid=6508) PyInit__raylet (pid=6508) PyInit__raylet (pid=6508) PyInit__raylet (pid=6508) PyInit__raylet (pid=6508) PyInit__raylet (pid=6508) PyInit__raylet (pid=6508) PyInit__raylet (pid=6508) PyInit__raylet (pid=6508) PyInit__raylet (pid=6508) PyNumber_InPlaceLshift (pid=6508) Py_CheckFunctionResult (pid=6508) PyEval_EvalFrameDefault (pid=6508) Py_CheckFunctionResult (pid=6508) PyEval_EvalFrameDefault (pid=6508) PyEval_EvalCodeWithName (pid=6508) PyEval_EvalCodeEx (pid=6508) PyEval_EvalCode (pid=6508) PyArena_New (pid=6508) PyArena_New (pid=6508) PyRun_FileExFlags (pid=6508) PyRun_SimpleFileExFlags (pid=6508) PyRun_AnyFileExFlags (pid=6508) Py_FatalError (pid=6508) Py_RunMain (pid=6508) Py_RunMain (pid=6508) Py_Main (pid=6508) BaseThreadInitThunk (pid=6508) RtlUserThreadStart (pid=6508) (pid=5972) Windows fatal exception: access violation (pid=5972) (pid=5972) Stack (most recent call first): (pid=5972) File &#34;c: users stefan git-repos private blog venv lib site-packages ray worker.py&#34;, line 425 in main_loop (pid=5972) File &#34; (pid=6508) Windows fatal exception: access violation (pid=6508) (pid=6508) Stack (most recent call first): (pid=6508) File &#34;c: users stefan git-repos private blog venv lib site-packages ray worker.py&#34;, line 425 in main_loop (pid=6508) File &#34;c: users stefan git-repos private blog venv lib site-packages ray workers/default_worker.py&#34;, line 218 in &lt;module&gt; (pid=5972) c: users stefan git-repos private blog venv lib site-packages ray workers/ (pid=5972) d (pid=5972) e (pid=5972) fau (pid=5972) l (pid=5972) t (pid=5972) _w (pid=5972) ork (pid=5972) er.p (pid=5972) y (pid=5972) &#34;, line 218 in &lt;m (pid=5972) o (pid=5972) dule&gt; 2021-11-23 19:48:34,300 INFO tune.py:630 -- Total run time: 94.92 seconds (94.22 seconds for the tuning loop). . Option 1: Training finished successfully . . results1.default_metric = &quot;episode_reward_mean&quot; results1.default_mode = &quot;max&quot; # print mean number of time steps the pole was balanced (higher = better) results1.best_result[&quot;episode_reward_mean&quot;] . 293.67 . import seaborn as sns def plot_rewards(results): &quot;&quot;&quot;Plot scatter plot of the last 100 training episodes&quot;&quot;&quot; eps_rewards = results.best_result[&quot;hist_stats&quot;][&quot;episode_reward&quot;] eps = [i for i in range(len(eps_rewards))] ax = sns.scatterplot(eps, eps_rewards) ax.set_title(&quot;Reward over the last 100 Episodes&quot;) ax.set_xlabel(&quot;Episodes&quot;) ax.set_ylabel(&quot;Episode Reward&quot;) plot_rewards(results1) . NameError Traceback (most recent call last) &lt;ipython-input-8-252cc9299da6&gt; in &lt;module&gt; 12 13 &gt; 14 plot_rewards(results1) 15 16 NameError: name &#39;results1&#39; is not defined . Including the missing state helps the agent learn a good policy quickly, leading to high reward. . Option 2: Ignore Missing State . In many practical scenarios, missing state cannot be simply added to complete the partial observations, e.g., because measuring/capturing the missing observations incurs prohibitive costs or is physically not feasible. . In this case, the simplest alternative is using the partial observations as they are available. This works if the observations still include enough information to learn a useful policy. . However, if too much important information is missing, learning a useful policy becomes slow or even impossible. In the CartPole example, partial observations that do not include the velocity of the cart and the pole keep the agent from learning a useful policy. . from ray.rllib.examples.env.stateless_cartpole import StatelessCartPole from ray.tune import registry registry.register_env(&quot;StatelessCartPole&quot;, lambda _: StatelessCartPole()) config2 = ppo.DEFAULT_CONFIG.copy() config2[&quot;env&quot;] = &quot;StatelessCartPole&quot; # train; this takes a while results2 = ray.tune.run(&quot;PPO&quot;, config=config2, stop=stop) print(&quot;Option 2: Training finished successfully&quot;) . == Status ==Current time: 2021-11-23 19:48:34 (running for 00:00:00.13)Memory usage on this node: 9.1/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 0/3 CPUs, 0/0 GPUs, 0.0/1.4 GiB heap, 0.0/0.7 GiB objectsResult logdir: C: Users Stefan ray_results PPONumber of trials: 1/1 (1 PENDING) Trial name status loc . PPO_StatelessCartPole_f6243_00000 | PENDING | | . (pid=None) c: users stefan git-repos private blog venv lib site-packages redis connection.py:77: UserWarning: redis-py works best with hiredis. Please consider installing (pid=None) warnings.warn(msg) . == Status ==Current time: 2021-11-23 19:48:39 (running for 00:00:05.14)Memory usage on this node: 9.1/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 0/3 CPUs, 0/0 GPUs, 0.0/1.4 GiB heap, 0.0/0.7 GiB objectsResult logdir: C: Users Stefan ray_results PPONumber of trials: 1/1 (1 PENDING) Trial name status loc . PPO_StatelessCartPole_f6243_00000 | PENDING | | . (pid=15776) 2021-11-23 19:48:45,935 INFO trainer.py:753 -- Tip: set framework=tfe or the --eager flag to enable TensorFlow eager execution (pid=15776) 2021-11-23 19:48:45,935 INFO ppo.py:166 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn&#39;t work for you. (pid=15776) 2021-11-23 19:48:45,935 INFO trainer.py:770 -- Current log_level is WARN. For more information, set &#39;log_level&#39;: &#39;INFO&#39; / &#39;DEBUG&#39; or use the -v and -vv flags. (pid=None) c: users stefan git-repos private blog venv lib site-packages redis connection.py:77: UserWarning: redis-py works best with hiredis. Please consider installing (pid=None) warnings.warn(msg) (pid=None) c: users stefan git-repos private blog venv lib site-packages redis connection.py:77: UserWarning: redis-py works best with hiredis. Please consider installing (pid=None) warnings.warn(msg) (pid=24480) 2021-11-23 19:48:59,018 WARNING deprecation.py:38 -- DeprecationWarning: `SampleBatch[&#39;is_training&#39;]` has been deprecated. Use `SampleBatch.is_training` instead. This will raise an error in the future! (pid=15776) 2021-11-23 19:49:00,468 WARNING deprecation.py:38 -- DeprecationWarning: `SampleBatch[&#39;is_training&#39;]` has been deprecated. Use `SampleBatch.is_training` instead. This will raise an error in the future! . == Status ==Current time: 2021-11-23 19:49:01 (running for 00:00:26.94)Memory usage on this node: 9.9/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.4 GiB heap, 0.0/0.7 GiB objectsResult logdir: C: Users Stefan ray_results PPONumber of trials: 1/1 (1 RUNNING) Trial name status loc . PPO_StatelessCartPole_f6243_00000 | RUNNING | 127.0.0.1:15776 | . (pid=15776) 2021-11-23 19:49:01,452 WARNING trainer_template.py:185 -- `execution_plan` functions should accept `trainer`, `workers`, and `config` as args! (pid=15776) 2021-11-23 19:49:01,452 INFO trainable.py:110 -- Trainable.setup took 15.517 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads. (pid=15776) 2021-11-23 19:49:01,468 WARNING util.py:57 -- Install gputil for GPU system monitoring. . == Status ==Current time: 2021-11-23 19:49:02 (running for 00:00:27.96)Memory usage on this node: 9.9/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.4 GiB heap, 0.0/0.7 GiB objectsResult logdir: C: Users Stefan ray_results PPONumber of trials: 1/1 (1 RUNNING) Trial name status loc . PPO_StatelessCartPole_f6243_00000 | RUNNING | 127.0.0.1:15776 | . (pid=15776) 2021-11-23 19:49:05,068 WARNING deprecation.py:38 -- DeprecationWarning: `slice` has been deprecated. Use `SampleBatch[start:stop]` instead. This will raise an error in the future! . == Status ==Current time: 2021-11-23 19:49:07 (running for 00:00:33.03)Memory usage on this node: 9.9/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.4 GiB heap, 0.0/0.7 GiB objectsResult logdir: C: Users Stefan ray_results PPONumber of trials: 1/1 (1 RUNNING) Trial name status loc . PPO_StatelessCartPole_f6243_00000 | RUNNING | 127.0.0.1:15776 | . Result for PPO_StatelessCartPole_f6243_00000: agent_timesteps_total: 4000 custom_metrics: {} date: 2021-11-23_19-49-08 done: false episode_len_mean: 20.723958333333332 episode_media: {} episode_reward_max: 64.0 episode_reward_mean: 20.723958333333332 episode_reward_min: 9.0 episodes_this_iter: 192 episodes_total: 192 experiment_id: e1ee612c6c3f4f48a9f19984a409dc5c hostname: nb-stschn info: learner: default_policy: custom_metrics: {} learner_stats: cur_kl_coeff: 0.20000000298023224 cur_lr: 4.999999873689376e-05 entropy: 0.6812710165977478 entropy_coeff: 0.0 kl: 0.012129432521760464 model: {} policy_loss: -0.019796958193182945 total_loss: 149.3815155029297 vf_explained_var: 0.007414208725094795 vf_loss: 149.39889526367188 num_agent_steps_sampled: 4000 num_agent_steps_trained: 4000 num_steps_sampled: 4000 num_steps_trained: 4000 iterations_since_restore: 1 node_ip: 127.0.0.1 num_healthy_workers: 2 off_policy_estimator: {} perf: cpu_util_percent: 63.45454545454545 ram_util_percent: 83.53636363636363 pid: 15776 policy_reward_max: {} policy_reward_mean: {} policy_reward_min: {} sampler_perf: mean_action_processing_ms: 0.07648677423201698 mean_env_render_ms: 0.0 mean_env_wait_ms: 0.09130326962443208 mean_inference_ms: 1.3772258331790361 mean_raw_obs_processing_ms: 0.19347365351690762 time_since_restore: 7.172215700149536 time_this_iter_s: 7.172215700149536 time_total_s: 7.172215700149536 timers: learn_throughput: 1121.407 learn_time_ms: 3566.947 load_throughput: 0.0 load_time_ms: 0.0 sample_throughput: 1107.595 sample_time_ms: 3611.43 update_time_ms: 16.655 timestamp: 1637693348 timesteps_since_restore: 0 timesteps_this_iter: 0 timesteps_total: 4000 training_iteration: 1 trial_id: f6243_00000 . == Status ==Current time: 2021-11-23 19:49:12 (running for 00:00:38.23)Memory usage on this node: 9.9/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.4 GiB heap, 0.0/0.7 GiB objectsResult logdir: C: Users Stefan ray_results PPONumber of trials: 1/1 (1 RUNNING) Trial name status loc iter total time (s) ts reward episode_reward_max episode_reward_min episode_len_mean . PPO_StatelessCartPole_f6243_00000 | RUNNING | 127.0.0.1:15776 | 1 | 7.17222 | 4000 | 20.724 | 64 | 9 | 20.724 | . Result for PPO_StatelessCartPole_f6243_00000: agent_timesteps_total: 8000 custom_metrics: {} date: 2021-11-23_19-49-15 done: false episode_len_mean: 27.685314685314687 episode_media: {} episode_reward_max: 77.0 episode_reward_mean: 27.685314685314687 episode_reward_min: 10.0 episodes_this_iter: 143 episodes_total: 335 experiment_id: e1ee612c6c3f4f48a9f19984a409dc5c hostname: nb-stschn info: learner: default_policy: custom_metrics: {} learner_stats: cur_kl_coeff: 0.20000000298023224 cur_lr: 4.999999873689376e-05 entropy: 0.6397432684898376 entropy_coeff: 0.0 kl: 0.014385893940925598 model: {} policy_loss: -0.024019667878746986 total_loss: 124.4566650390625 vf_explained_var: 0.03795936703681946 vf_loss: 124.4778060913086 num_agent_steps_sampled: 8000 num_agent_steps_trained: 8000 num_steps_sampled: 8000 num_steps_trained: 8000 num_steps_trained_this_iter: 0 iterations_since_restore: 2 node_ip: 127.0.0.1 num_healthy_workers: 2 off_policy_estimator: {} perf: cpu_util_percent: 66.28888888888889 ram_util_percent: 83.6 pid: 15776 policy_reward_max: {} policy_reward_mean: {} policy_reward_min: {} sampler_perf: mean_action_processing_ms: 0.0674684653726705 mean_env_render_ms: 0.0 mean_env_wait_ms: 0.10436432086360654 mean_inference_ms: 1.3726427800181444 mean_raw_obs_processing_ms: 0.17047277193303836 time_since_restore: 13.921866655349731 time_this_iter_s: 6.749650955200195 time_total_s: 13.921866655349731 timers: learn_throughput: 1159.465 learn_time_ms: 3449.867 load_throughput: 0.0 load_time_ms: 0.0 sample_throughput: 751.582 sample_time_ms: 5322.108 update_time_ms: 8.327 timestamp: 1637693355 timesteps_since_restore: 0 timesteps_this_iter: 0 timesteps_total: 8000 training_iteration: 2 trial_id: f6243_00000 . == Status ==Current time: 2021-11-23 19:49:18 (running for 00:00:43.99)Memory usage on this node: 10.0/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.4 GiB heap, 0.0/0.7 GiB objectsResult logdir: C: Users Stefan ray_results PPONumber of trials: 1/1 (1 RUNNING) Trial name status loc iter total time (s) ts reward episode_reward_max episode_reward_min episode_len_mean . PPO_StatelessCartPole_f6243_00000 | RUNNING | 127.0.0.1:15776 | 2 | 13.9219 | 8000 | 27.6853 | 77 | 10 | 27.6853 | . Result for PPO_StatelessCartPole_f6243_00000: agent_timesteps_total: 12000 custom_metrics: {} date: 2021-11-23_19-49-22 done: false episode_len_mean: 41.26 episode_media: {} episode_reward_max: 130.0 episode_reward_mean: 41.26 episode_reward_min: 10.0 episodes_this_iter: 96 episodes_total: 431 experiment_id: e1ee612c6c3f4f48a9f19984a409dc5c hostname: nb-stschn info: learner: default_policy: custom_metrics: {} learner_stats: cur_kl_coeff: 0.20000000298023224 cur_lr: 4.999999873689376e-05 entropy: 0.6044858694076538 entropy_coeff: 0.0 kl: 0.006583377253264189 model: {} policy_loss: -0.010080553591251373 total_loss: 215.9619140625 vf_explained_var: 0.08694871515035629 vf_loss: 215.97067260742188 num_agent_steps_sampled: 12000 num_agent_steps_trained: 12000 num_steps_sampled: 12000 num_steps_trained: 12000 num_steps_trained_this_iter: 0 iterations_since_restore: 3 node_ip: 127.0.0.1 num_healthy_workers: 2 off_policy_estimator: {} perf: cpu_util_percent: 66.15555555555555 ram_util_percent: 83.6888888888889 pid: 15776 policy_reward_max: {} policy_reward_mean: {} policy_reward_min: {} sampler_perf: mean_action_processing_ms: 0.06627052248314967 mean_env_render_ms: 0.0 mean_env_wait_ms: 0.10074768092138378 mean_inference_ms: 1.3647424179536052 mean_raw_obs_processing_ms: 0.17819369635152466 time_since_restore: 20.67140221595764 time_this_iter_s: 6.74953556060791 time_total_s: 20.67140221595764 timers: learn_throughput: 1174.47 learn_time_ms: 3405.791 load_throughput: 0.0 load_time_ms: 0.0 sample_throughput: 687.968 sample_time_ms: 5814.226 update_time_ms: 5.552 timestamp: 1637693362 timesteps_since_restore: 0 timesteps_this_iter: 0 timesteps_total: 12000 training_iteration: 3 trial_id: f6243_00000 . == Status ==Current time: 2021-11-23 19:49:24 (running for 00:00:49.76)Memory usage on this node: 9.9/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.4 GiB heap, 0.0/0.7 GiB objectsResult logdir: C: Users Stefan ray_results PPONumber of trials: 1/1 (1 RUNNING) Trial name status loc iter total time (s) ts reward episode_reward_max episode_reward_min episode_len_mean . PPO_StatelessCartPole_f6243_00000 | RUNNING | 127.0.0.1:15776 | 3 | 20.6714 | 12000 | 41.26 | 130 | 10 | 41.26 | . Result for PPO_StatelessCartPole_f6243_00000: agent_timesteps_total: 16000 custom_metrics: {} date: 2021-11-23_19-49-29 done: false episode_len_mean: 43.61 episode_media: {} episode_reward_max: 155.0 episode_reward_mean: 43.61 episode_reward_min: 11.0 episodes_this_iter: 90 episodes_total: 521 experiment_id: e1ee612c6c3f4f48a9f19984a409dc5c hostname: nb-stschn info: learner: default_policy: custom_metrics: {} learner_stats: cur_kl_coeff: 0.20000000298023224 cur_lr: 4.999999873689376e-05 entropy: 0.5590699315071106 entropy_coeff: 0.0 kl: 0.004985678941011429 model: {} policy_loss: -0.00941834133118391 total_loss: 248.4127655029297 vf_explained_var: 0.12295760214328766 vf_loss: 248.42120361328125 num_agent_steps_sampled: 16000 num_agent_steps_trained: 16000 num_steps_sampled: 16000 num_steps_trained: 16000 num_steps_trained_this_iter: 0 iterations_since_restore: 4 node_ip: 127.0.0.1 num_healthy_workers: 2 off_policy_estimator: {} perf: cpu_util_percent: 68.22 ram_util_percent: 83.62 pid: 15776 policy_reward_max: {} policy_reward_mean: {} policy_reward_min: {} sampler_perf: mean_action_processing_ms: 0.06685427241237019 mean_env_render_ms: 0.0 mean_env_wait_ms: 0.09767074502769837 mean_inference_ms: 1.3661803226509006 mean_raw_obs_processing_ms: 0.1706688797119504 time_since_restore: 27.404489040374756 time_this_iter_s: 6.733086824417114 time_total_s: 27.404489040374756 timers: learn_throughput: 1183.679 learn_time_ms: 3379.295 load_throughput: 0.0 load_time_ms: 0.0 sample_throughput: 660.274 sample_time_ms: 6058.087 update_time_ms: 4.164 timestamp: 1637693369 timesteps_since_restore: 0 timesteps_this_iter: 0 timesteps_total: 16000 training_iteration: 4 trial_id: f6243_00000 . == Status ==Current time: 2021-11-23 19:49:31 (running for 00:00:56.50)Memory usage on this node: 9.9/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.4 GiB heap, 0.0/0.7 GiB objectsResult logdir: C: Users Stefan ray_results PPONumber of trials: 1/1 (1 RUNNING) Trial name status loc iter total time (s) ts reward episode_reward_max episode_reward_min episode_len_mean . PPO_StatelessCartPole_f6243_00000 | RUNNING | 127.0.0.1:15776 | 4 | 27.4045 | 16000 | 43.61 | 155 | 11 | 43.61 | . == Status ==Current time: 2021-11-23 19:49:36 (running for 00:01:01.58)Memory usage on this node: 10.0/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.4 GiB heap, 0.0/0.7 GiB objectsResult logdir: C: Users Stefan ray_results PPONumber of trials: 1/1 (1 RUNNING) Trial name status loc iter total time (s) ts reward episode_reward_max episode_reward_min episode_len_mean . PPO_StatelessCartPole_f6243_00000 | RUNNING | 127.0.0.1:15776 | 4 | 27.4045 | 16000 | 43.61 | 155 | 11 | 43.61 | . Result for PPO_StatelessCartPole_f6243_00000: agent_timesteps_total: 20000 custom_metrics: {} date: 2021-11-23_19-49-36 done: false episode_len_mean: 45.82 episode_media: {} episode_reward_max: 155.0 episode_reward_mean: 45.82 episode_reward_min: 12.0 episodes_this_iter: 89 episodes_total: 610 experiment_id: e1ee612c6c3f4f48a9f19984a409dc5c hostname: nb-stschn info: learner: default_policy: custom_metrics: {} learner_stats: cur_kl_coeff: 0.10000000149011612 cur_lr: 4.999999873689376e-05 entropy: 0.5189774036407471 entropy_coeff: 0.0 kl: 0.003044182201847434 model: {} policy_loss: 0.001426202361471951 total_loss: 230.5676727294922 vf_explained_var: 0.16542400419712067 vf_loss: 230.5659637451172 num_agent_steps_sampled: 20000 num_agent_steps_trained: 20000 num_steps_sampled: 20000 num_steps_trained: 20000 num_steps_trained_this_iter: 0 iterations_since_restore: 5 node_ip: 127.0.0.1 num_healthy_workers: 2 off_policy_estimator: {} perf: cpu_util_percent: 77.62727272727274 ram_util_percent: 83.7090909090909 pid: 15776 policy_reward_max: {} policy_reward_mean: {} policy_reward_min: {} sampler_perf: mean_action_processing_ms: 0.06714989286621349 mean_env_render_ms: 0.0 mean_env_wait_ms: 0.09601894778190767 mean_inference_ms: 1.3755709465948727 mean_raw_obs_processing_ms: 0.1653164592918336 time_since_restore: 35.08799147605896 time_this_iter_s: 7.683502435684204 time_total_s: 35.08799147605896 timers: learn_throughput: 1132.994 learn_time_ms: 3530.469 load_throughput: 0.0 load_time_ms: 0.0 sample_throughput: 643.046 sample_time_ms: 6220.39 update_time_ms: 3.331 timestamp: 1637693376 timesteps_since_restore: 0 timesteps_this_iter: 0 timesteps_total: 20000 training_iteration: 5 trial_id: f6243_00000 . == Status ==Current time: 2021-11-23 19:49:41 (running for 00:01:07.28)Memory usage on this node: 10.1/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.4 GiB heap, 0.0/0.7 GiB objectsResult logdir: C: Users Stefan ray_results PPONumber of trials: 1/1 (1 RUNNING) Trial name status loc iter total time (s) ts reward episode_reward_max episode_reward_min episode_len_mean . PPO_StatelessCartPole_f6243_00000 | RUNNING | 127.0.0.1:15776 | 5 | 35.088 | 20000 | 45.82 | 155 | 12 | 45.82 | . Result for PPO_StatelessCartPole_f6243_00000: agent_timesteps_total: 24000 custom_metrics: {} date: 2021-11-23_19-49-44 done: false episode_len_mean: 47.12 episode_media: {} episode_reward_max: 123.0 episode_reward_mean: 47.12 episode_reward_min: 11.0 episodes_this_iter: 86 episodes_total: 696 experiment_id: e1ee612c6c3f4f48a9f19984a409dc5c hostname: nb-stschn info: learner: default_policy: custom_metrics: {} learner_stats: cur_kl_coeff: 0.05000000074505806 cur_lr: 4.999999873689376e-05 entropy: 0.4841262400150299 entropy_coeff: 0.0 kl: 0.002589465817436576 model: {} policy_loss: -0.0048765153624117374 total_loss: 186.0424041748047 vf_explained_var: 0.18889868259429932 vf_loss: 186.0471649169922 num_agent_steps_sampled: 24000 num_agent_steps_trained: 24000 num_steps_sampled: 24000 num_steps_trained: 24000 num_steps_trained_this_iter: 0 iterations_since_restore: 6 node_ip: 127.0.0.1 num_healthy_workers: 2 off_policy_estimator: {} perf: cpu_util_percent: 86.0909090909091 ram_util_percent: 84.58181818181816 pid: 15776 policy_reward_max: {} policy_reward_mean: {} policy_reward_min: {} sampler_perf: mean_action_processing_ms: 0.0701231674246916 mean_env_render_ms: 0.0 mean_env_wait_ms: 0.0948783101644181 mean_inference_ms: 1.4164701451124595 mean_raw_obs_processing_ms: 0.16299152776545836 time_since_restore: 43.03928899765015 time_this_iter_s: 7.9512975215911865 time_total_s: 43.03928899765015 timers: learn_throughput: 1114.411 learn_time_ms: 3589.34 load_throughput: 0.0 load_time_ms: 0.0 sample_throughput: 609.662 sample_time_ms: 6561.008 update_time_ms: 2.776 timestamp: 1637693384 timesteps_since_restore: 0 timesteps_this_iter: 0 timesteps_total: 24000 training_iteration: 6 trial_id: f6243_00000 . == Status ==Current time: 2021-11-23 19:49:47 (running for 00:01:13.28)Memory usage on this node: 10.2/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.4 GiB heap, 0.0/0.7 GiB objectsResult logdir: C: Users Stefan ray_results PPONumber of trials: 1/1 (1 RUNNING) Trial name status loc iter total time (s) ts reward episode_reward_max episode_reward_min episode_len_mean . PPO_StatelessCartPole_f6243_00000 | RUNNING | 127.0.0.1:15776 | 6 | 43.0393 | 24000 | 47.12 | 123 | 11 | 47.12 | . Result for PPO_StatelessCartPole_f6243_00000: agent_timesteps_total: 28000 custom_metrics: {} date: 2021-11-23_19-49-52 done: false episode_len_mean: 48.9 episode_media: {} episode_reward_max: 132.0 episode_reward_mean: 48.9 episode_reward_min: 10.0 episodes_this_iter: 78 episodes_total: 774 experiment_id: e1ee612c6c3f4f48a9f19984a409dc5c hostname: nb-stschn info: learner: default_policy: custom_metrics: {} learner_stats: cur_kl_coeff: 0.02500000037252903 cur_lr: 4.999999873689376e-05 entropy: 0.4601098895072937 entropy_coeff: 0.0 kl: 0.0032913885079324245 model: {} policy_loss: -0.009747156873345375 total_loss: 165.0985107421875 vf_explained_var: 0.2212902456521988 vf_loss: 165.10816955566406 num_agent_steps_sampled: 28000 num_agent_steps_trained: 28000 num_steps_sampled: 28000 num_steps_trained: 28000 num_steps_trained_this_iter: 0 iterations_since_restore: 7 node_ip: 127.0.0.1 num_healthy_workers: 2 off_policy_estimator: {} perf: cpu_util_percent: 82.04545454545453 ram_util_percent: 85.5909090909091 pid: 15776 policy_reward_max: {} policy_reward_mean: {} policy_reward_min: {} sampler_perf: mean_action_processing_ms: 0.07340913232282371 mean_env_render_ms: 0.0 mean_env_wait_ms: 0.10125854571462303 mean_inference_ms: 1.4386485193573981 mean_raw_obs_processing_ms: 0.16407616070027764 time_since_restore: 50.722336292266846 time_this_iter_s: 7.683047294616699 time_total_s: 50.722336292266846 timers: learn_throughput: 1112.463 learn_time_ms: 3595.625 load_throughput: 0.0 load_time_ms: 0.0 sample_throughput: 591.606 sample_time_ms: 6761.26 update_time_ms: 2.379 timestamp: 1637693392 timesteps_since_restore: 0 timesteps_this_iter: 0 timesteps_total: 28000 training_iteration: 7 trial_id: f6243_00000 . == Status ==Current time: 2021-11-23 19:49:53 (running for 00:01:18.95)Memory usage on this node: 10.3/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.4 GiB heap, 0.0/0.7 GiB objectsResult logdir: C: Users Stefan ray_results PPONumber of trials: 1/1 (1 RUNNING) Trial name status loc iter total time (s) ts reward episode_reward_max episode_reward_min episode_len_mean . PPO_StatelessCartPole_f6243_00000 | RUNNING | 127.0.0.1:15776 | 7 | 50.7223 | 28000 | 48.9 | 132 | 10 | 48.9 | . == Status ==Current time: 2021-11-23 19:49:58 (running for 00:01:24.03)Memory usage on this node: 10.3/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.4 GiB heap, 0.0/0.7 GiB objectsResult logdir: C: Users Stefan ray_results PPONumber of trials: 1/1 (1 RUNNING) Trial name status loc iter total time (s) ts reward episode_reward_max episode_reward_min episode_len_mean . PPO_StatelessCartPole_f6243_00000 | RUNNING | 127.0.0.1:15776 | 7 | 50.7223 | 28000 | 48.9 | 132 | 10 | 48.9 | . Result for PPO_StatelessCartPole_f6243_00000: agent_timesteps_total: 32000 custom_metrics: {} date: 2021-11-23_19-50-00 done: false episode_len_mean: 49.94 episode_media: {} episode_reward_max: 132.0 episode_reward_mean: 49.94 episode_reward_min: 10.0 episodes_this_iter: 80 episodes_total: 854 experiment_id: e1ee612c6c3f4f48a9f19984a409dc5c hostname: nb-stschn info: learner: default_policy: custom_metrics: {} learner_stats: cur_kl_coeff: 0.012500000186264515 cur_lr: 4.999999873689376e-05 entropy: 0.41464871168136597 entropy_coeff: 0.0 kl: 0.003240640740841627 model: {} policy_loss: -0.0035140279214829206 total_loss: 157.30711364746094 vf_explained_var: 0.2239331603050232 vf_loss: 157.31057739257812 num_agent_steps_sampled: 32000 num_agent_steps_trained: 32000 num_steps_sampled: 32000 num_steps_trained: 32000 num_steps_trained_this_iter: 0 iterations_since_restore: 8 node_ip: 127.0.0.1 num_healthy_workers: 2 off_policy_estimator: {} perf: cpu_util_percent: 83.00999999999999 ram_util_percent: 86.56999999999998 pid: 15776 policy_reward_max: {} policy_reward_mean: {} policy_reward_min: {} sampler_perf: mean_action_processing_ms: 0.08037184503765213 mean_env_render_ms: 0.0 mean_env_wait_ms: 0.10194644822663296 mean_inference_ms: 1.461113024900464 mean_raw_obs_processing_ms: 0.16301264450339187 time_since_restore: 58.39958596229553 time_this_iter_s: 7.6772496700286865 time_total_s: 58.39958596229553 timers: learn_throughput: 1111.65 learn_time_ms: 3598.255 load_throughput: 0.0 load_time_ms: 0.0 sample_throughput: 581.176 sample_time_ms: 6882.592 update_time_ms: 2.082 timestamp: 1637693400 timesteps_since_restore: 0 timesteps_this_iter: 0 timesteps_total: 32000 training_iteration: 8 trial_id: f6243_00000 . == Status ==Current time: 2021-11-23 19:50:04 (running for 00:01:29.73)Memory usage on this node: 10.4/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.4 GiB heap, 0.0/0.7 GiB objectsResult logdir: C: Users Stefan ray_results PPONumber of trials: 1/1 (1 RUNNING) Trial name status loc iter total time (s) ts reward episode_reward_max episode_reward_min episode_len_mean . PPO_StatelessCartPole_f6243_00000 | RUNNING | 127.0.0.1:15776 | 8 | 58.3996 | 32000 | 49.94 | 132 | 10 | 49.94 | . Result for PPO_StatelessCartPole_f6243_00000: agent_timesteps_total: 36000 custom_metrics: {} date: 2021-11-23_19-50-08 done: false episode_len_mean: 51.12 episode_media: {} episode_reward_max: 125.0 episode_reward_mean: 51.12 episode_reward_min: 14.0 episodes_this_iter: 79 episodes_total: 933 experiment_id: e1ee612c6c3f4f48a9f19984a409dc5c hostname: nb-stschn info: learner: default_policy: custom_metrics: {} learner_stats: cur_kl_coeff: 0.0062500000931322575 cur_lr: 4.999999873689376e-05 entropy: 0.43995144963264465 entropy_coeff: 0.0 kl: 0.002562695648521185 model: {} policy_loss: -0.008500334806740284 total_loss: 144.52923583984375 vf_explained_var: 0.27317333221435547 vf_loss: 144.5377197265625 num_agent_steps_sampled: 36000 num_agent_steps_trained: 36000 num_steps_sampled: 36000 num_steps_trained: 36000 num_steps_trained_this_iter: 0 iterations_since_restore: 9 node_ip: 127.0.0.1 num_healthy_workers: 2 off_policy_estimator: {} perf: cpu_util_percent: 82.54545454545455 ram_util_percent: 87.0 pid: 15776 policy_reward_max: {} policy_reward_mean: {} policy_reward_min: {} sampler_perf: mean_action_processing_ms: 0.0844253703671287 mean_env_render_ms: 0.0 mean_env_wait_ms: 0.10347341322560075 mean_inference_ms: 1.4792651792195692 mean_raw_obs_processing_ms: 0.1622538972715257 time_since_restore: 66.21657919883728 time_this_iter_s: 7.816993236541748 time_total_s: 66.21657919883728 timers: learn_throughput: 1107.594 learn_time_ms: 3611.432 load_throughput: 0.0 load_time_ms: 0.0 sample_throughput: 572.872 sample_time_ms: 6982.361 update_time_ms: 1.851 timestamp: 1637693408 timesteps_since_restore: 0 timesteps_this_iter: 0 timesteps_total: 36000 training_iteration: 9 trial_id: f6243_00000 . == Status ==Current time: 2021-11-23 19:50:11 (running for 00:01:36.58)Memory usage on this node: 10.3/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.4 GiB heap, 0.0/0.7 GiB objectsResult logdir: C: Users Stefan ray_results PPONumber of trials: 1/1 (1 RUNNING) Trial name status loc iter total time (s) ts reward episode_reward_max episode_reward_min episode_len_mean . PPO_StatelessCartPole_f6243_00000 | RUNNING | 127.0.0.1:15776 | 9 | 66.2166 | 36000 | 51.12 | 125 | 14 | 51.12 | . Result for PPO_StatelessCartPole_f6243_00000: agent_timesteps_total: 40000 custom_metrics: {} date: 2021-11-23_19-50-16 done: true episode_len_mean: 51.67 episode_media: {} episode_reward_max: 147.0 episode_reward_mean: 51.67 episode_reward_min: 16.0 episodes_this_iter: 74 episodes_total: 1007 experiment_id: e1ee612c6c3f4f48a9f19984a409dc5c hostname: nb-stschn info: learner: default_policy: custom_metrics: {} learner_stats: cur_kl_coeff: 0.0031250000465661287 cur_lr: 4.999999873689376e-05 entropy: 0.4446806311607361 entropy_coeff: 0.0 kl: 0.0022894737776368856 model: {} policy_loss: 0.003023507073521614 total_loss: 219.97442626953125 vf_explained_var: 0.1855245977640152 vf_loss: 219.9713897705078 num_agent_steps_sampled: 40000 num_agent_steps_trained: 40000 num_steps_sampled: 40000 num_steps_trained: 40000 num_steps_trained_this_iter: 0 iterations_since_restore: 10 node_ip: 127.0.0.1 num_healthy_workers: 2 off_policy_estimator: {} perf: cpu_util_percent: 83.63333333333334 ram_util_percent: 86.93333333333334 pid: 15776 policy_reward_max: {} policy_reward_mean: {} policy_reward_min: {} sampler_perf: mean_action_processing_ms: 0.08614659311321836 mean_env_render_ms: 0.0 mean_env_wait_ms: 0.10334351867174033 mean_inference_ms: 1.4976440046604949 mean_raw_obs_processing_ms: 0.16262805119902793 time_since_restore: 74.18315482139587 time_this_iter_s: 7.966575622558594 time_total_s: 74.18315482139587 timers: learn_throughput: 1102.352 learn_time_ms: 3628.605 load_throughput: 0.0 load_time_ms: 0.0 sample_throughput: 565.026 sample_time_ms: 7079.322 update_time_ms: 1.665 timestamp: 1637693416 timesteps_since_restore: 0 timesteps_this_iter: 0 timesteps_total: 40000 training_iteration: 10 trial_id: f6243_00000 . == Status ==Current time: 2021-11-23 19:50:16 (running for 00:01:41.61)Memory usage on this node: 10.3/11.9 GiBUsing FIFO scheduling algorithm.Resources requested: 0/3 CPUs, 0/0 GPUs, 0.0/1.4 GiB heap, 0.0/0.7 GiB objectsResult logdir: C: Users Stefan ray_results PPONumber of trials: 1/1 (1 TERMINATED) Trial name status loc iter total time (s) ts reward episode_reward_max episode_reward_min episode_len_mean . PPO_StatelessCartPole_f6243_00000 | TERMINATED | 127.0.0.1:15776 | 10 | 74.1832 | 40000 | 51.67 | 147 | 16 | 51.67 | . (pid=15776) Windows fatal exception: access violation (pid=15776) (pid=24480) [2021-11-23 19:50:16,710 E 24480 11408] raylet_client.cc:159: IOError: Unknown error [RayletClient] Failed to disconnect from raylet. (pid=16964) [2021-11-23 19:50:16,721 E 16964 23092] raylet_client.cc:159: IOError: Unknown error [RayletClient] Failed to disconnect from raylet. (pid=16964) Windows fatal exception: access violation (pid=16964) 2021-11-23 19:50:16,834 INFO tune.py:630 -- Total run time: 102.31 seconds (101.56 seconds for the tuning loop). . Option 2: Training finished successfully . . results2.default_metric = &quot;episode_reward_mean&quot; results2.default_mode = &quot;max&quot; # print the mean episode reward = episode length --&gt; higher = better results2.best_result[&quot;episode_reward_mean&quot;] . 51.67 . plot_rewards(results2) . c: users stefan git-repos private blog venv lib site-packages seaborn _decorators.py:36: FutureWarning: Pass the following variables as keyword args: x, y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation. warnings.warn( . With only the partial observations, i.e., without observing velocity, the RL agent does not learn a useful policy. The reward does not increase notably over time and the resulting episode reward is rather small. . Option 3: Use Sequence of Last Observations . Even if the velocity of cart and pole are not explicitly available in this example, it can be derived by the RL agent by looking at a sequence of previous observations. If the cart is always at the same position, its velocity is likely close to zero. If its position varies greatly, it likely has high velocity. . Hence, one useful approach is to simply stack the last $n$ observations and providing this sequence as input to the RL agent. . Option 3a: Use Raw Sequence as Input . Here, I consider the same default feed-forward neural network with PPO, just providing the stacked, partial observations as input. . Stacking Observations Using Gym&#39;s FrameStack Wrapper . To stack the last $n$ observations, I use Gym&#39;s FrameStack wrapper. As an example, I choose $n=4$. . from gym.wrappers import FrameStack NUM_FRAMES = 4 # stateless CartPole --&gt; only 2 observations: position of cart &amp; angle of pole (not: velocity of cart or pole) env = StatelessCartPole() print(f&quot;Shape of observation space (stateless CartPole): {env.observation_space.shape}&quot;) # stack last n observations into sequence --&gt; n x 2 env_stacked = FrameStack(env, NUM_FRAMES) print(f&quot;Shape of observation space (stacked stateless CartPole): {env_stacked.observation_space.shape}&quot;) # register env for RLlib registry.register_env(&quot;StackedStatelessCartPole&quot;, lambda _: FrameStack(StatelessCartPole(), NUM_FRAMES)) . Shape of observation space (stateless CartPole): (2,) Shape of observation space (stacked stateless CartPole): (4, 2) . # use PPO with vanilla MLP config3a = ppo.DEFAULT_CONFIG.copy() config3a[&quot;env&quot;] = &quot;StackedStatelessCartPole&quot; # train; this takes a while results3a = ray.tune.run(&quot;PPO&quot;, config=config3a, stop=stop) print(&quot;Option 3a with FrameStack: Training finished successfully&quot;) . == Status == Current time: 2021-11-29 18:20:59 (running for 00:00:00.23) Memory usage on this node: 9.0/11.9 GiB Using FIFO scheduling algorithm. Resources requested: 0/3 CPUs, 0/0 GPUs, 0.0/1.7 GiB heap, 0.0/0.85 GiB objects Result logdir: C: Users Stefan ray_results PPO Number of trials: 1/1 (1 PENDING) ++-+-+ | Trial name | status | loc | |+-+-| | PPO_StackedStatelessCartPole_b85a6_00000 | PENDING | | ++-+-+ . (pid=None) c: users stefan git-repos private blog venv lib site-packages redis connection.py:77: UserWarning: redis-py works best with hiredis. Please consider installing (pid=None) warnings.warn(msg) . == Status == Current time: 2021-11-29 18:21:04 (running for 00:00:05.23) Memory usage on this node: 9.1/11.9 GiB Using FIFO scheduling algorithm. Resources requested: 0/3 CPUs, 0/0 GPUs, 0.0/1.7 GiB heap, 0.0/0.85 GiB objects Result logdir: C: Users Stefan ray_results PPO Number of trials: 1/1 (1 PENDING) ++-+-+ | Trial name | status | loc | |+-+-| | PPO_StackedStatelessCartPole_b85a6_00000 | PENDING | | ++-+-+ . (pid=18488) 2021-11-29 18:21:17,962 INFO trainer.py:753 -- Tip: set framework=tfe or the --eager flag to enable TensorFlow eager execution (pid=18488) 2021-11-29 18:21:17,963 INFO ppo.py:166 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn&#39;t work for you. (pid=18488) 2021-11-29 18:21:17,963 INFO trainer.py:770 -- Current log_level is WARN. For more information, set &#39;log_level&#39;: &#39;INFO&#39; / &#39;DEBUG&#39; or use the -v and -vv flags. (pid=None) c: users stefan git-repos private blog venv lib site-packages redis connection.py:77: UserWarning: redis-py works best with hiredis. Please consider installing (pid=None) warnings.warn(msg) (pid=None) c: users stefan git-repos private blog venv lib site-packages redis connection.py:77: UserWarning: redis-py works best with hiredis. Please consider installing (pid=None) warnings.warn(msg) (pid=15384) c: users stefan git-repos private blog venv lib site-packages gym spaces box.py:142: UserWarning: WARN: Casting input x to numpy array. (pid=15384) logger.warn(&#34;Casting input x to numpy array.&#34;) (pid=14576) c: users stefan git-repos private blog venv lib site-packages gym spaces box.py:142: UserWarning: WARN: Casting input x to numpy array. (pid=14576) logger.warn(&#34;Casting input x to numpy array.&#34;) (pid=14576) 2021-11-29 18:21:35,116 WARNING deprecation.py:38 -- DeprecationWarning: `SampleBatch[&#39;is_training&#39;]` has been deprecated. Use `SampleBatch.is_training` instead. This will raise an error in the future! (pid=18488) 2021-11-29 18:21:38,147 WARNING deprecation.py:38 -- DeprecationWarning: `SampleBatch[&#39;is_training&#39;]` has been deprecated. Use `SampleBatch.is_training` instead. This will raise an error in the future! . == Status == Current time: 2021-11-29 18:21:41 (running for 00:00:42.45) Memory usage on this node: 10.4/11.9 GiB Using FIFO scheduling algorithm. Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.7 GiB heap, 0.0/0.85 GiB objects Result logdir: C: Users Stefan ray_results PPO Number of trials: 1/1 (1 RUNNING) ++-+--+ | Trial name | status | loc | |+-+--| | PPO_StackedStatelessCartPole_b85a6_00000 | RUNNING | 127.0.0.1:18488 | ++-+--+ . (pid=18488) 2021-11-29 18:21:41,850 WARNING trainer_template.py:185 -- `execution_plan` functions should accept `trainer`, `workers`, and `config` as args! (pid=18488) 2021-11-29 18:21:41,850 INFO trainable.py:110 -- Trainable.setup took 23.904 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads. (pid=18488) 2021-11-29 18:21:41,852 WARNING util.py:57 -- Install gputil for GPU system monitoring. . == Status == Current time: 2021-11-29 18:21:43 (running for 00:00:43.77) Memory usage on this node: 10.4/11.9 GiB Using FIFO scheduling algorithm. Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.7 GiB heap, 0.0/0.85 GiB objects Result logdir: C: Users Stefan ray_results PPO Number of trials: 1/1 (1 RUNNING) ++-+--+ | Trial name | status | loc | |+-+--| | PPO_StackedStatelessCartPole_b85a6_00000 | RUNNING | 127.0.0.1:18488 | ++-+--+ == Status == Current time: 2021-11-29 18:21:48 (running for 00:00:48.84) Memory usage on this node: 10.5/11.9 GiB Using FIFO scheduling algorithm. Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.7 GiB heap, 0.0/0.85 GiB objects Result logdir: C: Users Stefan ray_results PPO Number of trials: 1/1 (1 RUNNING) ++-+--+ | Trial name | status | loc | |+-+--| | PPO_StackedStatelessCartPole_b85a6_00000 | RUNNING | 127.0.0.1:18488 | ++-+--+ . (pid=18488) 2021-11-29 18:21:50,403 WARNING deprecation.py:38 -- DeprecationWarning: `slice` has been deprecated. Use `SampleBatch[start:stop]` instead. This will raise an error in the future! . == Status == Current time: 2021-11-29 18:21:53 (running for 00:00:54.04) Memory usage on this node: 10.5/11.9 GiB Using FIFO scheduling algorithm. Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.7 GiB heap, 0.0/0.85 GiB objects Result logdir: C: Users Stefan ray_results PPO Number of trials: 1/1 (1 RUNNING) ++-+--+ | Trial name | status | loc | |+-+--| | PPO_StackedStatelessCartPole_b85a6_00000 | RUNNING | 127.0.0.1:18488 | ++-+--+ Result for PPO_StackedStatelessCartPole_b85a6_00000: agent_timesteps_total: 4000 custom_metrics: {} date: 2021-11-29_18-21-56 done: false episode_len_mean: 23.60355029585799 episode_media: {} episode_reward_max: 88.0 episode_reward_mean: 23.60355029585799 episode_reward_min: 8.0 episodes_this_iter: 169 episodes_total: 169 experiment_id: d9131bf8d3a243fa9abb15b66fa051aa hostname: nb-stschn info: learner: default_policy: custom_metrics: {} learner_stats: cur_kl_coeff: 0.20000000298023224 cur_lr: 4.999999873689376e-05 entropy: 0.6828524470329285 entropy_coeff: 0.0 kl: 0.010669833049178123 model: {} policy_loss: -0.013517408631742 total_loss: 213.69320678710938 vf_explained_var: 0.0019094308372586966 vf_loss: 213.70460510253906 num_agent_steps_sampled: 4000 num_agent_steps_trained: 4000 num_steps_sampled: 4000 num_steps_trained: 4000 iterations_since_restore: 1 node_ip: 127.0.0.1 num_healthy_workers: 2 off_policy_estimator: {} perf: cpu_util_percent: 97.45 ram_util_percent: 88.26500000000001 pid: 18488 policy_reward_max: {} policy_reward_mean: {} policy_reward_min: {} sampler_perf: mean_action_processing_ms: 0.14027429708738465 mean_env_render_ms: 0.0 mean_env_wait_ms: 0.18660353164156432 mean_inference_ms: 3.242485123522411 mean_raw_obs_processing_ms: 0.47340093048834697 time_since_restore: 14.671286582946777 time_this_iter_s: 14.671286582946777 time_total_s: 14.671286582946777 timers: learn_throughput: 648.714 learn_time_ms: 6166.048 load_throughput: 3986033.737 load_time_ms: 1.004 sample_throughput: 467.76 sample_time_ms: 8551.386 update_time_ms: 3.997 timestamp: 1638206516 timesteps_since_restore: 0 timesteps_this_iter: 0 timesteps_total: 4000 training_iteration: 1 trial_id: b85a6_00000 == Status == Current time: 2021-11-29 18:21:58 (running for 00:00:59.19) Memory usage on this node: 10.5/11.9 GiB Using FIFO scheduling algorithm. Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.7 GiB heap, 0.0/0.85 GiB objects Result logdir: C: Users Stefan ray_results PPO Number of trials: 1/1 (1 RUNNING) ++-+--+--+++-+-+-+--+ | Trial name | status | loc | iter | total time (s) | ts | reward | episode_reward_max | episode_reward_min | episode_len_mean | |+-+--+--+++-+-+-+--| | PPO_StackedStatelessCartPole_b85a6_00000 | RUNNING | 127.0.0.1:18488 | 1 | 14.6713 | 4000 | 23.6036 | 88 | 8 | 23.6036 | ++-+--+--+++-+-+-+--+ == Status == Current time: 2021-11-29 18:22:03 (running for 00:01:04.29) Memory usage on this node: 10.5/11.9 GiB Using FIFO scheduling algorithm. Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.7 GiB heap, 0.0/0.85 GiB objects Result logdir: C: Users Stefan ray_results PPO Number of trials: 1/1 (1 RUNNING) ++-+--+--+++-+-+-+--+ | Trial name | status | loc | iter | total time (s) | ts | reward | episode_reward_max | episode_reward_min | episode_len_mean | |+-+--+--+++-+-+-+--| | PPO_StackedStatelessCartPole_b85a6_00000 | RUNNING | 127.0.0.1:18488 | 1 | 14.6713 | 4000 | 23.6036 | 88 | 8 | 23.6036 | ++-+--+--+++-+-+-+--+ Result for PPO_StackedStatelessCartPole_b85a6_00000: agent_timesteps_total: 8000 custom_metrics: {} date: 2021-11-29_18-22-04 done: false episode_len_mean: 26.993197278911566 episode_media: {} episode_reward_max: 198.0 episode_reward_mean: 26.993197278911566 episode_reward_min: 8.0 episodes_this_iter: 147 episodes_total: 316 experiment_id: d9131bf8d3a243fa9abb15b66fa051aa hostname: nb-stschn info: learner: default_policy: custom_metrics: {} learner_stats: cur_kl_coeff: 0.20000000298023224 cur_lr: 4.999999873689376e-05 entropy: 0.653245210647583 entropy_coeff: 0.0 kl: 0.009535541757941246 model: {} policy_loss: -0.014410953968763351 total_loss: 185.83935546875 vf_explained_var: 0.006426886655390263 vf_loss: 185.85186767578125 num_agent_steps_sampled: 8000 num_agent_steps_trained: 8000 num_steps_sampled: 8000 num_steps_trained: 8000 num_steps_trained_this_iter: 0 iterations_since_restore: 2 node_ip: 127.0.0.1 num_healthy_workers: 2 off_policy_estimator: {} perf: cpu_util_percent: 79.76363636363637 ram_util_percent: 88.46363636363637 pid: 18488 policy_reward_max: {} policy_reward_mean: {} policy_reward_min: {} sampler_perf: mean_action_processing_ms: 0.11731217454498617 mean_env_render_ms: 0.0 mean_env_wait_ms: 0.15637328311222465 mean_inference_ms: 2.4253732040536264 mean_raw_obs_processing_ms: 0.35383144962962687 time_since_restore: 22.566290616989136 time_this_iter_s: 7.895004034042358 time_total_s: 22.566290616989136 timers: learn_throughput: 811.931 learn_time_ms: 4926.525 load_throughput: 3996478.323 load_time_ms: 1.001 sample_throughput: 421.564 sample_time_ms: 9488.467 update_time_ms: 3.998 timestamp: 1638206524 timesteps_since_restore: 0 timesteps_this_iter: 0 timesteps_total: 8000 training_iteration: 2 trial_id: b85a6_00000 == Status == Current time: 2021-11-29 18:22:09 (running for 00:01:10.15) Memory usage on this node: 10.5/11.9 GiB Using FIFO scheduling algorithm. Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.7 GiB heap, 0.0/0.85 GiB objects Result logdir: C: Users Stefan ray_results PPO Number of trials: 1/1 (1 RUNNING) ++-+--+--+++-+-+-+--+ | Trial name | status | loc | iter | total time (s) | ts | reward | episode_reward_max | episode_reward_min | episode_len_mean | |+-+--+--+++-+-+-+--| | PPO_StackedStatelessCartPole_b85a6_00000 | RUNNING | 127.0.0.1:18488 | 2 | 22.5663 | 8000 | 26.9932 | 198 | 8 | 26.9932 | ++-+--+--+++-+-+-+--+ Result for PPO_StackedStatelessCartPole_b85a6_00000: agent_timesteps_total: 12000 custom_metrics: {} date: 2021-11-29_18-22-12 done: false episode_len_mean: 36.53211009174312 episode_media: {} episode_reward_max: 145.0 episode_reward_mean: 36.53211009174312 episode_reward_min: 10.0 episodes_this_iter: 109 episodes_total: 425 experiment_id: d9131bf8d3a243fa9abb15b66fa051aa hostname: nb-stschn info: learner: default_policy: custom_metrics: {} learner_stats: cur_kl_coeff: 0.20000000298023224 cur_lr: 4.999999873689376e-05 entropy: 0.6211905479431152 entropy_coeff: 0.0 kl: 0.010481786914169788 model: {} policy_loss: -0.01384791824966669 total_loss: 238.9881591796875 vf_explained_var: 0.04957703500986099 vf_loss: 238.99990844726562 num_agent_steps_sampled: 12000 num_agent_steps_trained: 12000 num_steps_sampled: 12000 num_steps_trained: 12000 num_steps_trained_this_iter: 0 iterations_since_restore: 3 node_ip: 127.0.0.1 num_healthy_workers: 2 off_policy_estimator: {} perf: cpu_util_percent: 73.74545454545455 ram_util_percent: 88.35454545454544 pid: 18488 policy_reward_max: {} policy_reward_mean: {} policy_reward_min: {} sampler_perf: mean_action_processing_ms: 0.10965367727927192 mean_env_render_ms: 0.0 mean_env_wait_ms: 0.14942419615546643 mean_inference_ms: 2.1387261502336283 mean_raw_obs_processing_ms: 0.3099031695301418 time_since_restore: 30.253284692764282 time_this_iter_s: 7.6869940757751465 time_total_s: 30.253284692764282 timers: learn_throughput: 893.518 learn_time_ms: 4476.685 load_throughput: 5994717.485 load_time_ms: 0.667 sample_throughput: 447.37 sample_time_ms: 8941.15 update_time_ms: 3.664 timestamp: 1638206532 timesteps_since_restore: 0 timesteps_this_iter: 0 timesteps_total: 12000 training_iteration: 3 trial_id: b85a6_00000 == Status == Current time: 2021-11-29 18:22:15 (running for 00:01:15.88) Memory usage on this node: 10.4/11.9 GiB Using FIFO scheduling algorithm. Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.7 GiB heap, 0.0/0.85 GiB objects Result logdir: C: Users Stefan ray_results PPO Number of trials: 1/1 (1 RUNNING) ++-+--+--++-+-+-+-+--+ | Trial name | status | loc | iter | total time (s) | ts | reward | episode_reward_max | episode_reward_min | episode_len_mean | |+-+--+--++-+-+-+-+--| | PPO_StackedStatelessCartPole_b85a6_00000 | RUNNING | 127.0.0.1:18488 | 3 | 30.2533 | 12000 | 36.5321 | 145 | 10 | 36.5321 | ++-+--+--++-+-+-+-+--+ == Status == Current time: 2021-11-29 18:22:20 (running for 00:01:20.93) Memory usage on this node: 10.4/11.9 GiB Using FIFO scheduling algorithm. Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.7 GiB heap, 0.0/0.85 GiB objects Result logdir: C: Users Stefan ray_results PPO Number of trials: 1/1 (1 RUNNING) ++-+--+--++-+-+-+-+--+ | Trial name | status | loc | iter | total time (s) | ts | reward | episode_reward_max | episode_reward_min | episode_len_mean | |+-+--+--++-+-+-+-+--| | PPO_StackedStatelessCartPole_b85a6_00000 | RUNNING | 127.0.0.1:18488 | 3 | 30.2533 | 12000 | 36.5321 | 145 | 10 | 36.5321 | ++-+--+--++-+-+-+-+--+ Result for PPO_StackedStatelessCartPole_b85a6_00000: agent_timesteps_total: 16000 custom_metrics: {} date: 2021-11-29_18-22-20 done: false episode_len_mean: 51.49 episode_media: {} episode_reward_max: 210.0 episode_reward_mean: 51.49 episode_reward_min: 9.0 episodes_this_iter: 64 episodes_total: 489 experiment_id: d9131bf8d3a243fa9abb15b66fa051aa hostname: nb-stschn info: learner: default_policy: custom_metrics: {} learner_stats: cur_kl_coeff: 0.20000000298023224 cur_lr: 4.999999873689376e-05 entropy: 0.6035293936729431 entropy_coeff: 0.0 kl: 0.008739106357097626 model: {} policy_loss: -0.011937112547457218 total_loss: 373.0289001464844 vf_explained_var: 0.08353171497583389 vf_loss: 373.03912353515625 num_agent_steps_sampled: 16000 num_agent_steps_trained: 16000 num_steps_sampled: 16000 num_steps_trained: 16000 num_steps_trained_this_iter: 0 iterations_since_restore: 4 node_ip: 127.0.0.1 num_healthy_workers: 2 off_policy_estimator: {} perf: cpu_util_percent: 81.26666666666668 ram_util_percent: 87.64166666666667 pid: 18488 policy_reward_max: {} policy_reward_mean: {} policy_reward_min: {} sampler_perf: mean_action_processing_ms: 0.112593405657435 mean_env_render_ms: 0.0 mean_env_wait_ms: 0.1450739126863283 mean_inference_ms: 2.064151207268679 mean_raw_obs_processing_ms: 0.2833542128760493 time_since_restore: 38.4389169216156 time_this_iter_s: 8.185632228851318 time_total_s: 38.4389169216156 timers: learn_throughput: 923.591 learn_time_ms: 4330.922 load_throughput: 5335416.123 load_time_ms: 0.75 sample_throughput: 460.462 sample_time_ms: 8686.925 update_time_ms: 3.748 timestamp: 1638206540 timesteps_since_restore: 0 timesteps_this_iter: 0 timesteps_total: 16000 training_iteration: 4 trial_id: b85a6_00000 == Status == Current time: 2021-11-29 18:22:26 (running for 00:01:26.58) Memory usage on this node: 10.5/11.9 GiB Using FIFO scheduling algorithm. Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.7 GiB heap, 0.0/0.85 GiB objects Result logdir: C: Users Stefan ray_results PPO Number of trials: 1/1 (1 RUNNING) ++-+--+--++-+-+-+-+--+ | Trial name | status | loc | iter | total time (s) | ts | reward | episode_reward_max | episode_reward_min | episode_len_mean | |+-+--+--++-+-+-+-+--| | PPO_StackedStatelessCartPole_b85a6_00000 | RUNNING | 127.0.0.1:18488 | 4 | 38.4389 | 16000 | 51.49 | 210 | 9 | 51.49 | ++-+--+--++-+-+-+-+--+ == Status == Current time: 2021-11-29 18:22:31 (running for 00:01:31.70) Memory usage on this node: 10.4/11.9 GiB Using FIFO scheduling algorithm. Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.7 GiB heap, 0.0/0.85 GiB objects Result logdir: C: Users Stefan ray_results PPO Number of trials: 1/1 (1 RUNNING) ++-+--+--++-+-+-+-+--+ | Trial name | status | loc | iter | total time (s) | ts | reward | episode_reward_max | episode_reward_min | episode_len_mean | |+-+--+--++-+-+-+-+--| | PPO_StackedStatelessCartPole_b85a6_00000 | RUNNING | 127.0.0.1:18488 | 4 | 38.4389 | 16000 | 51.49 | 210 | 9 | 51.49 | ++-+--+--++-+-+-+-+--+ Result for PPO_StackedStatelessCartPole_b85a6_00000: agent_timesteps_total: 20000 custom_metrics: {} date: 2021-11-29_18-22-36 done: false episode_len_mean: 73.19 episode_media: {} episode_reward_max: 287.0 episode_reward_mean: 73.19 episode_reward_min: 9.0 episodes_this_iter: 45 episodes_total: 534 experiment_id: d9131bf8d3a243fa9abb15b66fa051aa hostname: nb-stschn info: learner: default_policy: custom_metrics: {} learner_stats: cur_kl_coeff: 0.20000000298023224 cur_lr: 4.999999873689376e-05 entropy: 0.5757359862327576 entropy_coeff: 0.0 kl: 0.006448619533330202 model: {} policy_loss: -0.009859011508524418 total_loss: 474.3692932128906 vf_explained_var: 0.13715480268001556 vf_loss: 474.37786865234375 num_agent_steps_sampled: 20000 num_agent_steps_trained: 20000 num_steps_sampled: 20000 num_steps_trained: 20000 num_steps_trained_this_iter: 0 iterations_since_restore: 5 node_ip: 127.0.0.1 num_healthy_workers: 2 off_policy_estimator: {} perf: cpu_util_percent: 98.22000000000001 ram_util_percent: 87.96499999999999 pid: 18488 policy_reward_max: {} policy_reward_mean: {} policy_reward_min: {} sampler_perf: mean_action_processing_ms: 0.11565990497898916 mean_env_render_ms: 0.0 mean_env_wait_ms: 0.14905431684457787 mean_inference_ms: 2.1448950162700857 mean_raw_obs_processing_ms: 0.29478722227711357 time_since_restore: 53.9137442111969 time_this_iter_s: 15.474827289581299 time_total_s: 53.9137442111969 timers: learn_throughput: 840.68 learn_time_ms: 4758.054 load_throughput: 4984318.479 load_time_ms: 0.803 sample_throughput: 419.477 sample_time_ms: 9535.687 update_time_ms: 2.999 timestamp: 1638206556 timesteps_since_restore: 0 timesteps_this_iter: 0 timesteps_total: 20000 training_iteration: 5 trial_id: b85a6_00000 == Status == Current time: 2021-11-29 18:22:37 (running for 00:01:37.64) Memory usage on this node: 10.5/11.9 GiB Using FIFO scheduling algorithm. Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.7 GiB heap, 0.0/0.85 GiB objects Result logdir: C: Users Stefan ray_results PPO Number of trials: 1/1 (1 RUNNING) ++-+--+--++-+-+-+-+--+ | Trial name | status | loc | iter | total time (s) | ts | reward | episode_reward_max | episode_reward_min | episode_len_mean | |+-+--+--++-+-+-+-+--| | PPO_StackedStatelessCartPole_b85a6_00000 | RUNNING | 127.0.0.1:18488 | 5 | 53.9137 | 20000 | 73.19 | 287 | 9 | 73.19 | ++-+--+--++-+-+-+-+--+ == Status == Current time: 2021-11-29 18:22:42 (running for 00:01:42.73) Memory usage on this node: 10.5/11.9 GiB Using FIFO scheduling algorithm. Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.7 GiB heap, 0.0/0.85 GiB objects Result logdir: C: Users Stefan ray_results PPO Number of trials: 1/1 (1 RUNNING) ++-+--+--++-+-+-+-+--+ | Trial name | status | loc | iter | total time (s) | ts | reward | episode_reward_max | episode_reward_min | episode_len_mean | |+-+--+--++-+-+-+-+--| | PPO_StackedStatelessCartPole_b85a6_00000 | RUNNING | 127.0.0.1:18488 | 5 | 53.9137 | 20000 | 73.19 | 287 | 9 | 73.19 | ++-+--+--++-+-+-+-+--+ Result for PPO_StackedStatelessCartPole_b85a6_00000: agent_timesteps_total: 24000 custom_metrics: {} date: 2021-11-29_18-22-45 done: false episode_len_mean: 94.91 episode_media: {} episode_reward_max: 341.0 episode_reward_mean: 94.91 episode_reward_min: 9.0 episodes_this_iter: 29 episodes_total: 563 experiment_id: d9131bf8d3a243fa9abb15b66fa051aa hostname: nb-stschn info: learner: default_policy: custom_metrics: {} learner_stats: cur_kl_coeff: 0.20000000298023224 cur_lr: 4.999999873689376e-05 entropy: 0.5766456127166748 entropy_coeff: 0.0 kl: 0.010011927224695683 model: {} policy_loss: -0.013857519254088402 total_loss: 408.69921875 vf_explained_var: 0.3312024772167206 vf_loss: 408.7110900878906 num_agent_steps_sampled: 24000 num_agent_steps_trained: 24000 num_steps_sampled: 24000 num_steps_trained: 24000 num_steps_trained_this_iter: 0 iterations_since_restore: 6 node_ip: 127.0.0.1 num_healthy_workers: 2 off_policy_estimator: {} perf: cpu_util_percent: 89.0153846153846 ram_util_percent: 88.1846153846154 pid: 18488 policy_reward_max: {} policy_reward_mean: {} policy_reward_min: {} sampler_perf: mean_action_processing_ms: 0.11845266239030193 mean_env_render_ms: 0.0 mean_env_wait_ms: 0.15245982835362226 mean_inference_ms: 2.230100303885492 mean_raw_obs_processing_ms: 0.2962264701555328 time_since_restore: 63.56351852416992 time_this_iter_s: 9.649774312973022 time_total_s: 63.56351852416992 timers: learn_throughput: 869.45 learn_time_ms: 4600.609 load_throughput: 5981182.175 load_time_ms: 0.669 sample_throughput: 399.538 sample_time_ms: 10011.565 update_time_ms: 2.499 timestamp: 1638206565 timesteps_since_restore: 0 timesteps_this_iter: 0 timesteps_total: 24000 training_iteration: 6 trial_id: b85a6_00000 == Status == Current time: 2021-11-29 18:22:47 (running for 00:01:48.35) Memory usage on this node: 10.5/11.9 GiB Using FIFO scheduling algorithm. Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.7 GiB heap, 0.0/0.85 GiB objects Result logdir: C: Users Stefan ray_results PPO Number of trials: 1/1 (1 RUNNING) ++-+--+--++-+-+-+-+--+ | Trial name | status | loc | iter | total time (s) | ts | reward | episode_reward_max | episode_reward_min | episode_len_mean | |+-+--+--++-+-+-+-+--| | PPO_StackedStatelessCartPole_b85a6_00000 | RUNNING | 127.0.0.1:18488 | 6 | 63.5635 | 24000 | 94.91 | 341 | 9 | 94.91 | ++-+--+--++-+-+-+-+--+ == Status == Current time: 2021-11-29 18:22:52 (running for 00:01:53.43) Memory usage on this node: 10.5/11.9 GiB Using FIFO scheduling algorithm. Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.7 GiB heap, 0.0/0.85 GiB objects Result logdir: C: Users Stefan ray_results PPO Number of trials: 1/1 (1 RUNNING) ++-+--+--++-+-+-+-+--+ | Trial name | status | loc | iter | total time (s) | ts | reward | episode_reward_max | episode_reward_min | episode_len_mean | |+-+--+--++-+-+-+-+--| | PPO_StackedStatelessCartPole_b85a6_00000 | RUNNING | 127.0.0.1:18488 | 6 | 63.5635 | 24000 | 94.91 | 341 | 9 | 94.91 | ++-+--+--++-+-+-+-+--+ Result for PPO_StackedStatelessCartPole_b85a6_00000: agent_timesteps_total: 28000 custom_metrics: {} date: 2021-11-29_18-22-54 done: false episode_len_mean: 121.97 episode_media: {} episode_reward_max: 614.0 episode_reward_mean: 121.97 episode_reward_min: 11.0 episodes_this_iter: 19 episodes_total: 582 experiment_id: d9131bf8d3a243fa9abb15b66fa051aa hostname: nb-stschn info: learner: default_policy: custom_metrics: {} learner_stats: cur_kl_coeff: 0.20000000298023224 cur_lr: 4.999999873689376e-05 entropy: 0.5524585843086243 entropy_coeff: 0.0 kl: 0.008470031432807446 model: {} policy_loss: -0.009639520198106766 total_loss: 498.4787902832031 vf_explained_var: 0.29663658142089844 vf_loss: 498.4867858886719 num_agent_steps_sampled: 28000 num_agent_steps_trained: 28000 num_steps_sampled: 28000 num_steps_trained: 28000 num_steps_trained_this_iter: 0 iterations_since_restore: 7 node_ip: 127.0.0.1 num_healthy_workers: 2 off_policy_estimator: {} perf: cpu_util_percent: 82.06923076923077 ram_util_percent: 88.13076923076925 pid: 18488 policy_reward_max: {} policy_reward_mean: {} policy_reward_min: {} sampler_perf: mean_action_processing_ms: 0.11931244700954885 mean_env_render_ms: 0.0 mean_env_wait_ms: 0.15571075489876104 mean_inference_ms: 2.265673144307002 mean_raw_obs_processing_ms: 0.3031391507153599 time_since_restore: 72.76346039772034 time_this_iter_s: 9.199941873550415 time_total_s: 72.76346039772034 timers: learn_throughput: 870.115 learn_time_ms: 4597.092 load_throughput: 6978045.87 load_time_ms: 0.573 sample_throughput: 408.537 sample_time_ms: 9791.039 update_time_ms: 3.143 timestamp: 1638206574 timesteps_since_restore: 0 timesteps_this_iter: 0 timesteps_total: 28000 training_iteration: 7 trial_id: b85a6_00000 == Status == Current time: 2021-11-29 18:22:58 (running for 00:01:58.61) Memory usage on this node: 10.5/11.9 GiB Using FIFO scheduling algorithm. Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.7 GiB heap, 0.0/0.85 GiB objects Result logdir: C: Users Stefan ray_results PPO Number of trials: 1/1 (1 RUNNING) ++-+--+--++-+-+-+-+--+ | Trial name | status | loc | iter | total time (s) | ts | reward | episode_reward_max | episode_reward_min | episode_len_mean | |+-+--+--++-+-+-+-+--| | PPO_StackedStatelessCartPole_b85a6_00000 | RUNNING | 127.0.0.1:18488 | 7 | 72.7635 | 28000 | 121.97 | 614 | 11 | 121.97 | ++-+--+--++-+-+-+-+--+ == Status == Current time: 2021-11-29 18:23:03 (running for 00:02:03.80) Memory usage on this node: 10.5/11.9 GiB Using FIFO scheduling algorithm. Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.7 GiB heap, 0.0/0.85 GiB objects Result logdir: C: Users Stefan ray_results PPO Number of trials: 1/1 (1 RUNNING) ++-+--+--++-+-+-+-+--+ | Trial name | status | loc | iter | total time (s) | ts | reward | episode_reward_max | episode_reward_min | episode_len_mean | |+-+--+--++-+-+-+-+--| | PPO_StackedStatelessCartPole_b85a6_00000 | RUNNING | 127.0.0.1:18488 | 7 | 72.7635 | 28000 | 121.97 | 614 | 11 | 121.97 | ++-+--+--++-+-+-+-+--+ Result for PPO_StackedStatelessCartPole_b85a6_00000: agent_timesteps_total: 32000 custom_metrics: {} date: 2021-11-29_18-23-05 done: false episode_len_mean: 149.38 episode_media: {} episode_reward_max: 707.0 episode_reward_mean: 149.38 episode_reward_min: 11.0 episodes_this_iter: 21 episodes_total: 603 experiment_id: d9131bf8d3a243fa9abb15b66fa051aa hostname: nb-stschn info: learner: default_policy: custom_metrics: {} learner_stats: cur_kl_coeff: 0.20000000298023224 cur_lr: 4.999999873689376e-05 entropy: 0.5577117204666138 entropy_coeff: 0.0 kl: 0.009962840005755424 model: {} policy_loss: -0.010336116887629032 total_loss: 419.3426208496094 vf_explained_var: 0.34807512164115906 vf_loss: 419.3509521484375 num_agent_steps_sampled: 32000 num_agent_steps_trained: 32000 num_steps_sampled: 32000 num_steps_trained: 32000 num_steps_trained_this_iter: 0 iterations_since_restore: 8 node_ip: 127.0.0.1 num_healthy_workers: 2 off_policy_estimator: {} perf: cpu_util_percent: 88.24 ram_util_percent: 87.92000000000002 pid: 18488 policy_reward_max: {} policy_reward_mean: {} policy_reward_min: {} sampler_perf: mean_action_processing_ms: 0.12033684006932881 mean_env_render_ms: 0.0 mean_env_wait_ms: 0.1570082948431747 mean_inference_ms: 2.2636946383195324 mean_raw_obs_processing_ms: 0.3007474841886399 time_since_restore: 83.61372876167297 time_this_iter_s: 10.850268363952637 time_total_s: 83.61372876167297 timers: learn_throughput: 846.018 learn_time_ms: 4728.031 load_throughput: 7974909.566 load_time_ms: 0.502 sample_throughput: 408.341 sample_time_ms: 9795.741 update_time_ms: 2.751 timestamp: 1638206585 timesteps_since_restore: 0 timesteps_this_iter: 0 timesteps_total: 32000 training_iteration: 8 trial_id: b85a6_00000 == Status == Current time: 2021-11-29 18:23:09 (running for 00:02:09.63) Memory usage on this node: 10.5/11.9 GiB Using FIFO scheduling algorithm. Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.7 GiB heap, 0.0/0.85 GiB objects Result logdir: C: Users Stefan ray_results PPO Number of trials: 1/1 (1 RUNNING) ++-+--+--++-+-+-+-+--+ | Trial name | status | loc | iter | total time (s) | ts | reward | episode_reward_max | episode_reward_min | episode_len_mean | |+-+--+--++-+-+-+-+--| | PPO_StackedStatelessCartPole_b85a6_00000 | RUNNING | 127.0.0.1:18488 | 8 | 83.6137 | 32000 | 149.38 | 707 | 11 | 149.38 | ++-+--+--++-+-+-+-+--+ == Status == Current time: 2021-11-29 18:23:14 (running for 00:02:14.71) Memory usage on this node: 10.5/11.9 GiB Using FIFO scheduling algorithm. Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.7 GiB heap, 0.0/0.85 GiB objects Result logdir: C: Users Stefan ray_results PPO Number of trials: 1/1 (1 RUNNING) ++-+--+--++-+-+-+-+--+ | Trial name | status | loc | iter | total time (s) | ts | reward | episode_reward_max | episode_reward_min | episode_len_mean | |+-+--+--++-+-+-+-+--| | PPO_StackedStatelessCartPole_b85a6_00000 | RUNNING | 127.0.0.1:18488 | 8 | 83.6137 | 32000 | 149.38 | 707 | 11 | 149.38 | ++-+--+--++-+-+-+-+--+ Result for PPO_StackedStatelessCartPole_b85a6_00000: agent_timesteps_total: 36000 custom_metrics: {} date: 2021-11-29_18-23-14 done: false episode_len_mean: 166.16 episode_media: {} episode_reward_max: 707.0 episode_reward_mean: 166.16 episode_reward_min: 13.0 episodes_this_iter: 21 episodes_total: 624 experiment_id: d9131bf8d3a243fa9abb15b66fa051aa hostname: nb-stschn info: learner: default_policy: custom_metrics: {} learner_stats: cur_kl_coeff: 0.20000000298023224 cur_lr: 4.999999873689376e-05 entropy: 0.53549724817276 entropy_coeff: 0.0 kl: 0.004821674432605505 model: {} policy_loss: -0.00641833059489727 total_loss: 397.94146728515625 vf_explained_var: 0.41243332624435425 vf_loss: 397.9468994140625 num_agent_steps_sampled: 36000 num_agent_steps_trained: 36000 num_steps_sampled: 36000 num_steps_trained: 36000 num_steps_trained_this_iter: 0 iterations_since_restore: 9 node_ip: 127.0.0.1 num_healthy_workers: 2 off_policy_estimator: {} perf: cpu_util_percent: 82.04166666666667 ram_util_percent: 88.0 pid: 18488 policy_reward_max: {} policy_reward_mean: {} policy_reward_min: {} sampler_perf: mean_action_processing_ms: 0.12034288222491363 mean_env_render_ms: 0.0 mean_env_wait_ms: 0.15609246196123902 mean_inference_ms: 2.2279352502617815 mean_raw_obs_processing_ms: 0.29632910330651585 time_since_restore: 92.33211302757263 time_this_iter_s: 8.718384265899658 time_total_s: 92.33211302757263 timers: learn_throughput: 858.255 learn_time_ms: 4660.622 load_throughput: 8971773.262 load_time_ms: 0.446 sample_throughput: 405.511 sample_time_ms: 9864.099 update_time_ms: 2.89 timestamp: 1638206594 timesteps_since_restore: 0 timesteps_this_iter: 0 timesteps_total: 36000 training_iteration: 9 trial_id: b85a6_00000 == Status == Current time: 2021-11-29 18:23:20 (running for 00:02:20.63) Memory usage on this node: 10.5/11.9 GiB Using FIFO scheduling algorithm. Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.7 GiB heap, 0.0/0.85 GiB objects Result logdir: C: Users Stefan ray_results PPO Number of trials: 1/1 (1 RUNNING) ++-+--+--++-+-+-+-+--+ | Trial name | status | loc | iter | total time (s) | ts | reward | episode_reward_max | episode_reward_min | episode_len_mean | |+-+--+--++-+-+-+-+--| | PPO_StackedStatelessCartPole_b85a6_00000 | RUNNING | 127.0.0.1:18488 | 9 | 92.3321 | 36000 | 166.16 | 707 | 13 | 166.16 | ++-+--+--++-+-+-+-+--+ == Status == Current time: 2021-11-29 18:23:25 (running for 00:02:25.80) Memory usage on this node: 10.5/11.9 GiB Using FIFO scheduling algorithm. Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.7 GiB heap, 0.0/0.85 GiB objects Result logdir: C: Users Stefan ray_results PPO Number of trials: 1/1 (1 RUNNING) ++-+--+--++-+-+-+-+--+ | Trial name | status | loc | iter | total time (s) | ts | reward | episode_reward_max | episode_reward_min | episode_len_mean | |+-+--+--++-+-+-+-+--| | PPO_StackedStatelessCartPole_b85a6_00000 | RUNNING | 127.0.0.1:18488 | 9 | 92.3321 | 36000 | 166.16 | 707 | 13 | 166.16 | ++-+--+--++-+-+-+-+--+ Result for PPO_StackedStatelessCartPole_b85a6_00000: agent_timesteps_total: 40000 custom_metrics: {} date: 2021-11-29_18-23-26 done: true episode_len_mean: 185.29 episode_media: {} episode_reward_max: 707.0 episode_reward_mean: 185.29 episode_reward_min: 13.0 episodes_this_iter: 15 episodes_total: 639 experiment_id: d9131bf8d3a243fa9abb15b66fa051aa hostname: nb-stschn info: learner: default_policy: custom_metrics: {} learner_stats: cur_kl_coeff: 0.10000000149011612 cur_lr: 4.999999873689376e-05 entropy: 0.5337437391281128 entropy_coeff: 0.0 kl: 0.006424238905310631 model: {} policy_loss: -0.01005564909428358 total_loss: 314.1622314453125 vf_explained_var: 0.4722842574119568 vf_loss: 314.171630859375 num_agent_steps_sampled: 40000 num_agent_steps_trained: 40000 num_steps_sampled: 40000 num_steps_trained: 40000 num_steps_trained_this_iter: 0 iterations_since_restore: 10 node_ip: 127.0.0.1 num_healthy_workers: 2 off_policy_estimator: {} perf: cpu_util_percent: 89.4 ram_util_percent: 88.025 pid: 18488 policy_reward_max: {} policy_reward_mean: {} policy_reward_min: {} sampler_perf: mean_action_processing_ms: 0.12014477582554012 mean_env_render_ms: 0.0 mean_env_wait_ms: 0.15558315307542722 mean_inference_ms: 2.196684279841281 mean_raw_obs_processing_ms: 0.29364260184200963 time_since_restore: 103.67561793327332 time_this_iter_s: 11.343504905700684 time_total_s: 103.67561793327332 timers: learn_throughput: 823.975 learn_time_ms: 4854.515 load_throughput: 9968636.958 load_time_ms: 0.401 sample_throughput: 409.397 sample_time_ms: 9770.479 update_time_ms: 3.0 timestamp: 1638206606 timesteps_since_restore: 0 timesteps_this_iter: 0 timesteps_total: 40000 training_iteration: 10 trial_id: b85a6_00000 == Status == Current time: 2021-11-29 18:23:26 (running for 00:02:26.80) Memory usage on this node: 10.5/11.9 GiB Using FIFO scheduling algorithm. Resources requested: 0/3 CPUs, 0/0 GPUs, 0.0/1.7 GiB heap, 0.0/0.85 GiB objects Result logdir: C: Users Stefan ray_results PPO Number of trials: 1/1 (1 TERMINATED) +++--+--++-+-+-+-+--+ | Trial name | status | loc | iter | total time (s) | ts | reward | episode_reward_max | episode_reward_min | episode_len_mean | |++--+--++-+-+-+-+--| | PPO_StackedStatelessCartPole_b85a6_00000 | TERMINATED | 127.0.0.1:18488 | 10 | 103.676 | 40000 | 185.29 | 707 | 13 | 185.29 | +++--+--++-+-+-+-+--+ . (pid=18488) [2021-11-29 18:23:26,907 E 18488 22476] raylet_client.cc:159: IOError: Unknown error [RayletClient] Failed to disconnect from raylet. (pid=15384) [2021-11-29 18:23:26,906 E 15384 11108] raylet_client.cc:159: IOError: Unknown error [RayletClient] Failed to disconnect from raylet. (pid=15384) Windows fatal exception: access violation (pid=15384) (pid=14576) [2021-11-29 18:23:26,903 E 14576 5636] raylet_client.cc:159: IOError: Unknown error [RayletClient] Failed to disconnect from raylet. (pid=14576) Windows fatal exception: access violation (pid=14576) (pid=18488) Windows fatal exception: access violation (pid=18488) 2021-11-29 18:23:27,018 INFO tune.py:630 -- Total run time: 147.68 seconds (146.73 seconds for the tuning loop). . Option 3a with FrameStack: Training finished successfully . . results3a.default_metric = &quot;episode_reward_mean&quot; results3a.default_mode = &quot;max&quot; # print the mean episode reward = episode length --&gt; higher = better results3a.best_result[&quot;episode_reward_mean&quot;] . 185.29 . plot_rewards(results3a) . c: users stefan git-repos private blog venv lib site-packages seaborn _decorators.py:36: FutureWarning: Pass the following variables as keyword args: x, y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation. warnings.warn( . Simply by stacking the last $n$ observations, the RL agent learns a useful policy again - even though each observation is still partial, i.e., missing the cart and pole velocity. . Stacking Observations Using RLlib&#39;s Trajectory API . Above, I used Gym&#39;s FrameStack wrapper to stack the last $n$ observations inside the environment. Alternatively, the stacking can be implemented on the model side, e.g., using RLlib&#39;s trajectory API, which reduces space complexity for storing the stacked observations but should lead to similar results. . from ray.rllib.examples.models.trajectory_view_utilizing_models import FrameStackingCartPoleModel from ray.rllib.models.catalog import ModelCatalog ModelCatalog.register_custom_model(&quot;stacking_model&quot;, FrameStackingCartPoleModel) config3a2 = ppo.DEFAULT_CONFIG.copy() config3a2[&quot;env&quot;] = &quot;StatelessCartPole&quot; config3a2[&quot;model&quot;] = { &quot;custom_model&quot;: &quot;stacking_model&quot;, &quot;custom_model_config&quot;: { &quot;num_frames&quot;: NUM_FRAMES, } } results3a2 = ray.tune.run(&quot;PPO&quot;, config=config3a2, stop=stop) print(&quot;Option 3a2 with Trajectory API: Training finished successfully&quot;) . == Status == Current time: 2021-11-29 18:23:27 (running for 00:00:00.16) Memory usage on this node: 9.6/11.9 GiB Using FIFO scheduling algorithm. Resources requested: 0/3 CPUs, 0/0 GPUs, 0.0/1.7 GiB heap, 0.0/0.85 GiB objects Result logdir: C: Users Stefan ray_results PPO Number of trials: 1/1 (1 PENDING) +--+-+-+ | Trial name | status | loc | |--+-+-| | PPO_StatelessCartPole_10a86_00000 | PENDING | | +--+-+-+ . (pid=None) c: users stefan git-repos private blog venv lib site-packages redis connection.py:77: UserWarning: redis-py works best with hiredis. Please consider installing (pid=None) warnings.warn(msg) . == Status == Current time: 2021-11-29 18:23:32 (running for 00:00:05.16) Memory usage on this node: 9.7/11.9 GiB Using FIFO scheduling algorithm. Resources requested: 0/3 CPUs, 0/0 GPUs, 0.0/1.7 GiB heap, 0.0/0.85 GiB objects Result logdir: C: Users Stefan ray_results PPO Number of trials: 1/1 (1 PENDING) +--+-+-+ | Trial name | status | loc | |--+-+-| | PPO_StatelessCartPole_10a86_00000 | PENDING | | +--+-+-+ . (pid=3524) 2021-11-29 18:23:42,046 INFO trainer.py:753 -- Tip: set framework=tfe or the --eager flag to enable TensorFlow eager execution (pid=3524) 2021-11-29 18:23:42,046 INFO ppo.py:166 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn&#39;t work for you. (pid=3524) 2021-11-29 18:23:42,046 INFO trainer.py:770 -- Current log_level is WARN. For more information, set &#39;log_level&#39;: &#39;INFO&#39; / &#39;DEBUG&#39; or use the -v and -vv flags. (pid=None) c: users stefan git-repos private blog venv lib site-packages redis connection.py:77: UserWarning: redis-py works best with hiredis. Please consider installing (pid=None) warnings.warn(msg) (pid=None) c: users stefan git-repos private blog venv lib site-packages redis connection.py:77: UserWarning: redis-py works best with hiredis. Please consider installing (pid=None) warnings.warn(msg) (pid=23440) 2021-11-29 18:23:54,846 WARNING deprecation.py:38 -- DeprecationWarning: `SampleBatch[&#39;is_training&#39;]` has been deprecated. Use `SampleBatch.is_training` instead. This will raise an error in the future! (pid=3524) 2021-11-29 18:23:57,334 WARNING deprecation.py:38 -- DeprecationWarning: `SampleBatch[&#39;is_training&#39;]` has been deprecated. Use `SampleBatch.is_training` instead. This will raise an error in the future! . == Status == Current time: 2021-11-29 18:23:59 (running for 00:00:31.72) Memory usage on this node: 10.5/11.9 GiB Using FIFO scheduling algorithm. Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.7 GiB heap, 0.0/0.85 GiB objects Result logdir: C: Users Stefan ray_results PPO Number of trials: 1/1 (1 RUNNING) +--+-+-+ | Trial name | status | loc | |--+-+-| | PPO_StatelessCartPole_10a86_00000 | RUNNING | 127.0.0.1:3524 | +--+-+-+ . (pid=3524) 2021-11-29 18:23:59,330 WARNING trainer_template.py:185 -- `execution_plan` functions should accept `trainer`, `workers`, and `config` as args! (pid=3524) 2021-11-29 18:23:59,330 INFO trainable.py:110 -- Trainable.setup took 17.296 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads. (pid=3524) 2021-11-29 18:23:59,330 WARNING util.py:57 -- Install gputil for GPU system monitoring. . == Status == Current time: 2021-11-29 18:24:00 (running for 00:00:32.74) Memory usage on this node: 10.5/11.9 GiB Using FIFO scheduling algorithm. Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.7 GiB heap, 0.0/0.85 GiB objects Result logdir: C: Users Stefan ray_results PPO Number of trials: 1/1 (1 RUNNING) +--+-+-+ | Trial name | status | loc | |--+-+-| | PPO_StatelessCartPole_10a86_00000 | RUNNING | 127.0.0.1:3524 | +--+-+-+ . (pid=3524) 2021-11-29 18:24:03,613 WARNING deprecation.py:38 -- DeprecationWarning: `slice` has been deprecated. Use `SampleBatch[start:stop]` instead. This will raise an error in the future! . == Status == Current time: 2021-11-29 18:24:06 (running for 00:00:38.79) Memory usage on this node: 10.5/11.9 GiB Using FIFO scheduling algorithm. Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.7 GiB heap, 0.0/0.85 GiB objects Result logdir: C: Users Stefan ray_results PPO Number of trials: 1/1 (1 RUNNING) +--+-+-+ | Trial name | status | loc | |--+-+-| | PPO_StatelessCartPole_10a86_00000 | RUNNING | 127.0.0.1:3524 | +--+-+-+ Result for PPO_StatelessCartPole_10a86_00000: agent_timesteps_total: 4000 custom_metrics: {} date: 2021-11-29_18-24-06 done: false episode_len_mean: 19.265700483091788 episode_media: {} episode_reward_max: 47.0 episode_reward_mean: 19.265700483091788 episode_reward_min: 8.0 episodes_this_iter: 207 episodes_total: 207 experiment_id: 7d4c24fff65e435bbd107e9f1ece5775 hostname: nb-stschn info: learner: default_policy: custom_metrics: {} learner_stats: cur_kl_coeff: 0.20000000298023224 cur_lr: 4.999999873689376e-05 entropy: 0.6886222958564758 entropy_coeff: 0.0 kl: 0.021595144644379616 model: {} policy_loss: -0.020869048312306404 total_loss: 111.06871795654297 vf_explained_var: -0.059419725090265274 vf_loss: 111.08526611328125 num_agent_steps_sampled: 4000 num_agent_steps_trained: 4000 num_steps_sampled: 4000 num_steps_trained: 4000 iterations_since_restore: 1 node_ip: 127.0.0.1 num_healthy_workers: 2 off_policy_estimator: {} perf: cpu_util_percent: 62.372727272727275 ram_util_percent: 88.51818181818183 pid: 3524 policy_reward_max: {} policy_reward_mean: {} policy_reward_min: {} sampler_perf: mean_action_processing_ms: 0.07998385656388432 mean_env_render_ms: 0.0 mean_env_wait_ms: 0.14542296758373688 mean_inference_ms: 1.61667758007047 mean_raw_obs_processing_ms: 0.22882153697222535 time_since_restore: 7.610152244567871 time_this_iter_s: 7.610152244567871 time_total_s: 7.610152244567871 timers: learn_throughput: 1209.129 learn_time_ms: 3308.165 load_throughput: 0.0 load_time_ms: 0.0 sample_throughput: 934.661 sample_time_ms: 4279.627 update_time_ms: 5.002 timestamp: 1638206646 timesteps_since_restore: 0 timesteps_this_iter: 0 timesteps_total: 4000 training_iteration: 1 trial_id: 10a86_00000 == Status == Current time: 2021-11-29 18:24:12 (running for 00:00:44.46) Memory usage on this node: 10.5/11.9 GiB Using FIFO scheduling algorithm. Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.7 GiB heap, 0.0/0.85 GiB objects Result logdir: C: Users Stefan ray_results PPO Number of trials: 1/1 (1 RUNNING) +--+-+-+--+++-+-+-+--+ | Trial name | status | loc | iter | total time (s) | ts | reward | episode_reward_max | episode_reward_min | episode_len_mean | |--+-+-+--+++-+-+-+--| | PPO_StatelessCartPole_10a86_00000 | RUNNING | 127.0.0.1:3524 | 1 | 7.61015 | 4000 | 19.2657 | 47 | 8 | 19.2657 | +--+-+-+--+++-+-+-+--+ Result for PPO_StatelessCartPole_10a86_00000: agent_timesteps_total: 8000 custom_metrics: {} date: 2021-11-29_18-24-14 done: false episode_len_mean: 24.323170731707318 episode_media: {} episode_reward_max: 85.0 episode_reward_mean: 24.323170731707318 episode_reward_min: 9.0 episodes_this_iter: 164 episodes_total: 371 experiment_id: 7d4c24fff65e435bbd107e9f1ece5775 hostname: nb-stschn info: learner: default_policy: custom_metrics: {} learner_stats: cur_kl_coeff: 0.30000001192092896 cur_lr: 4.999999873689376e-05 entropy: 0.6796531081199646 entropy_coeff: 0.0 kl: 0.004038441926240921 model: {} policy_loss: -0.010176131501793861 total_loss: 117.6571273803711 vf_explained_var: -0.09520665556192398 vf_loss: 117.66609191894531 num_agent_steps_sampled: 8000 num_agent_steps_trained: 8000 num_steps_sampled: 8000 num_steps_trained: 8000 num_steps_trained_this_iter: 0 iterations_since_restore: 2 node_ip: 127.0.0.1 num_healthy_workers: 2 off_policy_estimator: {} perf: cpu_util_percent: 69.80000000000001 ram_util_percent: 88.55999999999999 pid: 3524 policy_reward_max: {} policy_reward_mean: {} policy_reward_min: {} sampler_perf: mean_action_processing_ms: 0.0844219316199571 mean_env_render_ms: 0.0 mean_env_wait_ms: 0.12437962595359744 mean_inference_ms: 1.6573376502999793 mean_raw_obs_processing_ms: 0.18405465203422883 time_since_restore: 14.843173742294312 time_this_iter_s: 7.23302149772644 time_total_s: 14.843173742294312 timers: learn_throughput: 1245.189 learn_time_ms: 3212.363 load_throughput: 0.0 load_time_ms: 0.0 sample_throughput: 680.084 sample_time_ms: 5881.622 update_time_ms: 2.501 timestamp: 1638206654 timesteps_since_restore: 0 timesteps_this_iter: 0 timesteps_total: 8000 training_iteration: 2 trial_id: 10a86_00000 == Status == Current time: 2021-11-29 18:24:17 (running for 00:00:49.71) Memory usage on this node: 10.5/11.9 GiB Using FIFO scheduling algorithm. Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.7 GiB heap, 0.0/0.85 GiB objects Result logdir: C: Users Stefan ray_results PPO Number of trials: 1/1 (1 RUNNING) +--+-+-+--+++-+-+-+--+ | Trial name | status | loc | iter | total time (s) | ts | reward | episode_reward_max | episode_reward_min | episode_len_mean | |--+-+-+--+++-+-+-+--| | PPO_StatelessCartPole_10a86_00000 | RUNNING | 127.0.0.1:3524 | 2 | 14.8432 | 8000 | 24.3232 | 85 | 9 | 24.3232 | +--+-+-+--+++-+-+-+--+ Result for PPO_StatelessCartPole_10a86_00000: agent_timesteps_total: 12000 custom_metrics: {} date: 2021-11-29_18-24-21 done: false episode_len_mean: 28.049295774647888 episode_media: {} episode_reward_max: 99.0 episode_reward_mean: 28.049295774647888 episode_reward_min: 9.0 episodes_this_iter: 142 episodes_total: 513 experiment_id: 7d4c24fff65e435bbd107e9f1ece5775 hostname: nb-stschn info: learner: default_policy: custom_metrics: {} learner_stats: cur_kl_coeff: 0.15000000596046448 cur_lr: 4.999999873689376e-05 entropy: 0.6625799536705017 entropy_coeff: 0.0 kl: 0.009470137767493725 model: {} policy_loss: -0.0061936089769005775 total_loss: 146.39207458496094 vf_explained_var: -0.09814939647912979 vf_loss: 146.39683532714844 num_agent_steps_sampled: 12000 num_agent_steps_trained: 12000 num_steps_sampled: 12000 num_steps_trained: 12000 num_steps_trained_this_iter: 0 iterations_since_restore: 3 node_ip: 127.0.0.1 num_healthy_workers: 2 off_policy_estimator: {} perf: cpu_util_percent: 69.2909090909091 ram_util_percent: 88.22727272727273 pid: 3524 policy_reward_max: {} policy_reward_mean: {} policy_reward_min: {} sampler_perf: mean_action_processing_ms: 0.09170712608844997 mean_env_render_ms: 0.0 mean_env_wait_ms: 0.11897874250987923 mean_inference_ms: 1.6684947554661607 mean_raw_obs_processing_ms: 0.18008555734453105 time_since_restore: 22.40989089012146 time_this_iter_s: 7.566717147827148 time_total_s: 22.40989089012146 timers: learn_throughput: 1229.742 learn_time_ms: 3252.714 load_throughput: 0.0 load_time_ms: 0.0 sample_throughput: 626.16 sample_time_ms: 6388.146 update_time_ms: 1.667 timestamp: 1638206661 timesteps_since_restore: 0 timesteps_this_iter: 0 timesteps_total: 12000 training_iteration: 3 trial_id: 10a86_00000 == Status == Current time: 2021-11-29 18:24:22 (running for 00:00:55.29) Memory usage on this node: 10.5/11.9 GiB Using FIFO scheduling algorithm. Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.7 GiB heap, 0.0/0.85 GiB objects Result logdir: C: Users Stefan ray_results PPO Number of trials: 1/1 (1 RUNNING) +--+-+-+--++-+-+-+-+--+ | Trial name | status | loc | iter | total time (s) | ts | reward | episode_reward_max | episode_reward_min | episode_len_mean | |--+-+-+--++-+-+-+-+--| | PPO_StatelessCartPole_10a86_00000 | RUNNING | 127.0.0.1:3524 | 3 | 22.4099 | 12000 | 28.0493 | 99 | 9 | 28.0493 | +--+-+-+--++-+-+-+-+--+ == Status == Current time: 2021-11-29 18:24:27 (running for 00:01:00.34) Memory usage on this node: 10.5/11.9 GiB Using FIFO scheduling algorithm. Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.7 GiB heap, 0.0/0.85 GiB objects Result logdir: C: Users Stefan ray_results PPO Number of trials: 1/1 (1 RUNNING) +--+-+-+--++-+-+-+-+--+ | Trial name | status | loc | iter | total time (s) | ts | reward | episode_reward_max | episode_reward_min | episode_len_mean | |--+-+-+--++-+-+-+-+--| | PPO_StatelessCartPole_10a86_00000 | RUNNING | 127.0.0.1:3524 | 3 | 22.4099 | 12000 | 28.0493 | 99 | 9 | 28.0493 | +--+-+-+--++-+-+-+-+--+ Result for PPO_StatelessCartPole_10a86_00000: agent_timesteps_total: 16000 custom_metrics: {} date: 2021-11-29_18-24-29 done: false episode_len_mean: 35.20175438596491 episode_media: {} episode_reward_max: 101.0 episode_reward_mean: 35.20175438596491 episode_reward_min: 10.0 episodes_this_iter: 114 episodes_total: 627 experiment_id: 7d4c24fff65e435bbd107e9f1ece5775 hostname: nb-stschn info: learner: default_policy: custom_metrics: {} learner_stats: cur_kl_coeff: 0.15000000596046448 cur_lr: 4.999999873689376e-05 entropy: 0.6402544379234314 entropy_coeff: 0.0 kl: 0.00787932239472866 model: {} policy_loss: -0.010451151058077812 total_loss: 155.21902465820312 vf_explained_var: -0.06683431565761566 vf_loss: 155.22828674316406 num_agent_steps_sampled: 16000 num_agent_steps_trained: 16000 num_steps_sampled: 16000 num_steps_trained: 16000 num_steps_trained_this_iter: 0 iterations_since_restore: 4 node_ip: 127.0.0.1 num_healthy_workers: 2 off_policy_estimator: {} perf: cpu_util_percent: 69.77000000000001 ram_util_percent: 88.03 pid: 3524 policy_reward_max: {} policy_reward_mean: {} policy_reward_min: {} sampler_perf: mean_action_processing_ms: 0.08782784772990902 mean_env_render_ms: 0.0 mean_env_wait_ms: 0.11721316723635167 mean_inference_ms: 1.6961190759485176 mean_raw_obs_processing_ms: 0.17694523996361392 time_since_restore: 29.893202781677246 time_this_iter_s: 7.483311891555786 time_total_s: 29.893202781677246 timers: learn_throughput: 1239.717 learn_time_ms: 3226.542 load_throughput: 0.0 load_time_ms: 0.0 sample_throughput: 595.132 sample_time_ms: 6721.195 update_time_ms: 1.251 timestamp: 1638206669 timesteps_since_restore: 0 timesteps_this_iter: 0 timesteps_total: 16000 training_iteration: 4 trial_id: 10a86_00000 == Status == Current time: 2021-11-29 18:24:33 (running for 00:01:05.84) Memory usage on this node: 10.5/11.9 GiB Using FIFO scheduling algorithm. Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.7 GiB heap, 0.0/0.85 GiB objects Result logdir: C: Users Stefan ray_results PPO Number of trials: 1/1 (1 RUNNING) +--+-+-+--++-+-+-+-+--+ | Trial name | status | loc | iter | total time (s) | ts | reward | episode_reward_max | episode_reward_min | episode_len_mean | |--+-+-+--++-+-+-+-+--| | PPO_StatelessCartPole_10a86_00000 | RUNNING | 127.0.0.1:3524 | 4 | 29.8932 | 16000 | 35.2018 | 101 | 10 | 35.2018 | +--+-+-+--++-+-+-+-+--+ Result for PPO_StatelessCartPole_10a86_00000: agent_timesteps_total: 20000 custom_metrics: {} date: 2021-11-29_18-24-36 done: false episode_len_mean: 41.04 episode_media: {} episode_reward_max: 109.0 episode_reward_mean: 41.04 episode_reward_min: 11.0 episodes_this_iter: 95 episodes_total: 722 experiment_id: 7d4c24fff65e435bbd107e9f1ece5775 hostname: nb-stschn info: learner: default_policy: custom_metrics: {} learner_stats: cur_kl_coeff: 0.15000000596046448 cur_lr: 4.999999873689376e-05 entropy: 0.6226502656936646 entropy_coeff: 0.0 kl: 0.017211856320500374 model: {} policy_loss: -0.012698379345238209 total_loss: 203.50961303710938 vf_explained_var: -0.0032532268669456244 vf_loss: 203.51974487304688 num_agent_steps_sampled: 20000 num_agent_steps_trained: 20000 num_steps_sampled: 20000 num_steps_trained: 20000 num_steps_trained_this_iter: 0 iterations_since_restore: 5 node_ip: 127.0.0.1 num_healthy_workers: 2 off_policy_estimator: {} perf: cpu_util_percent: 68.35999999999999 ram_util_percent: 88.16000000000001 pid: 3524 policy_reward_max: {} policy_reward_mean: {} policy_reward_min: {} sampler_perf: mean_action_processing_ms: 0.09040986836668251 mean_env_render_ms: 0.0 mean_env_wait_ms: 0.11222753479971719 mean_inference_ms: 1.6809684919954235 mean_raw_obs_processing_ms: 0.17502617184152944 time_since_restore: 36.9858672618866 time_this_iter_s: 7.092664480209351 time_total_s: 36.9858672618866 timers: learn_throughput: 1254.72 learn_time_ms: 3187.961 load_throughput: 0.0 load_time_ms: 0.0 sample_throughput: 585.957 sample_time_ms: 6826.443 update_time_ms: 1.0 timestamp: 1638206676 timesteps_since_restore: 0 timesteps_this_iter: 0 timesteps_total: 20000 training_iteration: 5 trial_id: 10a86_00000 == Status == Current time: 2021-11-29 18:24:38 (running for 00:01:10.97) Memory usage on this node: 10.5/11.9 GiB Using FIFO scheduling algorithm. Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.7 GiB heap, 0.0/0.85 GiB objects Result logdir: C: Users Stefan ray_results PPO Number of trials: 1/1 (1 RUNNING) +--+-+-+--++-+-+-+-+--+ | Trial name | status | loc | iter | total time (s) | ts | reward | episode_reward_max | episode_reward_min | episode_len_mean | |--+-+-+--++-+-+-+-+--| | PPO_StatelessCartPole_10a86_00000 | RUNNING | 127.0.0.1:3524 | 5 | 36.9859 | 20000 | 41.04 | 109 | 11 | 41.04 | +--+-+-+--++-+-+-+-+--+ == Status == Current time: 2021-11-29 18:24:43 (running for 00:01:16.07) Memory usage on this node: 10.5/11.9 GiB Using FIFO scheduling algorithm. Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.7 GiB heap, 0.0/0.85 GiB objects Result logdir: C: Users Stefan ray_results PPO Number of trials: 1/1 (1 RUNNING) +--+-+-+--++-+-+-+-+--+ | Trial name | status | loc | iter | total time (s) | ts | reward | episode_reward_max | episode_reward_min | episode_len_mean | |--+-+-+--++-+-+-+-+--| | PPO_StatelessCartPole_10a86_00000 | RUNNING | 127.0.0.1:3524 | 5 | 36.9859 | 20000 | 41.04 | 109 | 11 | 41.04 | +--+-+-+--++-+-+-+-+--+ Result for PPO_StatelessCartPole_10a86_00000: agent_timesteps_total: 24000 custom_metrics: {} date: 2021-11-29_18-24-44 done: false episode_len_mean: 49.26 episode_media: {} episode_reward_max: 131.0 episode_reward_mean: 49.26 episode_reward_min: 11.0 episodes_this_iter: 76 episodes_total: 798 experiment_id: 7d4c24fff65e435bbd107e9f1ece5775 hostname: nb-stschn info: learner: default_policy: custom_metrics: {} learner_stats: cur_kl_coeff: 0.15000000596046448 cur_lr: 4.999999873689376e-05 entropy: 0.6099566221237183 entropy_coeff: 0.0 kl: 0.007545969914644957 model: {} policy_loss: -0.009934185072779655 total_loss: 242.66400146484375 vf_explained_var: -0.03155364468693733 vf_loss: 242.67282104492188 num_agent_steps_sampled: 24000 num_agent_steps_trained: 24000 num_steps_sampled: 24000 num_steps_trained: 24000 num_steps_trained_this_iter: 0 iterations_since_restore: 6 node_ip: 127.0.0.1 num_healthy_workers: 2 off_policy_estimator: {} perf: cpu_util_percent: 76.23333333333333 ram_util_percent: 88.16666666666669 pid: 3524 policy_reward_max: {} policy_reward_mean: {} policy_reward_min: {} sampler_perf: mean_action_processing_ms: 0.09588192834222205 mean_env_render_ms: 0.0 mean_env_wait_ms: 0.11236996566932105 mean_inference_ms: 1.6786688208231442 mean_raw_obs_processing_ms: 0.17285567792594644 time_since_restore: 44.981568336486816 time_this_iter_s: 7.99570107460022 time_total_s: 44.981568336486816 timers: learn_throughput: 1218.972 learn_time_ms: 3281.455 load_throughput: 0.0 load_time_ms: 0.0 sample_throughput: 578.762 sample_time_ms: 6911.301 update_time_ms: 1.005 timestamp: 1638206684 timesteps_since_restore: 0 timesteps_this_iter: 0 timesteps_total: 24000 training_iteration: 6 trial_id: 10a86_00000 == Status == Current time: 2021-11-29 18:24:49 (running for 00:01:22.06) Memory usage on this node: 10.5/11.9 GiB Using FIFO scheduling algorithm. Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.7 GiB heap, 0.0/0.85 GiB objects Result logdir: C: Users Stefan ray_results PPO Number of trials: 1/1 (1 RUNNING) +--+-+-+--++-+-+-+-+--+ | Trial name | status | loc | iter | total time (s) | ts | reward | episode_reward_max | episode_reward_min | episode_len_mean | |--+-+-+--++-+-+-+-+--| | PPO_StatelessCartPole_10a86_00000 | RUNNING | 127.0.0.1:3524 | 6 | 44.9816 | 24000 | 49.26 | 131 | 11 | 49.26 | +--+-+-+--++-+-+-+-+--+ Result for PPO_StatelessCartPole_10a86_00000: agent_timesteps_total: 28000 custom_metrics: {} date: 2021-11-29_18-24-52 done: false episode_len_mean: 58.37 episode_media: {} episode_reward_max: 124.0 episode_reward_mean: 58.37 episode_reward_min: 14.0 episodes_this_iter: 65 episodes_total: 863 experiment_id: 7d4c24fff65e435bbd107e9f1ece5775 hostname: nb-stschn info: learner: default_policy: custom_metrics: {} learner_stats: cur_kl_coeff: 0.15000000596046448 cur_lr: 4.999999873689376e-05 entropy: 0.5877225995063782 entropy_coeff: 0.0 kl: 0.009110619314014912 model: {} policy_loss: -0.0034982727374881506 total_loss: 258.46380615234375 vf_explained_var: 0.01713959127664566 vf_loss: 258.4659423828125 num_agent_steps_sampled: 28000 num_agent_steps_trained: 28000 num_steps_sampled: 28000 num_steps_trained: 28000 num_steps_trained_this_iter: 0 iterations_since_restore: 7 node_ip: 127.0.0.1 num_healthy_workers: 2 off_policy_estimator: {} perf: cpu_util_percent: 74.64545454545454 ram_util_percent: 88.14545454545454 pid: 3524 policy_reward_max: {} policy_reward_mean: {} policy_reward_min: {} sampler_perf: mean_action_processing_ms: 0.09758102258436516 mean_env_render_ms: 0.0 mean_env_wait_ms: 0.11402532201158655 mean_inference_ms: 1.6973743478770467 mean_raw_obs_processing_ms: 0.17373906883397264 time_since_restore: 52.79775881767273 time_this_iter_s: 7.816190481185913 time_total_s: 52.79775881767273 timers: learn_throughput: 1226.905 learn_time_ms: 3260.237 load_throughput: 0.0 load_time_ms: 0.0 sample_throughput: 560.776 sample_time_ms: 7132.976 update_time_ms: 0.861 timestamp: 1638206692 timesteps_since_restore: 0 timesteps_this_iter: 0 timesteps_total: 28000 training_iteration: 7 trial_id: 10a86_00000 == Status == Current time: 2021-11-29 18:24:55 (running for 00:01:27.89) Memory usage on this node: 10.5/11.9 GiB Using FIFO scheduling algorithm. Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.7 GiB heap, 0.0/0.85 GiB objects Result logdir: C: Users Stefan ray_results PPO Number of trials: 1/1 (1 RUNNING) +--+-+-+--++-+-+-+-+--+ | Trial name | status | loc | iter | total time (s) | ts | reward | episode_reward_max | episode_reward_min | episode_len_mean | |--+-+-+--++-+-+-+-+--| | PPO_StatelessCartPole_10a86_00000 | RUNNING | 127.0.0.1:3524 | 7 | 52.7978 | 28000 | 58.37 | 124 | 14 | 58.37 | +--+-+-+--++-+-+-+-+--+ Result for PPO_StatelessCartPole_10a86_00000: agent_timesteps_total: 32000 custom_metrics: {} date: 2021-11-29_18-24-59 done: false episode_len_mean: 62.05 episode_media: {} episode_reward_max: 152.0 episode_reward_mean: 62.05 episode_reward_min: 16.0 episodes_this_iter: 64 episodes_total: 927 experiment_id: 7d4c24fff65e435bbd107e9f1ece5775 hostname: nb-stschn info: learner: default_policy: custom_metrics: {} learner_stats: cur_kl_coeff: 0.15000000596046448 cur_lr: 4.999999873689376e-05 entropy: 0.5817587375640869 entropy_coeff: 0.0 kl: 0.00792761892080307 model: {} policy_loss: -0.0012906708288937807 total_loss: 358.649658203125 vf_explained_var: 0.0025409103836864233 vf_loss: 358.64971923828125 num_agent_steps_sampled: 32000 num_agent_steps_trained: 32000 num_steps_sampled: 32000 num_steps_trained: 32000 num_steps_trained_this_iter: 0 iterations_since_restore: 8 node_ip: 127.0.0.1 num_healthy_workers: 2 off_policy_estimator: {} perf: cpu_util_percent: 67.47 ram_util_percent: 87.96000000000001 pid: 3524 policy_reward_max: {} policy_reward_mean: {} policy_reward_min: {} sampler_perf: mean_action_processing_ms: 0.1015822110214221 mean_env_render_ms: 0.0 mean_env_wait_ms: 0.11516028800779558 mean_inference_ms: 1.7051025329074971 mean_raw_obs_processing_ms: 0.17298850790331832 time_since_restore: 60.130942583084106 time_this_iter_s: 7.333183765411377 time_total_s: 60.130942583084106 timers: learn_throughput: 1236.876 learn_time_ms: 3233.954 load_throughput: 0.0 load_time_ms: 0.0 sample_throughput: 557.58 sample_time_ms: 7173.857 update_time_ms: 0.753 timestamp: 1638206699 timesteps_since_restore: 0 timesteps_this_iter: 0 timesteps_total: 32000 training_iteration: 8 trial_id: 10a86_00000 == Status == Current time: 2021-11-29 18:25:00 (running for 00:01:33.21) Memory usage on this node: 10.5/11.9 GiB Using FIFO scheduling algorithm. Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.7 GiB heap, 0.0/0.85 GiB objects Result logdir: C: Users Stefan ray_results PPO Number of trials: 1/1 (1 RUNNING) +--+-+-+--++-+-+-+-+--+ | Trial name | status | loc | iter | total time (s) | ts | reward | episode_reward_max | episode_reward_min | episode_len_mean | |--+-+-+--++-+-+-+-+--| | PPO_StatelessCartPole_10a86_00000 | RUNNING | 127.0.0.1:3524 | 8 | 60.1309 | 32000 | 62.05 | 152 | 16 | 62.05 | +--+-+-+--++-+-+-+-+--+ == Status == Current time: 2021-11-29 18:25:05 (running for 00:01:38.31) Memory usage on this node: 10.5/11.9 GiB Using FIFO scheduling algorithm. Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.7 GiB heap, 0.0/0.85 GiB objects Result logdir: C: Users Stefan ray_results PPO Number of trials: 1/1 (1 RUNNING) +--+-+-+--++-+-+-+-+--+ | Trial name | status | loc | iter | total time (s) | ts | reward | episode_reward_max | episode_reward_min | episode_len_mean | |--+-+-+--++-+-+-+-+--| | PPO_StatelessCartPole_10a86_00000 | RUNNING | 127.0.0.1:3524 | 8 | 60.1309 | 32000 | 62.05 | 152 | 16 | 62.05 | +--+-+-+--++-+-+-+-+--+ Result for PPO_StatelessCartPole_10a86_00000: agent_timesteps_total: 36000 custom_metrics: {} date: 2021-11-29_18-25-07 done: false episode_len_mean: 69.74 episode_media: {} episode_reward_max: 179.0 episode_reward_mean: 69.74 episode_reward_min: 16.0 episodes_this_iter: 53 episodes_total: 980 experiment_id: 7d4c24fff65e435bbd107e9f1ece5775 hostname: nb-stschn info: learner: default_policy: custom_metrics: {} learner_stats: cur_kl_coeff: 0.15000000596046448 cur_lr: 4.999999873689376e-05 entropy: 0.5835209488868713 entropy_coeff: 0.0 kl: 0.007502125576138496 model: {} policy_loss: 0.002612495329231024 total_loss: 432.0804443359375 vf_explained_var: 0.0019506452372297645 vf_loss: 432.07672119140625 num_agent_steps_sampled: 36000 num_agent_steps_trained: 36000 num_steps_sampled: 36000 num_steps_trained: 36000 num_steps_trained_this_iter: 0 iterations_since_restore: 9 node_ip: 127.0.0.1 num_healthy_workers: 2 off_policy_estimator: {} perf: cpu_util_percent: 75.7909090909091 ram_util_percent: 87.89090909090909 pid: 3524 policy_reward_max: {} policy_reward_mean: {} policy_reward_min: {} sampler_perf: mean_action_processing_ms: 0.1049023219704576 mean_env_render_ms: 0.0 mean_env_wait_ms: 0.11544358278733036 mean_inference_ms: 1.7089986456134836 mean_raw_obs_processing_ms: 0.16874296264413524 time_since_restore: 68.01425743103027 time_this_iter_s: 7.883314847946167 time_total_s: 68.01425743103027 timers: learn_throughput: 1225.622 learn_time_ms: 3263.65 load_throughput: 2302208.425 load_time_ms: 1.737 sample_throughput: 555.207 sample_time_ms: 7204.515 update_time_ms: 0.67 timestamp: 1638206707 timesteps_since_restore: 0 timesteps_this_iter: 0 timesteps_total: 36000 training_iteration: 9 trial_id: 10a86_00000 == Status == Current time: 2021-11-29 18:25:11 (running for 00:01:44.17) Memory usage on this node: 10.5/11.9 GiB Using FIFO scheduling algorithm. Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.7 GiB heap, 0.0/0.85 GiB objects Result logdir: C: Users Stefan ray_results PPO Number of trials: 1/1 (1 RUNNING) +--+-+-+--++-+-+-+-+--+ | Trial name | status | loc | iter | total time (s) | ts | reward | episode_reward_max | episode_reward_min | episode_len_mean | |--+-+-+--++-+-+-+-+--| | PPO_StatelessCartPole_10a86_00000 | RUNNING | 127.0.0.1:3524 | 9 | 68.0143 | 36000 | 69.74 | 179 | 16 | 69.74 | +--+-+-+--++-+-+-+-+--+ Result for PPO_StatelessCartPole_10a86_00000: agent_timesteps_total: 40000 custom_metrics: {} date: 2021-11-29_18-25-15 done: true episode_len_mean: 75.68 episode_media: {} episode_reward_max: 179.0 episode_reward_mean: 75.68 episode_reward_min: 17.0 episodes_this_iter: 53 episodes_total: 1033 experiment_id: 7d4c24fff65e435bbd107e9f1ece5775 hostname: nb-stschn info: learner: default_policy: custom_metrics: {} learner_stats: cur_kl_coeff: 0.15000000596046448 cur_lr: 4.999999873689376e-05 entropy: 0.5569567084312439 entropy_coeff: 0.0 kl: 0.010418258607387543 model: {} policy_loss: -0.009912493638694286 total_loss: 344.77105712890625 vf_explained_var: 0.06800872832536697 vf_loss: 344.7794494628906 num_agent_steps_sampled: 40000 num_agent_steps_trained: 40000 num_steps_sampled: 40000 num_steps_trained: 40000 num_steps_trained_this_iter: 0 iterations_since_restore: 10 node_ip: 127.0.0.1 num_healthy_workers: 2 off_policy_estimator: {} perf: cpu_util_percent: 72.01 ram_util_percent: 87.97 pid: 3524 policy_reward_max: {} policy_reward_mean: {} policy_reward_min: {} sampler_perf: mean_action_processing_ms: 0.10250035902130619 mean_env_render_ms: 0.0 mean_env_wait_ms: 0.11447514042173512 mean_inference_ms: 1.7157662969913339 mean_raw_obs_processing_ms: 0.16733786590001506 time_since_restore: 75.34810090065002 time_this_iter_s: 7.333843469619751 time_total_s: 75.34810090065002 timers: learn_throughput: 1233.691 learn_time_ms: 3242.303 load_throughput: 2558009.362 load_time_ms: 1.564 sample_throughput: 550.441 sample_time_ms: 7266.896 update_time_ms: 0.603 timestamp: 1638206715 timesteps_since_restore: 0 timesteps_this_iter: 0 timesteps_total: 40000 training_iteration: 10 trial_id: 10a86_00000 == Status == Current time: 2021-11-29 18:25:15 (running for 00:01:47.51) Memory usage on this node: 10.5/11.9 GiB Using FIFO scheduling algorithm. Resources requested: 0/3 CPUs, 0/0 GPUs, 0.0/1.7 GiB heap, 0.0/0.85 GiB objects Result logdir: C: Users Stefan ray_results PPO Number of trials: 1/1 (1 TERMINATED) +--++-+--++-+-+-+-+--+ | Trial name | status | loc | iter | total time (s) | ts | reward | episode_reward_max | episode_reward_min | episode_len_mean | |--++-+--++-+-+-+-+--| | PPO_StatelessCartPole_10a86_00000 | TERMINATED | 127.0.0.1:3524 | 10 | 75.3481 | 40000 | 75.68 | 179 | 17 | 75.68 | +--++-+--++-+-+-+-+--+ . (pid=3524) [2021-11-29 18:25:15,326 E 3524 10736] raylet_client.cc:159: IOError: Unknown error [RayletClient] Failed to disconnect from raylet. (pid=23440) [2021-11-29 18:25:15,333 E 23440 3596] raylet_client.cc:159: IOError: Unknown error [RayletClient] Failed to disconnect from raylet. (pid=8864) [2021-11-29 18:25:15,330 E 8864 8836] raylet_client.cc:159: IOError: Unknown error [RayletClient] Failed to disconnect from raylet. 2021-11-29 18:25:15,446 INFO tune.py:630 -- Total run time: 107.86 seconds (107.47 seconds for the tuning loop). . Option 3a2 with Trajectory API: Training finished successfully . (pid=3524) Windows fatal exception: access violation (pid=3524) (pid=23440) Windows fatal exception: access violation (pid=23440) (pid=8864) Windows fatal exception: access violation (pid=8864) . . results3a2.default_metric = &quot;episode_reward_mean&quot; results3a2.default_mode = &quot;max&quot; # print the mean episode reward = episode length --&gt; higher = better results3a2.best_result[&quot;episode_reward_mean&quot;] . 75.68 . plot_rewards(results3a2) . c: users stefan git-repos private blog venv lib site-packages seaborn _decorators.py:36: FutureWarning: Pass the following variables as keyword args: x, y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation. warnings.warn( . Option 3b: Use an LSTM for Processing the Sequence . Instead of stacking the last $n$ observations and providing this sequence as input to a regular feed-forward neural network, a recurrent neural network (RNN) can be used, keeping track of a learned state that is passed onwards from observation to observation. . Long short-term memory (LSTM) networks are a variant of RNNs that are good at keeping state for longer durations. To use an LSTM with RLlib, simply set the corresponding flag in the model config: . config3b = ppo.DEFAULT_CONFIG.copy() config3b[&quot;env&quot;] = &quot;StatelessCartPole&quot; config3b[&quot;model&quot;] = { &quot;use_lstm&quot;: True, # &quot;max_seq_len&quot;: 10, } results3b = ray.tune.run(&quot;PPO&quot;, config=config3b, stop=stop) print(&quot;Option 3b: Training finished successfully&quot;) . == Status == Current time: 2021-11-29 18:28:37 (running for 00:00:00.17) Memory usage on this node: 9.3/11.9 GiB Using FIFO scheduling algorithm. Resources requested: 0/3 CPUs, 0/0 GPUs, 0.0/1.7 GiB heap, 0.0/0.85 GiB objects Result logdir: C: Users Stefan ray_results PPO Number of trials: 1/1 (1 PENDING) +--+-+-+ | Trial name | status | loc | |--+-+-| | PPO_StatelessCartPole_c9203_00000 | PENDING | | +--+-+-+ . (pid=None) c: users stefan git-repos private blog venv lib site-packages redis connection.py:77: UserWarning: redis-py works best with hiredis. Please consider installing (pid=None) warnings.warn(msg) . == Status == Current time: 2021-11-29 18:28:42 (running for 00:00:05.17) Memory usage on this node: 9.4/11.9 GiB Using FIFO scheduling algorithm. Resources requested: 0/3 CPUs, 0/0 GPUs, 0.0/1.7 GiB heap, 0.0/0.85 GiB objects Result logdir: C: Users Stefan ray_results PPO Number of trials: 1/1 (1 PENDING) +--+-+-+ | Trial name | status | loc | |--+-+-| | PPO_StatelessCartPole_c9203_00000 | PENDING | | +--+-+-+ . (pid=8716) 2021-11-29 18:28:53,746 INFO trainer.py:753 -- Tip: set framework=tfe or the --eager flag to enable TensorFlow eager execution (pid=8716) 2021-11-29 18:28:53,746 INFO ppo.py:166 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn&#39;t work for you. (pid=8716) 2021-11-29 18:28:53,746 INFO trainer.py:770 -- Current log_level is WARN. For more information, set &#39;log_level&#39;: &#39;INFO&#39; / &#39;DEBUG&#39; or use the -v and -vv flags. (pid=None) c: users stefan git-repos private blog venv lib site-packages redis connection.py:77: UserWarning: redis-py works best with hiredis. Please consider installing (pid=None) warnings.warn(msg) (pid=None) c: users stefan git-repos private blog venv lib site-packages redis connection.py:77: UserWarning: redis-py works best with hiredis. Please consider installing (pid=None) warnings.warn(msg) (pid=2820) 2021-11-29 18:29:06,789 WARNING deprecation.py:38 -- DeprecationWarning: `SampleBatch[&#39;is_training&#39;]` has been deprecated. Use `SampleBatch.is_training` instead. This will raise an error in the future! (pid=8716) 2021-11-29 18:29:11,840 WARNING deprecation.py:38 -- DeprecationWarning: `SampleBatch[&#39;is_training&#39;]` has been deprecated. Use `SampleBatch.is_training` instead. This will raise an error in the future! . == Status == Current time: 2021-11-29 18:29:14 (running for 00:00:37.87) Memory usage on this node: 10.1/11.9 GiB Using FIFO scheduling algorithm. Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.7 GiB heap, 0.0/0.85 GiB objects Result logdir: C: Users Stefan ray_results PPO Number of trials: 1/1 (1 RUNNING) +--+-+-+ | Trial name | status | loc | |--+-+-| | PPO_StatelessCartPole_c9203_00000 | RUNNING | 127.0.0.1:8716 | +--+-+-+ . (pid=8716) 2021-11-29 18:29:14,881 WARNING trainer_template.py:185 -- `execution_plan` functions should accept `trainer`, `workers`, and `config` as args! (pid=8716) 2021-11-29 18:29:14,881 INFO trainable.py:110 -- Trainable.setup took 21.135 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads. (pid=8716) 2021-11-29 18:29:14,896 WARNING util.py:57 -- Install gputil for GPU system monitoring. . == Status == Current time: 2021-11-29 18:29:16 (running for 00:00:38.92) Memory usage on this node: 10.1/11.9 GiB Using FIFO scheduling algorithm. Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.7 GiB heap, 0.0/0.85 GiB objects Result logdir: C: Users Stefan ray_results PPO Number of trials: 1/1 (1 RUNNING) +--+-+-+ | Trial name | status | loc | |--+-+-| | PPO_StatelessCartPole_c9203_00000 | RUNNING | 127.0.0.1:8716 | +--+-+-+ == Status == Current time: 2021-11-29 18:29:22 (running for 00:00:45.04) Memory usage on this node: 10.0/11.9 GiB Using FIFO scheduling algorithm. Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.7 GiB heap, 0.0/0.85 GiB objects Result logdir: C: Users Stefan ray_results PPO Number of trials: 1/1 (1 RUNNING) +--+-+-+ | Trial name | status | loc | |--+-+-| | PPO_StatelessCartPole_c9203_00000 | RUNNING | 127.0.0.1:8716 | +--+-+-+ . (pid=8716) 2021-11-29 18:29:23,213 WARNING deprecation.py:38 -- DeprecationWarning: `slice` has been deprecated. Use `SampleBatch[start:stop]` instead. This will raise an error in the future! . == Status == Current time: 2021-11-29 18:29:27 (running for 00:00:50.17) Memory usage on this node: 10.1/11.9 GiB Using FIFO scheduling algorithm. Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.7 GiB heap, 0.0/0.85 GiB objects Result logdir: C: Users Stefan ray_results PPO Number of trials: 1/1 (1 RUNNING) +--+-+-+ | Trial name | status | loc | |--+-+-| | PPO_StatelessCartPole_c9203_00000 | RUNNING | 127.0.0.1:8716 | +--+-+-+ == Status == Current time: 2021-11-29 18:29:32 (running for 00:00:55.28) Memory usage on this node: 10.1/11.9 GiB Using FIFO scheduling algorithm. Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.7 GiB heap, 0.0/0.85 GiB objects Result logdir: C: Users Stefan ray_results PPO Number of trials: 1/1 (1 RUNNING) +--+-+-+ | Trial name | status | loc | |--+-+-| | PPO_StatelessCartPole_c9203_00000 | RUNNING | 127.0.0.1:8716 | +--+-+-+ == Status == Current time: 2021-11-29 18:29:37 (running for 00:01:00.41) Memory usage on this node: 10.0/11.9 GiB Using FIFO scheduling algorithm. Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.7 GiB heap, 0.0/0.85 GiB objects Result logdir: C: Users Stefan ray_results PPO Number of trials: 1/1 (1 RUNNING) +--+-+-+ | Trial name | status | loc | |--+-+-| | PPO_StatelessCartPole_c9203_00000 | RUNNING | 127.0.0.1:8716 | +--+-+-+ == Status == Current time: 2021-11-29 18:29:42 (running for 00:01:05.56) Memory usage on this node: 10.0/11.9 GiB Using FIFO scheduling algorithm. Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.7 GiB heap, 0.0/0.85 GiB objects Result logdir: C: Users Stefan ray_results PPO Number of trials: 1/1 (1 RUNNING) +--+-+-+ | Trial name | status | loc | |--+-+-| | PPO_StatelessCartPole_c9203_00000 | RUNNING | 127.0.0.1:8716 | +--+-+-+ == Status == Current time: 2021-11-29 18:29:47 (running for 00:01:10.69) Memory usage on this node: 10.0/11.9 GiB Using FIFO scheduling algorithm. Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.7 GiB heap, 0.0/0.85 GiB objects Result logdir: C: Users Stefan ray_results PPO Number of trials: 1/1 (1 RUNNING) +--+-+-+ | Trial name | status | loc | |--+-+-| | PPO_StatelessCartPole_c9203_00000 | RUNNING | 127.0.0.1:8716 | +--+-+-+ == Status == Current time: 2021-11-29 18:29:52 (running for 00:01:15.77) Memory usage on this node: 10.0/11.9 GiB Using FIFO scheduling algorithm. Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.7 GiB heap, 0.0/0.85 GiB objects Result logdir: C: Users Stefan ray_results PPO Number of trials: 1/1 (1 RUNNING) +--+-+-+ | Trial name | status | loc | |--+-+-| | PPO_StatelessCartPole_c9203_00000 | RUNNING | 127.0.0.1:8716 | +--+-+-+ == Status == Current time: 2021-11-29 18:29:58 (running for 00:01:20.99) Memory usage on this node: 10.0/11.9 GiB Using FIFO scheduling algorithm. Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.7 GiB heap, 0.0/0.85 GiB objects Result logdir: C: Users Stefan ray_results PPO Number of trials: 1/1 (1 RUNNING) +--+-+-+ | Trial name | status | loc | |--+-+-| | PPO_StatelessCartPole_c9203_00000 | RUNNING | 127.0.0.1:8716 | +--+-+-+ == Status == Current time: 2021-11-29 18:30:03 (running for 00:01:26.14) Memory usage on this node: 10.0/11.9 GiB Using FIFO scheduling algorithm. Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.7 GiB heap, 0.0/0.85 GiB objects Result logdir: C: Users Stefan ray_results PPO Number of trials: 1/1 (1 RUNNING) +--+-+-+ | Trial name | status | loc | |--+-+-| | PPO_StatelessCartPole_c9203_00000 | RUNNING | 127.0.0.1:8716 | +--+-+-+ == Status == Current time: 2021-11-29 18:30:08 (running for 00:01:31.29) Memory usage on this node: 10.0/11.9 GiB Using FIFO scheduling algorithm. Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.7 GiB heap, 0.0/0.85 GiB objects Result logdir: C: Users Stefan ray_results PPO Number of trials: 1/1 (1 RUNNING) +--+-+-+ | Trial name | status | loc | |--+-+-| | PPO_StatelessCartPole_c9203_00000 | RUNNING | 127.0.0.1:8716 | +--+-+-+ == Status == Current time: 2021-11-29 18:30:13 (running for 00:01:36.39) Memory usage on this node: 10.0/11.9 GiB Using FIFO scheduling algorithm. Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.7 GiB heap, 0.0/0.85 GiB objects Result logdir: C: Users Stefan ray_results PPO Number of trials: 1/1 (1 RUNNING) +--+-+-+ | Trial name | status | loc | |--+-+-| | PPO_StatelessCartPole_c9203_00000 | RUNNING | 127.0.0.1:8716 | +--+-+-+ Result for PPO_StatelessCartPole_c9203_00000: agent_timesteps_total: 4000 custom_metrics: {} date: 2021-11-29_18-30-17 done: false episode_len_mean: 19.83 episode_media: {} episode_reward_max: 49.0 episode_reward_mean: 19.83 episode_reward_min: 9.0 episodes_this_iter: 200 episodes_total: 200 experiment_id: d11020ea6a8d4dee8d7930b4a3aca15c hostname: nb-stschn info: learner: default_policy: custom_metrics: {} learner_stats: cur_kl_coeff: 0.20000000298023224 cur_lr: 4.999999873689376e-05 entropy: 0.6852563619613647 entropy_coeff: 0.0 kl: 0.01399336475878954 model: {} policy_loss: -0.01928005926311016 total_loss: 89.16743469238281 vf_explained_var: -0.08103209733963013 vf_loss: 89.18391418457031 num_agent_steps_sampled: 4000 num_agent_steps_trained: 4000 num_steps_sampled: 4000 num_steps_trained: 4000 iterations_since_restore: 1 node_ip: 127.0.0.1 num_healthy_workers: 2 off_policy_estimator: {} perf: cpu_util_percent: 95.16162790697673 ram_util_percent: 84.42906976744186 pid: 8716 policy_reward_max: {} policy_reward_mean: {} policy_reward_min: {} sampler_perf: mean_action_processing_ms: 0.2116219619701411 mean_env_render_ms: 0.0 mean_env_wait_ms: 0.16549067399550654 mean_inference_ms: 3.319920330033309 mean_raw_obs_processing_ms: 0.2508498042419754 time_since_restore: 62.34723520278931 time_this_iter_s: 62.34723520278931 time_total_s: 62.34723520278931 timers: learn_throughput: 74.028 learn_time_ms: 54033.452 load_throughput: 0.0 load_time_ms: 0.0 sample_throughput: 480.136 sample_time_ms: 8330.974 update_time_ms: 7.528 timestamp: 1638207017 timesteps_since_restore: 0 timesteps_this_iter: 0 timesteps_total: 4000 training_iteration: 1 trial_id: c9203_00000 == Status == Current time: 2021-11-29 18:30:19 (running for 00:01:42.24) Memory usage on this node: 10.0/11.9 GiB Using FIFO scheduling algorithm. Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.7 GiB heap, 0.0/0.85 GiB objects Result logdir: C: Users Stefan ray_results PPO Number of trials: 1/1 (1 RUNNING) +--+-+-+--+++-+-+-+--+ | Trial name | status | loc | iter | total time (s) | ts | reward | episode_reward_max | episode_reward_min | episode_len_mean | |--+-+-+--+++-+-+-+--| | PPO_StatelessCartPole_c9203_00000 | RUNNING | 127.0.0.1:8716 | 1 | 62.3472 | 4000 | 19.83 | 49 | 9 | 19.83 | +--+-+-+--+++-+-+-+--+ == Status == Current time: 2021-11-29 18:30:24 (running for 00:01:47.29) Memory usage on this node: 10.0/11.9 GiB Using FIFO scheduling algorithm. Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.7 GiB heap, 0.0/0.85 GiB objects Result logdir: C: Users Stefan ray_results PPO Number of trials: 1/1 (1 RUNNING) +--+-+-+--+++-+-+-+--+ | Trial name | status | loc | iter | total time (s) | ts | reward | episode_reward_max | episode_reward_min | episode_len_mean | |--+-+-+--+++-+-+-+--| | PPO_StatelessCartPole_c9203_00000 | RUNNING | 127.0.0.1:8716 | 1 | 62.3472 | 4000 | 19.83 | 49 | 9 | 19.83 | +--+-+-+--+++-+-+-+--+ == Status == Current time: 2021-11-29 18:30:29 (running for 00:01:52.56) Memory usage on this node: 10.0/11.9 GiB Using FIFO scheduling algorithm. Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.7 GiB heap, 0.0/0.85 GiB objects Result logdir: C: Users Stefan ray_results PPO Number of trials: 1/1 (1 RUNNING) +--+-+-+--+++-+-+-+--+ | Trial name | status | loc | iter | total time (s) | ts | reward | episode_reward_max | episode_reward_min | episode_len_mean | |--+-+-+--+++-+-+-+--| | PPO_StatelessCartPole_c9203_00000 | RUNNING | 127.0.0.1:8716 | 1 | 62.3472 | 4000 | 19.83 | 49 | 9 | 19.83 | +--+-+-+--+++-+-+-+--+ == Status == Current time: 2021-11-29 18:30:34 (running for 00:01:57.64) Memory usage on this node: 10.0/11.9 GiB Using FIFO scheduling algorithm. Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.7 GiB heap, 0.0/0.85 GiB objects Result logdir: C: Users Stefan ray_results PPO Number of trials: 1/1 (1 RUNNING) +--+-+-+--+++-+-+-+--+ | Trial name | status | loc | iter | total time (s) | ts | reward | episode_reward_max | episode_reward_min | episode_len_mean | |--+-+-+--+++-+-+-+--| | PPO_StatelessCartPole_c9203_00000 | RUNNING | 127.0.0.1:8716 | 1 | 62.3472 | 4000 | 19.83 | 49 | 9 | 19.83 | +--+-+-+--+++-+-+-+--+ == Status == Current time: 2021-11-29 18:30:40 (running for 00:02:02.90) Memory usage on this node: 10.0/11.9 GiB Using FIFO scheduling algorithm. Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.7 GiB heap, 0.0/0.85 GiB objects Result logdir: C: Users Stefan ray_results PPO Number of trials: 1/1 (1 RUNNING) +--+-+-+--+++-+-+-+--+ | Trial name | status | loc | iter | total time (s) | ts | reward | episode_reward_max | episode_reward_min | episode_len_mean | |--+-+-+--+++-+-+-+--| | PPO_StatelessCartPole_c9203_00000 | RUNNING | 127.0.0.1:8716 | 1 | 62.3472 | 4000 | 19.83 | 49 | 9 | 19.83 | +--+-+-+--+++-+-+-+--+ == Status == Current time: 2021-11-29 18:30:45 (running for 00:02:08.01) Memory usage on this node: 10.0/11.9 GiB Using FIFO scheduling algorithm. Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.7 GiB heap, 0.0/0.85 GiB objects Result logdir: C: Users Stefan ray_results PPO Number of trials: 1/1 (1 RUNNING) +--+-+-+--+++-+-+-+--+ | Trial name | status | loc | iter | total time (s) | ts | reward | episode_reward_max | episode_reward_min | episode_len_mean | |--+-+-+--+++-+-+-+--| | PPO_StatelessCartPole_c9203_00000 | RUNNING | 127.0.0.1:8716 | 1 | 62.3472 | 4000 | 19.83 | 49 | 9 | 19.83 | +--+-+-+--+++-+-+-+--+ == Status == Current time: 2021-11-29 18:30:50 (running for 00:02:13.25) Memory usage on this node: 10.0/11.9 GiB Using FIFO scheduling algorithm. Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.7 GiB heap, 0.0/0.85 GiB objects Result logdir: C: Users Stefan ray_results PPO Number of trials: 1/1 (1 RUNNING) +--+-+-+--+++-+-+-+--+ | Trial name | status | loc | iter | total time (s) | ts | reward | episode_reward_max | episode_reward_min | episode_len_mean | |--+-+-+--+++-+-+-+--| | PPO_StatelessCartPole_c9203_00000 | RUNNING | 127.0.0.1:8716 | 1 | 62.3472 | 4000 | 19.83 | 49 | 9 | 19.83 | +--+-+-+--+++-+-+-+--+ == Status == Current time: 2021-11-29 18:30:55 (running for 00:02:18.42) Memory usage on this node: 10.0/11.9 GiB Using FIFO scheduling algorithm. Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.7 GiB heap, 0.0/0.85 GiB objects Result logdir: C: Users Stefan ray_results PPO Number of trials: 1/1 (1 RUNNING) +--+-+-+--+++-+-+-+--+ | Trial name | status | loc | iter | total time (s) | ts | reward | episode_reward_max | episode_reward_min | episode_len_mean | |--+-+-+--+++-+-+-+--| | PPO_StatelessCartPole_c9203_00000 | RUNNING | 127.0.0.1:8716 | 1 | 62.3472 | 4000 | 19.83 | 49 | 9 | 19.83 | +--+-+-+--+++-+-+-+--+ == Status == Current time: 2021-11-29 18:31:00 (running for 00:02:23.66) Memory usage on this node: 10.0/11.9 GiB Using FIFO scheduling algorithm. Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.7 GiB heap, 0.0/0.85 GiB objects Result logdir: C: Users Stefan ray_results PPO Number of trials: 1/1 (1 RUNNING) +--+-+-+--+++-+-+-+--+ | Trial name | status | loc | iter | total time (s) | ts | reward | episode_reward_max | episode_reward_min | episode_len_mean | |--+-+-+--+++-+-+-+--| | PPO_StatelessCartPole_c9203_00000 | RUNNING | 127.0.0.1:8716 | 1 | 62.3472 | 4000 | 19.83 | 49 | 9 | 19.83 | +--+-+-+--+++-+-+-+--+ == Status == Current time: 2021-11-29 18:31:06 (running for 00:02:28.90) Memory usage on this node: 10.1/11.9 GiB Using FIFO scheduling algorithm. Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.7 GiB heap, 0.0/0.85 GiB objects Result logdir: C: Users Stefan ray_results PPO Number of trials: 1/1 (1 RUNNING) +--+-+-+--+++-+-+-+--+ | Trial name | status | loc | iter | total time (s) | ts | reward | episode_reward_max | episode_reward_min | episode_len_mean | |--+-+-+--+++-+-+-+--| | PPO_StatelessCartPole_c9203_00000 | RUNNING | 127.0.0.1:8716 | 1 | 62.3472 | 4000 | 19.83 | 49 | 9 | 19.83 | +--+-+-+--+++-+-+-+--+ == Status == Current time: 2021-11-29 18:31:11 (running for 00:02:34.13) Memory usage on this node: 10.0/11.9 GiB Using FIFO scheduling algorithm. Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.7 GiB heap, 0.0/0.85 GiB objects Result logdir: C: Users Stefan ray_results PPO Number of trials: 1/1 (1 RUNNING) +--+-+-+--+++-+-+-+--+ | Trial name | status | loc | iter | total time (s) | ts | reward | episode_reward_max | episode_reward_min | episode_len_mean | |--+-+-+--+++-+-+-+--| | PPO_StatelessCartPole_c9203_00000 | RUNNING | 127.0.0.1:8716 | 1 | 62.3472 | 4000 | 19.83 | 49 | 9 | 19.83 | +--+-+-+--+++-+-+-+--+ == Status == Current time: 2021-11-29 18:31:16 (running for 00:02:39.24) Memory usage on this node: 10.0/11.9 GiB Using FIFO scheduling algorithm. Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.7 GiB heap, 0.0/0.85 GiB objects Result logdir: C: Users Stefan ray_results PPO Number of trials: 1/1 (1 RUNNING) +--+-+-+--+++-+-+-+--+ | Trial name | status | loc | iter | total time (s) | ts | reward | episode_reward_max | episode_reward_min | episode_len_mean | |--+-+-+--+++-+-+-+--| | PPO_StatelessCartPole_c9203_00000 | RUNNING | 127.0.0.1:8716 | 1 | 62.3472 | 4000 | 19.83 | 49 | 9 | 19.83 | +--+-+-+--+++-+-+-+--+ Result for PPO_StatelessCartPole_c9203_00000: agent_timesteps_total: 8000 custom_metrics: {} date: 2021-11-29_18-31-18 done: false episode_len_mean: 30.88372093023256 episode_media: {} episode_reward_max: 126.0 episode_reward_mean: 30.88372093023256 episode_reward_min: 10.0 episodes_this_iter: 129 episodes_total: 329 experiment_id: d11020ea6a8d4dee8d7930b4a3aca15c hostname: nb-stschn info: learner: default_policy: custom_metrics: {} learner_stats: cur_kl_coeff: 0.20000000298023224 cur_lr: 4.999999873689376e-05 entropy: 0.6626631021499634 entropy_coeff: 0.0 kl: 0.009692317806184292 model: {} policy_loss: -0.013915668241679668 total_loss: 159.7660675048828 vf_explained_var: 0.19702386856079102 vf_loss: 159.77804565429688 num_agent_steps_sampled: 8000 num_agent_steps_trained: 8000 num_steps_sampled: 8000 num_steps_trained: 8000 num_steps_trained_this_iter: 0 iterations_since_restore: 2 node_ip: 127.0.0.1 num_healthy_workers: 2 off_policy_estimator: {} perf: cpu_util_percent: 96.00357142857142 ram_util_percent: 84.33571428571429 pid: 8716 policy_reward_max: {} policy_reward_mean: {} policy_reward_min: {} sampler_perf: mean_action_processing_ms: 0.16923386110094576 mean_env_render_ms: 0.0 mean_env_wait_ms: 0.1328801176300587 mean_inference_ms: 3.08621483417635 mean_raw_obs_processing_ms: 0.21111840017877148 time_since_restore: 123.85823655128479 time_this_iter_s: 61.51100134849548 time_total_s: 123.85823655128479 timers: learn_throughput: 73.471 learn_time_ms: 54443.437 load_throughput: 0.0 load_time_ms: 0.0 sample_throughput: 115.863 sample_time_ms: 34523.416 update_time_ms: 8.809 timestamp: 1638207078 timesteps_since_restore: 0 timesteps_this_iter: 0 timesteps_total: 8000 training_iteration: 2 trial_id: c9203_00000 == Status == Current time: 2021-11-29 18:31:22 (running for 00:02:45.00) Memory usage on this node: 10.0/11.9 GiB Using FIFO scheduling algorithm. Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.7 GiB heap, 0.0/0.85 GiB objects Result logdir: C: Users Stefan ray_results PPO Number of trials: 1/1 (1 RUNNING) +--+-+-+--+++-+-+-+--+ | Trial name | status | loc | iter | total time (s) | ts | reward | episode_reward_max | episode_reward_min | episode_len_mean | |--+-+-+--+++-+-+-+--| | PPO_StatelessCartPole_c9203_00000 | RUNNING | 127.0.0.1:8716 | 2 | 123.858 | 8000 | 30.8837 | 126 | 10 | 30.8837 | +--+-+-+--+++-+-+-+--+ == Status == Current time: 2021-11-29 18:31:27 (running for 00:02:50.31) Memory usage on this node: 10.0/11.9 GiB Using FIFO scheduling algorithm. Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.7 GiB heap, 0.0/0.85 GiB objects Result logdir: C: Users Stefan ray_results PPO Number of trials: 1/1 (1 RUNNING) +--+-+-+--+++-+-+-+--+ | Trial name | status | loc | iter | total time (s) | ts | reward | episode_reward_max | episode_reward_min | episode_len_mean | |--+-+-+--+++-+-+-+--| | PPO_StatelessCartPole_c9203_00000 | RUNNING | 127.0.0.1:8716 | 2 | 123.858 | 8000 | 30.8837 | 126 | 10 | 30.8837 | +--+-+-+--+++-+-+-+--+ == Status == Current time: 2021-11-29 18:31:33 (running for 00:02:55.92) Memory usage on this node: 10.0/11.9 GiB Using FIFO scheduling algorithm. Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.7 GiB heap, 0.0/0.85 GiB objects Result logdir: C: Users Stefan ray_results PPO Number of trials: 1/1 (1 RUNNING) +--+-+-+--+++-+-+-+--+ | Trial name | status | loc | iter | total time (s) | ts | reward | episode_reward_max | episode_reward_min | episode_len_mean | |--+-+-+--+++-+-+-+--| | PPO_StatelessCartPole_c9203_00000 | RUNNING | 127.0.0.1:8716 | 2 | 123.858 | 8000 | 30.8837 | 126 | 10 | 30.8837 | +--+-+-+--+++-+-+-+--+ == Status == Current time: 2021-11-29 18:31:38 (running for 00:03:01.10) Memory usage on this node: 10.0/11.9 GiB Using FIFO scheduling algorithm. Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.7 GiB heap, 0.0/0.85 GiB objects Result logdir: C: Users Stefan ray_results PPO Number of trials: 1/1 (1 RUNNING) +--+-+-+--+++-+-+-+--+ | Trial name | status | loc | iter | total time (s) | ts | reward | episode_reward_max | episode_reward_min | episode_len_mean | |--+-+-+--+++-+-+-+--| | PPO_StatelessCartPole_c9203_00000 | RUNNING | 127.0.0.1:8716 | 2 | 123.858 | 8000 | 30.8837 | 126 | 10 | 30.8837 | +--+-+-+--+++-+-+-+--+ == Status == Current time: 2021-11-29 18:31:43 (running for 00:03:06.50) Memory usage on this node: 10.0/11.9 GiB Using FIFO scheduling algorithm. Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.7 GiB heap, 0.0/0.85 GiB objects Result logdir: C: Users Stefan ray_results PPO Number of trials: 1/1 (1 RUNNING) +--+-+-+--+++-+-+-+--+ | Trial name | status | loc | iter | total time (s) | ts | reward | episode_reward_max | episode_reward_min | episode_len_mean | |--+-+-+--+++-+-+-+--| | PPO_StatelessCartPole_c9203_00000 | RUNNING | 127.0.0.1:8716 | 2 | 123.858 | 8000 | 30.8837 | 126 | 10 | 30.8837 | +--+-+-+--+++-+-+-+--+ == Status == Current time: 2021-11-29 18:31:48 (running for 00:03:11.64) Memory usage on this node: 10.1/11.9 GiB Using FIFO scheduling algorithm. Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.7 GiB heap, 0.0/0.85 GiB objects Result logdir: C: Users Stefan ray_results PPO Number of trials: 1/1 (1 RUNNING) +--+-+-+--+++-+-+-+--+ | Trial name | status | loc | iter | total time (s) | ts | reward | episode_reward_max | episode_reward_min | episode_len_mean | |--+-+-+--+++-+-+-+--| | PPO_StatelessCartPole_c9203_00000 | RUNNING | 127.0.0.1:8716 | 2 | 123.858 | 8000 | 30.8837 | 126 | 10 | 30.8837 | +--+-+-+--+++-+-+-+--+ == Status == Current time: 2021-11-29 18:31:54 (running for 00:03:17.31) Memory usage on this node: 10.0/11.9 GiB Using FIFO scheduling algorithm. Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.7 GiB heap, 0.0/0.85 GiB objects Result logdir: C: Users Stefan ray_results PPO Number of trials: 1/1 (1 RUNNING) +--+-+-+--+++-+-+-+--+ | Trial name | status | loc | iter | total time (s) | ts | reward | episode_reward_max | episode_reward_min | episode_len_mean | |--+-+-+--+++-+-+-+--| | PPO_StatelessCartPole_c9203_00000 | RUNNING | 127.0.0.1:8716 | 2 | 123.858 | 8000 | 30.8837 | 126 | 10 | 30.8837 | +--+-+-+--+++-+-+-+--+ == Status == Current time: 2021-11-29 18:31:59 (running for 00:03:22.41) Memory usage on this node: 10.0/11.9 GiB Using FIFO scheduling algorithm. Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.7 GiB heap, 0.0/0.85 GiB objects Result logdir: C: Users Stefan ray_results PPO Number of trials: 1/1 (1 RUNNING) +--+-+-+--+++-+-+-+--+ | Trial name | status | loc | iter | total time (s) | ts | reward | episode_reward_max | episode_reward_min | episode_len_mean | |--+-+-+--+++-+-+-+--| | PPO_StatelessCartPole_c9203_00000 | RUNNING | 127.0.0.1:8716 | 2 | 123.858 | 8000 | 30.8837 | 126 | 10 | 30.8837 | +--+-+-+--+++-+-+-+--+ == Status == Current time: 2021-11-29 18:32:04 (running for 00:03:27.67) Memory usage on this node: 10.1/11.9 GiB Using FIFO scheduling algorithm. Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.7 GiB heap, 0.0/0.85 GiB objects Result logdir: C: Users Stefan ray_results PPO Number of trials: 1/1 (1 RUNNING) +--+-+-+--+++-+-+-+--+ | Trial name | status | loc | iter | total time (s) | ts | reward | episode_reward_max | episode_reward_min | episode_len_mean | |--+-+-+--+++-+-+-+--| | PPO_StatelessCartPole_c9203_00000 | RUNNING | 127.0.0.1:8716 | 2 | 123.858 | 8000 | 30.8837 | 126 | 10 | 30.8837 | +--+-+-+--+++-+-+-+--+ == Status == Current time: 2021-11-29 18:32:09 (running for 00:03:32.83) Memory usage on this node: 10.1/11.9 GiB Using FIFO scheduling algorithm. Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.7 GiB heap, 0.0/0.85 GiB objects Result logdir: C: Users Stefan ray_results PPO Number of trials: 1/1 (1 RUNNING) +--+-+-+--+++-+-+-+--+ | Trial name | status | loc | iter | total time (s) | ts | reward | episode_reward_max | episode_reward_min | episode_len_mean | |--+-+-+--+++-+-+-+--| | PPO_StatelessCartPole_c9203_00000 | RUNNING | 127.0.0.1:8716 | 2 | 123.858 | 8000 | 30.8837 | 126 | 10 | 30.8837 | +--+-+-+--+++-+-+-+--+ == Status == Current time: 2021-11-29 18:32:15 (running for 00:03:38.10) Memory usage on this node: 10.1/11.9 GiB Using FIFO scheduling algorithm. Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.7 GiB heap, 0.0/0.85 GiB objects Result logdir: C: Users Stefan ray_results PPO Number of trials: 1/1 (1 RUNNING) +--+-+-+--+++-+-+-+--+ | Trial name | status | loc | iter | total time (s) | ts | reward | episode_reward_max | episode_reward_min | episode_len_mean | |--+-+-+--+++-+-+-+--| | PPO_StatelessCartPole_c9203_00000 | RUNNING | 127.0.0.1:8716 | 2 | 123.858 | 8000 | 30.8837 | 126 | 10 | 30.8837 | +--+-+-+--+++-+-+-+--+ == Status == Current time: 2021-11-29 18:32:20 (running for 00:03:43.24) Memory usage on this node: 10.1/11.9 GiB Using FIFO scheduling algorithm. Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.7 GiB heap, 0.0/0.85 GiB objects Result logdir: C: Users Stefan ray_results PPO Number of trials: 1/1 (1 RUNNING) +--+-+-+--+++-+-+-+--+ | Trial name | status | loc | iter | total time (s) | ts | reward | episode_reward_max | episode_reward_min | episode_len_mean | |--+-+-+--+++-+-+-+--| | PPO_StatelessCartPole_c9203_00000 | RUNNING | 127.0.0.1:8716 | 2 | 123.858 | 8000 | 30.8837 | 126 | 10 | 30.8837 | +--+-+-+--+++-+-+-+--+ == Status == Current time: 2021-11-29 18:32:25 (running for 00:03:48.53) Memory usage on this node: 10.1/11.9 GiB Using FIFO scheduling algorithm. Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.7 GiB heap, 0.0/0.85 GiB objects Result logdir: C: Users Stefan ray_results PPO Number of trials: 1/1 (1 RUNNING) +--+-+-+--+++-+-+-+--+ | Trial name | status | loc | iter | total time (s) | ts | reward | episode_reward_max | episode_reward_min | episode_len_mean | |--+-+-+--+++-+-+-+--| | PPO_StatelessCartPole_c9203_00000 | RUNNING | 127.0.0.1:8716 | 2 | 123.858 | 8000 | 30.8837 | 126 | 10 | 30.8837 | +--+-+-+--+++-+-+-+--+ == Status == Current time: 2021-11-29 18:32:30 (running for 00:03:53.64) Memory usage on this node: 10.1/11.9 GiB Using FIFO scheduling algorithm. Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.7 GiB heap, 0.0/0.85 GiB objects Result logdir: C: Users Stefan ray_results PPO Number of trials: 1/1 (1 RUNNING) +--+-+-+--+++-+-+-+--+ | Trial name | status | loc | iter | total time (s) | ts | reward | episode_reward_max | episode_reward_min | episode_len_mean | |--+-+-+--+++-+-+-+--| | PPO_StatelessCartPole_c9203_00000 | RUNNING | 127.0.0.1:8716 | 2 | 123.858 | 8000 | 30.8837 | 126 | 10 | 30.8837 | +--+-+-+--+++-+-+-+--+ Result for PPO_StatelessCartPole_c9203_00000: agent_timesteps_total: 12000 custom_metrics: {} date: 2021-11-29_18-32-34 done: false episode_len_mean: 35.48672566371681 episode_media: {} episode_reward_max: 122.0 episode_reward_mean: 35.48672566371681 episode_reward_min: 9.0 episodes_this_iter: 113 episodes_total: 442 experiment_id: d11020ea6a8d4dee8d7930b4a3aca15c hostname: nb-stschn info: learner: default_policy: custom_metrics: {} learner_stats: cur_kl_coeff: 0.20000000298023224 cur_lr: 4.999999873689376e-05 entropy: 0.6404949426651001 entropy_coeff: 0.0 kl: 0.004881380125880241 model: {} policy_loss: -0.0072944313287734985 total_loss: 181.40847778320312 vf_explained_var: 0.19194267690181732 vf_loss: 181.414794921875 num_agent_steps_sampled: 12000 num_agent_steps_trained: 12000 num_steps_sampled: 12000 num_steps_trained: 12000 num_steps_trained_this_iter: 0 iterations_since_restore: 3 node_ip: 127.0.0.1 num_healthy_workers: 2 off_policy_estimator: {} perf: cpu_util_percent: 98.322 ram_util_percent: 84.56099999999998 pid: 8716 policy_reward_max: {} policy_reward_mean: {} policy_reward_min: {} sampler_perf: mean_action_processing_ms: 0.15931838910400126 mean_env_render_ms: 0.0 mean_env_wait_ms: 0.1322726976606865 mean_inference_ms: 3.163183476404039 mean_raw_obs_processing_ms: 0.21592900502523038 time_since_restore: 199.92653393745422 time_this_iter_s: 76.06829738616943 time_total_s: 199.92653393745422 timers: learn_throughput: 67.787 learn_time_ms: 59008.592 load_throughput: 1707199.24 load_time_ms: 2.343 sample_throughput: 90.985 sample_time_ms: 43963.426 update_time_ms: 8.874 timestamp: 1638207154 timesteps_since_restore: 0 timesteps_this_iter: 0 timesteps_total: 12000 training_iteration: 3 trial_id: c9203_00000 == Status == Current time: 2021-11-29 18:32:36 (running for 00:03:58.98) Memory usage on this node: 10.1/11.9 GiB Using FIFO scheduling algorithm. Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.7 GiB heap, 0.0/0.85 GiB objects Result logdir: C: Users Stefan ray_results PPO Number of trials: 1/1 (1 RUNNING) +--+-+-+--++-+-+-+-+--+ | Trial name | status | loc | iter | total time (s) | ts | reward | episode_reward_max | episode_reward_min | episode_len_mean | |--+-+-+--++-+-+-+-+--| | PPO_StatelessCartPole_c9203_00000 | RUNNING | 127.0.0.1:8716 | 3 | 199.927 | 12000 | 35.4867 | 122 | 9 | 35.4867 | +--+-+-+--++-+-+-+-+--+ == Status == Current time: 2021-11-29 18:32:41 (running for 00:04:04.15) Memory usage on this node: 10.0/11.9 GiB Using FIFO scheduling algorithm. Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.7 GiB heap, 0.0/0.85 GiB objects Result logdir: C: Users Stefan ray_results PPO Number of trials: 1/1 (1 RUNNING) +--+-+-+--++-+-+-+-+--+ | Trial name | status | loc | iter | total time (s) | ts | reward | episode_reward_max | episode_reward_min | episode_len_mean | |--+-+-+--++-+-+-+-+--| | PPO_StatelessCartPole_c9203_00000 | RUNNING | 127.0.0.1:8716 | 3 | 199.927 | 12000 | 35.4867 | 122 | 9 | 35.4867 | +--+-+-+--++-+-+-+-+--+ == Status == Current time: 2021-11-29 18:32:46 (running for 00:04:09.45) Memory usage on this node: 9.8/11.9 GiB Using FIFO scheduling algorithm. Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.7 GiB heap, 0.0/0.85 GiB objects Result logdir: C: Users Stefan ray_results PPO Number of trials: 1/1 (1 RUNNING) +--+-+-+--++-+-+-+-+--+ | Trial name | status | loc | iter | total time (s) | ts | reward | episode_reward_max | episode_reward_min | episode_len_mean | |--+-+-+--++-+-+-+-+--| | PPO_StatelessCartPole_c9203_00000 | RUNNING | 127.0.0.1:8716 | 3 | 199.927 | 12000 | 35.4867 | 122 | 9 | 35.4867 | +--+-+-+--++-+-+-+-+--+ == Status == Current time: 2021-11-29 18:32:51 (running for 00:04:14.60) Memory usage on this node: 9.8/11.9 GiB Using FIFO scheduling algorithm. Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.7 GiB heap, 0.0/0.85 GiB objects Result logdir: C: Users Stefan ray_results PPO Number of trials: 1/1 (1 RUNNING) +--+-+-+--++-+-+-+-+--+ | Trial name | status | loc | iter | total time (s) | ts | reward | episode_reward_max | episode_reward_min | episode_len_mean | |--+-+-+--++-+-+-+-+--| | PPO_StatelessCartPole_c9203_00000 | RUNNING | 127.0.0.1:8716 | 3 | 199.927 | 12000 | 35.4867 | 122 | 9 | 35.4867 | +--+-+-+--++-+-+-+-+--+ == Status == Current time: 2021-11-29 18:32:56 (running for 00:04:19.86) Memory usage on this node: 9.8/11.9 GiB Using FIFO scheduling algorithm. Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.7 GiB heap, 0.0/0.85 GiB objects Result logdir: C: Users Stefan ray_results PPO Number of trials: 1/1 (1 RUNNING) +--+-+-+--++-+-+-+-+--+ | Trial name | status | loc | iter | total time (s) | ts | reward | episode_reward_max | episode_reward_min | episode_len_mean | |--+-+-+--++-+-+-+-+--| | PPO_StatelessCartPole_c9203_00000 | RUNNING | 127.0.0.1:8716 | 3 | 199.927 | 12000 | 35.4867 | 122 | 9 | 35.4867 | +--+-+-+--++-+-+-+-+--+ == Status == Current time: 2021-11-29 18:33:02 (running for 00:04:24.97) Memory usage on this node: 9.8/11.9 GiB Using FIFO scheduling algorithm. Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.7 GiB heap, 0.0/0.85 GiB objects Result logdir: C: Users Stefan ray_results PPO Number of trials: 1/1 (1 RUNNING) +--+-+-+--++-+-+-+-+--+ | Trial name | status | loc | iter | total time (s) | ts | reward | episode_reward_max | episode_reward_min | episode_len_mean | |--+-+-+--++-+-+-+-+--| | PPO_StatelessCartPole_c9203_00000 | RUNNING | 127.0.0.1:8716 | 3 | 199.927 | 12000 | 35.4867 | 122 | 9 | 35.4867 | +--+-+-+--++-+-+-+-+--+ == Status == Current time: 2021-11-29 18:33:07 (running for 00:04:30.56) Memory usage on this node: 9.8/11.9 GiB Using FIFO scheduling algorithm. Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.7 GiB heap, 0.0/0.85 GiB objects Result logdir: C: Users Stefan ray_results PPO Number of trials: 1/1 (1 RUNNING) +--+-+-+--++-+-+-+-+--+ | Trial name | status | loc | iter | total time (s) | ts | reward | episode_reward_max | episode_reward_min | episode_len_mean | |--+-+-+--++-+-+-+-+--| | PPO_StatelessCartPole_c9203_00000 | RUNNING | 127.0.0.1:8716 | 3 | 199.927 | 12000 | 35.4867 | 122 | 9 | 35.4867 | +--+-+-+--++-+-+-+-+--+ == Status == Current time: 2021-11-29 18:33:12 (running for 00:04:35.72) Memory usage on this node: 9.8/11.9 GiB Using FIFO scheduling algorithm. Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.7 GiB heap, 0.0/0.85 GiB objects Result logdir: C: Users Stefan ray_results PPO Number of trials: 1/1 (1 RUNNING) +--+-+-+--++-+-+-+-+--+ | Trial name | status | loc | iter | total time (s) | ts | reward | episode_reward_max | episode_reward_min | episode_len_mean | |--+-+-+--++-+-+-+-+--| | PPO_StatelessCartPole_c9203_00000 | RUNNING | 127.0.0.1:8716 | 3 | 199.927 | 12000 | 35.4867 | 122 | 9 | 35.4867 | +--+-+-+--++-+-+-+-+--+ == Status == Current time: 2021-11-29 18:33:18 (running for 00:04:40.90) Memory usage on this node: 9.8/11.9 GiB Using FIFO scheduling algorithm. Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.7 GiB heap, 0.0/0.85 GiB objects Result logdir: C: Users Stefan ray_results PPO Number of trials: 1/1 (1 RUNNING) +--+-+-+--++-+-+-+-+--+ | Trial name | status | loc | iter | total time (s) | ts | reward | episode_reward_max | episode_reward_min | episode_len_mean | |--+-+-+--++-+-+-+-+--| | PPO_StatelessCartPole_c9203_00000 | RUNNING | 127.0.0.1:8716 | 3 | 199.927 | 12000 | 35.4867 | 122 | 9 | 35.4867 | +--+-+-+--++-+-+-+-+--+ == Status == Current time: 2021-11-29 18:33:23 (running for 00:04:46.03) Memory usage on this node: 9.8/11.9 GiB Using FIFO scheduling algorithm. Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.7 GiB heap, 0.0/0.85 GiB objects Result logdir: C: Users Stefan ray_results PPO Number of trials: 1/1 (1 RUNNING) +--+-+-+--++-+-+-+-+--+ | Trial name | status | loc | iter | total time (s) | ts | reward | episode_reward_max | episode_reward_min | episode_len_mean | |--+-+-+--++-+-+-+-+--| | PPO_StatelessCartPole_c9203_00000 | RUNNING | 127.0.0.1:8716 | 3 | 199.927 | 12000 | 35.4867 | 122 | 9 | 35.4867 | +--+-+-+--++-+-+-+-+--+ == Status == Current time: 2021-11-29 18:33:28 (running for 00:04:51.43) Memory usage on this node: 9.8/11.9 GiB Using FIFO scheduling algorithm. Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.7 GiB heap, 0.0/0.85 GiB objects Result logdir: C: Users Stefan ray_results PPO Number of trials: 1/1 (1 RUNNING) +--+-+-+--++-+-+-+-+--+ | Trial name | status | loc | iter | total time (s) | ts | reward | episode_reward_max | episode_reward_min | episode_len_mean | |--+-+-+--++-+-+-+-+--| | PPO_StatelessCartPole_c9203_00000 | RUNNING | 127.0.0.1:8716 | 3 | 199.927 | 12000 | 35.4867 | 122 | 9 | 35.4867 | +--+-+-+--++-+-+-+-+--+ == Status == Current time: 2021-11-29 18:33:33 (running for 00:04:56.58) Memory usage on this node: 9.8/11.9 GiB Using FIFO scheduling algorithm. Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.7 GiB heap, 0.0/0.85 GiB objects Result logdir: C: Users Stefan ray_results PPO Number of trials: 1/1 (1 RUNNING) +--+-+-+--++-+-+-+-+--+ | Trial name | status | loc | iter | total time (s) | ts | reward | episode_reward_max | episode_reward_min | episode_len_mean | |--+-+-+--++-+-+-+-+--| | PPO_StatelessCartPole_c9203_00000 | RUNNING | 127.0.0.1:8716 | 3 | 199.927 | 12000 | 35.4867 | 122 | 9 | 35.4867 | +--+-+-+--++-+-+-+-+--+ == Status == Current time: 2021-11-29 18:33:38 (running for 00:05:01.80) Memory usage on this node: 9.8/11.9 GiB Using FIFO scheduling algorithm. Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.7 GiB heap, 0.0/0.85 GiB objects Result logdir: C: Users Stefan ray_results PPO Number of trials: 1/1 (1 RUNNING) +--+-+-+--++-+-+-+-+--+ | Trial name | status | loc | iter | total time (s) | ts | reward | episode_reward_max | episode_reward_min | episode_len_mean | |--+-+-+--++-+-+-+-+--| | PPO_StatelessCartPole_c9203_00000 | RUNNING | 127.0.0.1:8716 | 3 | 199.927 | 12000 | 35.4867 | 122 | 9 | 35.4867 | +--+-+-+--++-+-+-+-+--+ == Status == Current time: 2021-11-29 18:33:44 (running for 00:05:06.97) Memory usage on this node: 9.8/11.9 GiB Using FIFO scheduling algorithm. Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.7 GiB heap, 0.0/0.85 GiB objects Result logdir: C: Users Stefan ray_results PPO Number of trials: 1/1 (1 RUNNING) +--+-+-+--++-+-+-+-+--+ | Trial name | status | loc | iter | total time (s) | ts | reward | episode_reward_max | episode_reward_min | episode_len_mean | |--+-+-+--++-+-+-+-+--| | PPO_StatelessCartPole_c9203_00000 | RUNNING | 127.0.0.1:8716 | 3 | 199.927 | 12000 | 35.4867 | 122 | 9 | 35.4867 | +--+-+-+--++-+-+-+-+--+ == Status == Current time: 2021-11-29 18:33:49 (running for 00:05:12.29) Memory usage on this node: 9.8/11.9 GiB Using FIFO scheduling algorithm. Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.7 GiB heap, 0.0/0.85 GiB objects Result logdir: C: Users Stefan ray_results PPO Number of trials: 1/1 (1 RUNNING) +--+-+-+--++-+-+-+-+--+ | Trial name | status | loc | iter | total time (s) | ts | reward | episode_reward_max | episode_reward_min | episode_len_mean | |--+-+-+--++-+-+-+-+--| | PPO_StatelessCartPole_c9203_00000 | RUNNING | 127.0.0.1:8716 | 3 | 199.927 | 12000 | 35.4867 | 122 | 9 | 35.4867 | +--+-+-+--++-+-+-+-+--+ Result for PPO_StatelessCartPole_c9203_00000: agent_timesteps_total: 16000 custom_metrics: {} date: 2021-11-29_18-33-53 done: false episode_len_mean: 32.578512396694215 episode_media: {} episode_reward_max: 139.0 episode_reward_mean: 32.578512396694215 episode_reward_min: 9.0 episodes_this_iter: 121 episodes_total: 563 experiment_id: d11020ea6a8d4dee8d7930b4a3aca15c hostname: nb-stschn info: learner: default_policy: custom_metrics: {} learner_stats: cur_kl_coeff: 0.10000000149011612 cur_lr: 4.999999873689376e-05 entropy: 0.6078895330429077 entropy_coeff: 0.0 kl: 0.009514451026916504 model: {} policy_loss: -0.0011090048355981708 total_loss: 205.1411590576172 vf_explained_var: 0.18781685829162598 vf_loss: 205.14129638671875 num_agent_steps_sampled: 16000 num_agent_steps_trained: 16000 num_steps_sampled: 16000 num_steps_trained: 16000 num_steps_trained_this_iter: 0 iterations_since_restore: 4 node_ip: 127.0.0.1 num_healthy_workers: 2 off_policy_estimator: {} perf: cpu_util_percent: 99.7701923076923 ram_util_percent: 82.52499999999999 pid: 8716 policy_reward_max: {} policy_reward_mean: {} policy_reward_min: {} sampler_perf: mean_action_processing_ms: 0.15689486860298657 mean_env_render_ms: 0.0 mean_env_wait_ms: 0.14005257664806203 mean_inference_ms: 3.3369294314660523 mean_raw_obs_processing_ms: 0.23163356969805826 time_since_restore: 278.696261882782 time_this_iter_s: 78.76972794532776 time_total_s: 278.696261882782 timers: learn_throughput: 64.884 learn_time_ms: 61648.382 load_throughput: 1992898.497 load_time_ms: 2.007 sample_throughput: 76.433 sample_time_ms: 52333.078 update_time_ms: 6.656 timestamp: 1638207233 timesteps_since_restore: 0 timesteps_this_iter: 0 timesteps_total: 16000 training_iteration: 4 trial_id: c9203_00000 == Status == Current time: 2021-11-29 18:33:54 (running for 00:05:17.81) Memory usage on this node: 9.8/11.9 GiB Using FIFO scheduling algorithm. Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.7 GiB heap, 0.0/0.85 GiB objects Result logdir: C: Users Stefan ray_results PPO Number of trials: 1/1 (1 RUNNING) +--+-+-+--++-+-+-+-+--+ | Trial name | status | loc | iter | total time (s) | ts | reward | episode_reward_max | episode_reward_min | episode_len_mean | |--+-+-+--++-+-+-+-+--| | PPO_StatelessCartPole_c9203_00000 | RUNNING | 127.0.0.1:8716 | 4 | 278.696 | 16000 | 32.5785 | 139 | 9 | 32.5785 | +--+-+-+--++-+-+-+-+--+ == Status == Current time: 2021-11-29 18:34:00 (running for 00:05:23.19) Memory usage on this node: 9.8/11.9 GiB Using FIFO scheduling algorithm. Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.7 GiB heap, 0.0/0.85 GiB objects Result logdir: C: Users Stefan ray_results PPO Number of trials: 1/1 (1 RUNNING) +--+-+-+--++-+-+-+-+--+ | Trial name | status | loc | iter | total time (s) | ts | reward | episode_reward_max | episode_reward_min | episode_len_mean | |--+-+-+--++-+-+-+-+--| | PPO_StatelessCartPole_c9203_00000 | RUNNING | 127.0.0.1:8716 | 4 | 278.696 | 16000 | 32.5785 | 139 | 9 | 32.5785 | +--+-+-+--++-+-+-+-+--+ == Status == Current time: 2021-11-29 18:34:05 (running for 00:05:28.42) Memory usage on this node: 9.8/11.9 GiB Using FIFO scheduling algorithm. Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.7 GiB heap, 0.0/0.85 GiB objects Result logdir: C: Users Stefan ray_results PPO Number of trials: 1/1 (1 RUNNING) +--+-+-+--++-+-+-+-+--+ | Trial name | status | loc | iter | total time (s) | ts | reward | episode_reward_max | episode_reward_min | episode_len_mean | |--+-+-+--++-+-+-+-+--| | PPO_StatelessCartPole_c9203_00000 | RUNNING | 127.0.0.1:8716 | 4 | 278.696 | 16000 | 32.5785 | 139 | 9 | 32.5785 | +--+-+-+--++-+-+-+-+--+ == Status == Current time: 2021-11-29 18:34:10 (running for 00:05:33.62) Memory usage on this node: 9.8/11.9 GiB Using FIFO scheduling algorithm. Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.7 GiB heap, 0.0/0.85 GiB objects Result logdir: C: Users Stefan ray_results PPO Number of trials: 1/1 (1 RUNNING) +--+-+-+--++-+-+-+-+--+ | Trial name | status | loc | iter | total time (s) | ts | reward | episode_reward_max | episode_reward_min | episode_len_mean | |--+-+-+--++-+-+-+-+--| | PPO_StatelessCartPole_c9203_00000 | RUNNING | 127.0.0.1:8716 | 4 | 278.696 | 16000 | 32.5785 | 139 | 9 | 32.5785 | +--+-+-+--++-+-+-+-+--+ == Status == Current time: 2021-11-29 18:34:16 (running for 00:05:38.94) Memory usage on this node: 9.8/11.9 GiB Using FIFO scheduling algorithm. Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.7 GiB heap, 0.0/0.85 GiB objects Result logdir: C: Users Stefan ray_results PPO Number of trials: 1/1 (1 RUNNING) +--+-+-+--++-+-+-+-+--+ | Trial name | status | loc | iter | total time (s) | ts | reward | episode_reward_max | episode_reward_min | episode_len_mean | |--+-+-+--++-+-+-+-+--| | PPO_StatelessCartPole_c9203_00000 | RUNNING | 127.0.0.1:8716 | 4 | 278.696 | 16000 | 32.5785 | 139 | 9 | 32.5785 | +--+-+-+--++-+-+-+-+--+ == Status == Current time: 2021-11-29 18:34:21 (running for 00:05:44.34) Memory usage on this node: 9.8/11.9 GiB Using FIFO scheduling algorithm. Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.7 GiB heap, 0.0/0.85 GiB objects Result logdir: C: Users Stefan ray_results PPO Number of trials: 1/1 (1 RUNNING) +--+-+-+--++-+-+-+-+--+ | Trial name | status | loc | iter | total time (s) | ts | reward | episode_reward_max | episode_reward_min | episode_len_mean | |--+-+-+--++-+-+-+-+--| | PPO_StatelessCartPole_c9203_00000 | RUNNING | 127.0.0.1:8716 | 4 | 278.696 | 16000 | 32.5785 | 139 | 9 | 32.5785 | +--+-+-+--++-+-+-+-+--+ == Status == Current time: 2021-11-29 18:34:26 (running for 00:05:49.53) Memory usage on this node: 9.8/11.9 GiB Using FIFO scheduling algorithm. Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.7 GiB heap, 0.0/0.85 GiB objects Result logdir: C: Users Stefan ray_results PPO Number of trials: 1/1 (1 RUNNING) +--+-+-+--++-+-+-+-+--+ | Trial name | status | loc | iter | total time (s) | ts | reward | episode_reward_max | episode_reward_min | episode_len_mean | |--+-+-+--++-+-+-+-+--| | PPO_StatelessCartPole_c9203_00000 | RUNNING | 127.0.0.1:8716 | 4 | 278.696 | 16000 | 32.5785 | 139 | 9 | 32.5785 | +--+-+-+--++-+-+-+-+--+ == Status == Current time: 2021-11-29 18:34:31 (running for 00:05:54.68) Memory usage on this node: 9.8/11.9 GiB Using FIFO scheduling algorithm. Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.7 GiB heap, 0.0/0.85 GiB objects Result logdir: C: Users Stefan ray_results PPO Number of trials: 1/1 (1 RUNNING) +--+-+-+--++-+-+-+-+--+ | Trial name | status | loc | iter | total time (s) | ts | reward | episode_reward_max | episode_reward_min | episode_len_mean | |--+-+-+--++-+-+-+-+--| | PPO_StatelessCartPole_c9203_00000 | RUNNING | 127.0.0.1:8716 | 4 | 278.696 | 16000 | 32.5785 | 139 | 9 | 32.5785 | +--+-+-+--++-+-+-+-+--+ == Status == Current time: 2021-11-29 18:34:37 (running for 00:06:00.02) Memory usage on this node: 9.8/11.9 GiB Using FIFO scheduling algorithm. Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.7 GiB heap, 0.0/0.85 GiB objects Result logdir: C: Users Stefan ray_results PPO Number of trials: 1/1 (1 RUNNING) +--+-+-+--++-+-+-+-+--+ | Trial name | status | loc | iter | total time (s) | ts | reward | episode_reward_max | episode_reward_min | episode_len_mean | |--+-+-+--++-+-+-+-+--| | PPO_StatelessCartPole_c9203_00000 | RUNNING | 127.0.0.1:8716 | 4 | 278.696 | 16000 | 32.5785 | 139 | 9 | 32.5785 | +--+-+-+--++-+-+-+-+--+ == Status == Current time: 2021-11-29 18:34:42 (running for 00:06:05.28) Memory usage on this node: 9.8/11.9 GiB Using FIFO scheduling algorithm. Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.7 GiB heap, 0.0/0.85 GiB objects Result logdir: C: Users Stefan ray_results PPO Number of trials: 1/1 (1 RUNNING) +--+-+-+--++-+-+-+-+--+ | Trial name | status | loc | iter | total time (s) | ts | reward | episode_reward_max | episode_reward_min | episode_len_mean | |--+-+-+--++-+-+-+-+--| | PPO_StatelessCartPole_c9203_00000 | RUNNING | 127.0.0.1:8716 | 4 | 278.696 | 16000 | 32.5785 | 139 | 9 | 32.5785 | +--+-+-+--++-+-+-+-+--+ == Status == Current time: 2021-11-29 18:34:47 (running for 00:06:10.37) Memory usage on this node: 9.8/11.9 GiB Using FIFO scheduling algorithm. Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.7 GiB heap, 0.0/0.85 GiB objects Result logdir: C: Users Stefan ray_results PPO Number of trials: 1/1 (1 RUNNING) +--+-+-+--++-+-+-+-+--+ | Trial name | status | loc | iter | total time (s) | ts | reward | episode_reward_max | episode_reward_min | episode_len_mean | |--+-+-+--++-+-+-+-+--| | PPO_StatelessCartPole_c9203_00000 | RUNNING | 127.0.0.1:8716 | 4 | 278.696 | 16000 | 32.5785 | 139 | 9 | 32.5785 | +--+-+-+--++-+-+-+-+--+ == Status == Current time: 2021-11-29 18:34:52 (running for 00:06:15.58) Memory usage on this node: 9.8/11.9 GiB Using FIFO scheduling algorithm. Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.7 GiB heap, 0.0/0.85 GiB objects Result logdir: C: Users Stefan ray_results PPO Number of trials: 1/1 (1 RUNNING) +--+-+-+--++-+-+-+-+--+ | Trial name | status | loc | iter | total time (s) | ts | reward | episode_reward_max | episode_reward_min | episode_len_mean | |--+-+-+--++-+-+-+-+--| | PPO_StatelessCartPole_c9203_00000 | RUNNING | 127.0.0.1:8716 | 4 | 278.696 | 16000 | 32.5785 | 139 | 9 | 32.5785 | +--+-+-+--++-+-+-+-+--+ == Status == Current time: 2021-11-29 18:34:57 (running for 00:06:20.75) Memory usage on this node: 9.8/11.9 GiB Using FIFO scheduling algorithm. Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.7 GiB heap, 0.0/0.85 GiB objects Result logdir: C: Users Stefan ray_results PPO Number of trials: 1/1 (1 RUNNING) +--+-+-+--++-+-+-+-+--+ | Trial name | status | loc | iter | total time (s) | ts | reward | episode_reward_max | episode_reward_min | episode_len_mean | |--+-+-+--++-+-+-+-+--| | PPO_StatelessCartPole_c9203_00000 | RUNNING | 127.0.0.1:8716 | 4 | 278.696 | 16000 | 32.5785 | 139 | 9 | 32.5785 | +--+-+-+--++-+-+-+-+--+ == Status == Current time: 2021-11-29 18:35:03 (running for 00:06:25.98) Memory usage on this node: 9.8/11.9 GiB Using FIFO scheduling algorithm. Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.7 GiB heap, 0.0/0.85 GiB objects Result logdir: C: Users Stefan ray_results PPO Number of trials: 1/1 (1 RUNNING) +--+-+-+--++-+-+-+-+--+ | Trial name | status | loc | iter | total time (s) | ts | reward | episode_reward_max | episode_reward_min | episode_len_mean | |--+-+-+--++-+-+-+-+--| | PPO_StatelessCartPole_c9203_00000 | RUNNING | 127.0.0.1:8716 | 4 | 278.696 | 16000 | 32.5785 | 139 | 9 | 32.5785 | +--+-+-+--++-+-+-+-+--+ == Status == Current time: 2021-11-29 18:35:08 (running for 00:06:31.33) Memory usage on this node: 9.8/11.9 GiB Using FIFO scheduling algorithm. Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.7 GiB heap, 0.0/0.85 GiB objects Result logdir: C: Users Stefan ray_results PPO Number of trials: 1/1 (1 RUNNING) +--+-+-+--++-+-+-+-+--+ | Trial name | status | loc | iter | total time (s) | ts | reward | episode_reward_max | episode_reward_min | episode_len_mean | |--+-+-+--++-+-+-+-+--| | PPO_StatelessCartPole_c9203_00000 | RUNNING | 127.0.0.1:8716 | 4 | 278.696 | 16000 | 32.5785 | 139 | 9 | 32.5785 | +--+-+-+--++-+-+-+-+--+ Result for PPO_StatelessCartPole_c9203_00000: agent_timesteps_total: 20000 custom_metrics: {} date: 2021-11-29_18-35-10 done: false episode_len_mean: 40.0 episode_media: {} episode_reward_max: 135.0 episode_reward_mean: 40.0 episode_reward_min: 9.0 episodes_this_iter: 100 episodes_total: 663 experiment_id: d11020ea6a8d4dee8d7930b4a3aca15c hostname: nb-stschn info: learner: default_policy: custom_metrics: {} learner_stats: cur_kl_coeff: 0.10000000149011612 cur_lr: 4.999999873689376e-05 entropy: 0.6035186648368835 entropy_coeff: 0.0 kl: 0.01621817611157894 model: {} policy_loss: 0.011358164250850677 total_loss: 190.80917358398438 vf_explained_var: 0.1807851344347 vf_loss: 190.7961883544922 num_agent_steps_sampled: 20000 num_agent_steps_trained: 20000 num_steps_sampled: 20000 num_steps_trained: 20000 num_steps_trained_this_iter: 0 iterations_since_restore: 5 node_ip: 127.0.0.1 num_healthy_workers: 2 off_policy_estimator: {} perf: cpu_util_percent: 98.77623762376237 ram_util_percent: 82.390099009901 pid: 8716 policy_reward_max: {} policy_reward_mean: {} policy_reward_min: {} sampler_perf: mean_action_processing_ms: 0.15449772047026258 mean_env_render_ms: 0.0 mean_env_wait_ms: 0.15324612520132258 mean_inference_ms: 3.4809547743193203 mean_raw_obs_processing_ms: 0.24795529079751927 time_since_restore: 355.3572907447815 time_this_iter_s: 76.66102886199951 time_total_s: 355.3572907447815 timers: learn_throughput: 63.859 learn_time_ms: 62637.608 load_throughput: 2491123.122 load_time_ms: 1.606 sample_throughput: 69.195 sample_time_ms: 57807.309 update_time_ms: 6.744 timestamp: 1638207310 timesteps_since_restore: 0 timesteps_this_iter: 0 timesteps_total: 20000 training_iteration: 5 trial_id: c9203_00000 == Status == Current time: 2021-11-29 18:35:13 (running for 00:06:36.57) Memory usage on this node: 9.8/11.9 GiB Using FIFO scheduling algorithm. Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.7 GiB heap, 0.0/0.85 GiB objects Result logdir: C: Users Stefan ray_results PPO Number of trials: 1/1 (1 RUNNING) +--+-+-+--++-+-+-+-+--+ | Trial name | status | loc | iter | total time (s) | ts | reward | episode_reward_max | episode_reward_min | episode_len_mean | |--+-+-+--++-+-+-+-+--| | PPO_StatelessCartPole_c9203_00000 | RUNNING | 127.0.0.1:8716 | 5 | 355.357 | 20000 | 40 | 135 | 9 | 40 | +--+-+-+--++-+-+-+-+--+ == Status == Current time: 2021-11-29 18:35:18 (running for 00:06:41.81) Memory usage on this node: 9.8/11.9 GiB Using FIFO scheduling algorithm. Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.7 GiB heap, 0.0/0.85 GiB objects Result logdir: C: Users Stefan ray_results PPO Number of trials: 1/1 (1 RUNNING) +--+-+-+--++-+-+-+-+--+ | Trial name | status | loc | iter | total time (s) | ts | reward | episode_reward_max | episode_reward_min | episode_len_mean | |--+-+-+--++-+-+-+-+--| | PPO_StatelessCartPole_c9203_00000 | RUNNING | 127.0.0.1:8716 | 5 | 355.357 | 20000 | 40 | 135 | 9 | 40 | +--+-+-+--++-+-+-+-+--+ == Status == Current time: 2021-11-29 18:35:24 (running for 00:06:47.23) Memory usage on this node: 9.7/11.9 GiB Using FIFO scheduling algorithm. Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.7 GiB heap, 0.0/0.85 GiB objects Result logdir: C: Users Stefan ray_results PPO Number of trials: 1/1 (1 RUNNING) +--+-+-+--++-+-+-+-+--+ | Trial name | status | loc | iter | total time (s) | ts | reward | episode_reward_max | episode_reward_min | episode_len_mean | |--+-+-+--++-+-+-+-+--| | PPO_StatelessCartPole_c9203_00000 | RUNNING | 127.0.0.1:8716 | 5 | 355.357 | 20000 | 40 | 135 | 9 | 40 | +--+-+-+--++-+-+-+-+--+ == Status == Current time: 2021-11-29 18:35:29 (running for 00:06:52.50) Memory usage on this node: 9.7/11.9 GiB Using FIFO scheduling algorithm. Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.7 GiB heap, 0.0/0.85 GiB objects Result logdir: C: Users Stefan ray_results PPO Number of trials: 1/1 (1 RUNNING) +--+-+-+--++-+-+-+-+--+ | Trial name | status | loc | iter | total time (s) | ts | reward | episode_reward_max | episode_reward_min | episode_len_mean | |--+-+-+--++-+-+-+-+--| | PPO_StatelessCartPole_c9203_00000 | RUNNING | 127.0.0.1:8716 | 5 | 355.357 | 20000 | 40 | 135 | 9 | 40 | +--+-+-+--++-+-+-+-+--+ == Status == Current time: 2021-11-29 18:35:34 (running for 00:06:57.79) Memory usage on this node: 9.8/11.9 GiB Using FIFO scheduling algorithm. Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.7 GiB heap, 0.0/0.85 GiB objects Result logdir: C: Users Stefan ray_results PPO Number of trials: 1/1 (1 RUNNING) +--+-+-+--++-+-+-+-+--+ | Trial name | status | loc | iter | total time (s) | ts | reward | episode_reward_max | episode_reward_min | episode_len_mean | |--+-+-+--++-+-+-+-+--| | PPO_StatelessCartPole_c9203_00000 | RUNNING | 127.0.0.1:8716 | 5 | 355.357 | 20000 | 40 | 135 | 9 | 40 | +--+-+-+--++-+-+-+-+--+ == Status == Current time: 2021-11-29 18:35:40 (running for 00:07:02.95) Memory usage on this node: 9.8/11.9 GiB Using FIFO scheduling algorithm. Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.7 GiB heap, 0.0/0.85 GiB objects Result logdir: C: Users Stefan ray_results PPO Number of trials: 1/1 (1 RUNNING) +--+-+-+--++-+-+-+-+--+ | Trial name | status | loc | iter | total time (s) | ts | reward | episode_reward_max | episode_reward_min | episode_len_mean | |--+-+-+--++-+-+-+-+--| | PPO_StatelessCartPole_c9203_00000 | RUNNING | 127.0.0.1:8716 | 5 | 355.357 | 20000 | 40 | 135 | 9 | 40 | +--+-+-+--++-+-+-+-+--+ == Status == Current time: 2021-11-29 18:35:45 (running for 00:07:08.35) Memory usage on this node: 9.8/11.9 GiB Using FIFO scheduling algorithm. Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.7 GiB heap, 0.0/0.85 GiB objects Result logdir: C: Users Stefan ray_results PPO Number of trials: 1/1 (1 RUNNING) +--+-+-+--++-+-+-+-+--+ | Trial name | status | loc | iter | total time (s) | ts | reward | episode_reward_max | episode_reward_min | episode_len_mean | |--+-+-+--++-+-+-+-+--| | PPO_StatelessCartPole_c9203_00000 | RUNNING | 127.0.0.1:8716 | 5 | 355.357 | 20000 | 40 | 135 | 9 | 40 | +--+-+-+--++-+-+-+-+--+ == Status == Current time: 2021-11-29 18:35:50 (running for 00:07:13.76) Memory usage on this node: 9.8/11.9 GiB Using FIFO scheduling algorithm. Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.7 GiB heap, 0.0/0.85 GiB objects Result logdir: C: Users Stefan ray_results PPO Number of trials: 1/1 (1 RUNNING) +--+-+-+--++-+-+-+-+--+ | Trial name | status | loc | iter | total time (s) | ts | reward | episode_reward_max | episode_reward_min | episode_len_mean | |--+-+-+--++-+-+-+-+--| | PPO_StatelessCartPole_c9203_00000 | RUNNING | 127.0.0.1:8716 | 5 | 355.357 | 20000 | 40 | 135 | 9 | 40 | +--+-+-+--++-+-+-+-+--+ == Status == Current time: 2021-11-29 18:35:56 (running for 00:07:19.17) Memory usage on this node: 9.8/11.9 GiB Using FIFO scheduling algorithm. Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.7 GiB heap, 0.0/0.85 GiB objects Result logdir: C: Users Stefan ray_results PPO Number of trials: 1/1 (1 RUNNING) +--+-+-+--++-+-+-+-+--+ | Trial name | status | loc | iter | total time (s) | ts | reward | episode_reward_max | episode_reward_min | episode_len_mean | |--+-+-+--++-+-+-+-+--| | PPO_StatelessCartPole_c9203_00000 | RUNNING | 127.0.0.1:8716 | 5 | 355.357 | 20000 | 40 | 135 | 9 | 40 | +--+-+-+--++-+-+-+-+--+ == Status == Current time: 2021-11-29 18:36:01 (running for 00:07:24.43) Memory usage on this node: 9.8/11.9 GiB Using FIFO scheduling algorithm. Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.7 GiB heap, 0.0/0.85 GiB objects Result logdir: C: Users Stefan ray_results PPO Number of trials: 1/1 (1 RUNNING) +--+-+-+--++-+-+-+-+--+ | Trial name | status | loc | iter | total time (s) | ts | reward | episode_reward_max | episode_reward_min | episode_len_mean | |--+-+-+--++-+-+-+-+--| | PPO_StatelessCartPole_c9203_00000 | RUNNING | 127.0.0.1:8716 | 5 | 355.357 | 20000 | 40 | 135 | 9 | 40 | +--+-+-+--++-+-+-+-+--+ == Status == Current time: 2021-11-29 18:36:06 (running for 00:07:29.77) Memory usage on this node: 9.8/11.9 GiB Using FIFO scheduling algorithm. Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.7 GiB heap, 0.0/0.85 GiB objects Result logdir: C: Users Stefan ray_results PPO Number of trials: 1/1 (1 RUNNING) +--+-+-+--++-+-+-+-+--+ | Trial name | status | loc | iter | total time (s) | ts | reward | episode_reward_max | episode_reward_min | episode_len_mean | |--+-+-+--++-+-+-+-+--| | PPO_StatelessCartPole_c9203_00000 | RUNNING | 127.0.0.1:8716 | 5 | 355.357 | 20000 | 40 | 135 | 9 | 40 | +--+-+-+--++-+-+-+-+--+ == Status == Current time: 2021-11-29 18:36:13 (running for 00:07:35.90) Memory usage on this node: 9.7/11.9 GiB Using FIFO scheduling algorithm. Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.7 GiB heap, 0.0/0.85 GiB objects Result logdir: C: Users Stefan ray_results PPO Number of trials: 1/1 (1 RUNNING) +--+-+-+--++-+-+-+-+--+ | Trial name | status | loc | iter | total time (s) | ts | reward | episode_reward_max | episode_reward_min | episode_len_mean | |--+-+-+--++-+-+-+-+--| | PPO_StatelessCartPole_c9203_00000 | RUNNING | 127.0.0.1:8716 | 5 | 355.357 | 20000 | 40 | 135 | 9 | 40 | +--+-+-+--++-+-+-+-+--+ == Status == Current time: 2021-11-29 18:36:18 (running for 00:07:41.26) Memory usage on this node: 9.7/11.9 GiB Using FIFO scheduling algorithm. Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.7 GiB heap, 0.0/0.85 GiB objects Result logdir: C: Users Stefan ray_results PPO Number of trials: 1/1 (1 RUNNING) +--+-+-+--++-+-+-+-+--+ | Trial name | status | loc | iter | total time (s) | ts | reward | episode_reward_max | episode_reward_min | episode_len_mean | |--+-+-+--++-+-+-+-+--| | PPO_StatelessCartPole_c9203_00000 | RUNNING | 127.0.0.1:8716 | 5 | 355.357 | 20000 | 40 | 135 | 9 | 40 | +--+-+-+--++-+-+-+-+--+ == Status == Current time: 2021-11-29 18:36:23 (running for 00:07:46.48) Memory usage on this node: 9.9/11.9 GiB Using FIFO scheduling algorithm. Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.7 GiB heap, 0.0/0.85 GiB objects Result logdir: C: Users Stefan ray_results PPO Number of trials: 1/1 (1 RUNNING) +--+-+-+--++-+-+-+-+--+ | Trial name | status | loc | iter | total time (s) | ts | reward | episode_reward_max | episode_reward_min | episode_len_mean | |--+-+-+--++-+-+-+-+--| | PPO_StatelessCartPole_c9203_00000 | RUNNING | 127.0.0.1:8716 | 5 | 355.357 | 20000 | 40 | 135 | 9 | 40 | +--+-+-+--++-+-+-+-+--+ Result for PPO_StatelessCartPole_c9203_00000: agent_timesteps_total: 24000 custom_metrics: {} date: 2021-11-29_18-36-27 done: false episode_len_mean: 37.25688073394495 episode_media: {} episode_reward_max: 126.0 episode_reward_mean: 37.25688073394495 episode_reward_min: 9.0 episodes_this_iter: 109 episodes_total: 772 experiment_id: d11020ea6a8d4dee8d7930b4a3aca15c hostname: nb-stschn info: learner: default_policy: custom_metrics: {} learner_stats: cur_kl_coeff: 0.10000000149011612 cur_lr: 4.999999873689376e-05 entropy: 0.5847723484039307 entropy_coeff: 0.0 kl: 0.01790153980255127 model: {} policy_loss: -0.009266258217394352 total_loss: 153.43453979492188 vf_explained_var: 0.272097110748291 vf_loss: 153.44200134277344 num_agent_steps_sampled: 24000 num_agent_steps_trained: 24000 num_steps_sampled: 24000 num_steps_trained: 24000 num_steps_trained_this_iter: 0 iterations_since_restore: 6 node_ip: 127.0.0.1 num_healthy_workers: 2 off_policy_estimator: {} perf: cpu_util_percent: 99.12941176470588 ram_util_percent: 82.09509803921567 pid: 8716 policy_reward_max: {} policy_reward_mean: {} policy_reward_min: {} sampler_perf: mean_action_processing_ms: 0.15671405125201096 mean_env_render_ms: 0.0 mean_env_wait_ms: 0.15450842609668944 mean_inference_ms: 3.5497709189712223 mean_raw_obs_processing_ms: 0.24926500915359642 time_since_restore: 432.6279237270355 time_this_iter_s: 77.27063298225403 time_total_s: 432.6279237270355 timers: learn_throughput: 62.95 learn_time_ms: 63542.533 load_throughput: 2658759.568 load_time_ms: 1.504 sample_throughput: 65.771 sample_time_ms: 60817.341 update_time_ms: 7.645 timestamp: 1638207387 timesteps_since_restore: 0 timesteps_this_iter: 0 timesteps_total: 24000 training_iteration: 6 trial_id: c9203_00000 == Status == Current time: 2021-11-29 18:36:28 (running for 00:07:51.86) Memory usage on this node: 9.9/11.9 GiB Using FIFO scheduling algorithm. Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.7 GiB heap, 0.0/0.85 GiB objects Result logdir: C: Users Stefan ray_results PPO Number of trials: 1/1 (1 RUNNING) +--+-+-+--++-+-+-+-+--+ | Trial name | status | loc | iter | total time (s) | ts | reward | episode_reward_max | episode_reward_min | episode_len_mean | |--+-+-+--++-+-+-+-+--| | PPO_StatelessCartPole_c9203_00000 | RUNNING | 127.0.0.1:8716 | 6 | 432.628 | 24000 | 37.2569 | 126 | 9 | 37.2569 | +--+-+-+--++-+-+-+-+--+ == Status == Current time: 2021-11-29 18:36:35 (running for 00:07:57.99) Memory usage on this node: 10.0/11.9 GiB Using FIFO scheduling algorithm. Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.7 GiB heap, 0.0/0.85 GiB objects Result logdir: C: Users Stefan ray_results PPO Number of trials: 1/1 (1 RUNNING) +--+-+-+--++-+-+-+-+--+ | Trial name | status | loc | iter | total time (s) | ts | reward | episode_reward_max | episode_reward_min | episode_len_mean | |--+-+-+--++-+-+-+-+--| | PPO_StatelessCartPole_c9203_00000 | RUNNING | 127.0.0.1:8716 | 6 | 432.628 | 24000 | 37.2569 | 126 | 9 | 37.2569 | +--+-+-+--++-+-+-+-+--+ == Status == Current time: 2021-11-29 18:36:40 (running for 00:08:03.30) Memory usage on this node: 10.1/11.9 GiB Using FIFO scheduling algorithm. Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.7 GiB heap, 0.0/0.85 GiB objects Result logdir: C: Users Stefan ray_results PPO Number of trials: 1/1 (1 RUNNING) +--+-+-+--++-+-+-+-+--+ | Trial name | status | loc | iter | total time (s) | ts | reward | episode_reward_max | episode_reward_min | episode_len_mean | |--+-+-+--++-+-+-+-+--| | PPO_StatelessCartPole_c9203_00000 | RUNNING | 127.0.0.1:8716 | 6 | 432.628 | 24000 | 37.2569 | 126 | 9 | 37.2569 | +--+-+-+--++-+-+-+-+--+ == Status == Current time: 2021-11-29 18:36:45 (running for 00:08:08.46) Memory usage on this node: 10.1/11.9 GiB Using FIFO scheduling algorithm. Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.7 GiB heap, 0.0/0.85 GiB objects Result logdir: C: Users Stefan ray_results PPO Number of trials: 1/1 (1 RUNNING) +--+-+-+--++-+-+-+-+--+ | Trial name | status | loc | iter | total time (s) | ts | reward | episode_reward_max | episode_reward_min | episode_len_mean | |--+-+-+--++-+-+-+-+--| | PPO_StatelessCartPole_c9203_00000 | RUNNING | 127.0.0.1:8716 | 6 | 432.628 | 24000 | 37.2569 | 126 | 9 | 37.2569 | +--+-+-+--++-+-+-+-+--+ == Status == Current time: 2021-11-29 18:36:50 (running for 00:08:13.69) Memory usage on this node: 10.1/11.9 GiB Using FIFO scheduling algorithm. Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.7 GiB heap, 0.0/0.85 GiB objects Result logdir: C: Users Stefan ray_results PPO Number of trials: 1/1 (1 RUNNING) +--+-+-+--++-+-+-+-+--+ | Trial name | status | loc | iter | total time (s) | ts | reward | episode_reward_max | episode_reward_min | episode_len_mean | |--+-+-+--++-+-+-+-+--| | PPO_StatelessCartPole_c9203_00000 | RUNNING | 127.0.0.1:8716 | 6 | 432.628 | 24000 | 37.2569 | 126 | 9 | 37.2569 | +--+-+-+--++-+-+-+-+--+ == Status == Current time: 2021-11-29 18:36:55 (running for 00:08:18.77) Memory usage on this node: 10.0/11.9 GiB Using FIFO scheduling algorithm. Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.7 GiB heap, 0.0/0.85 GiB objects Result logdir: C: Users Stefan ray_results PPO Number of trials: 1/1 (1 RUNNING) +--+-+-+--++-+-+-+-+--+ | Trial name | status | loc | iter | total time (s) | ts | reward | episode_reward_max | episode_reward_min | episode_len_mean | |--+-+-+--++-+-+-+-+--| | PPO_StatelessCartPole_c9203_00000 | RUNNING | 127.0.0.1:8716 | 6 | 432.628 | 24000 | 37.2569 | 126 | 9 | 37.2569 | +--+-+-+--++-+-+-+-+--+ == Status == Current time: 2021-11-29 18:37:01 (running for 00:08:24.22) Memory usage on this node: 10.0/11.9 GiB Using FIFO scheduling algorithm. Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.7 GiB heap, 0.0/0.85 GiB objects Result logdir: C: Users Stefan ray_results PPO Number of trials: 1/1 (1 RUNNING) +--+-+-+--++-+-+-+-+--+ | Trial name | status | loc | iter | total time (s) | ts | reward | episode_reward_max | episode_reward_min | episode_len_mean | |--+-+-+--++-+-+-+-+--| | PPO_StatelessCartPole_c9203_00000 | RUNNING | 127.0.0.1:8716 | 6 | 432.628 | 24000 | 37.2569 | 126 | 9 | 37.2569 | +--+-+-+--++-+-+-+-+--+ == Status == Current time: 2021-11-29 18:37:06 (running for 00:08:29.43) Memory usage on this node: 10.0/11.9 GiB Using FIFO scheduling algorithm. Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.7 GiB heap, 0.0/0.85 GiB objects Result logdir: C: Users Stefan ray_results PPO Number of trials: 1/1 (1 RUNNING) +--+-+-+--++-+-+-+-+--+ | Trial name | status | loc | iter | total time (s) | ts | reward | episode_reward_max | episode_reward_min | episode_len_mean | |--+-+-+--++-+-+-+-+--| | PPO_StatelessCartPole_c9203_00000 | RUNNING | 127.0.0.1:8716 | 6 | 432.628 | 24000 | 37.2569 | 126 | 9 | 37.2569 | +--+-+-+--++-+-+-+-+--+ == Status == Current time: 2021-11-29 18:37:11 (running for 00:08:34.68) Memory usage on this node: 10.0/11.9 GiB Using FIFO scheduling algorithm. Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.7 GiB heap, 0.0/0.85 GiB objects Result logdir: C: Users Stefan ray_results PPO Number of trials: 1/1 (1 RUNNING) +--+-+-+--++-+-+-+-+--+ | Trial name | status | loc | iter | total time (s) | ts | reward | episode_reward_max | episode_reward_min | episode_len_mean | |--+-+-+--++-+-+-+-+--| | PPO_StatelessCartPole_c9203_00000 | RUNNING | 127.0.0.1:8716 | 6 | 432.628 | 24000 | 37.2569 | 126 | 9 | 37.2569 | +--+-+-+--++-+-+-+-+--+ == Status == Current time: 2021-11-29 18:37:16 (running for 00:08:39.88) Memory usage on this node: 10.1/11.9 GiB Using FIFO scheduling algorithm. Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.7 GiB heap, 0.0/0.85 GiB objects Result logdir: C: Users Stefan ray_results PPO Number of trials: 1/1 (1 RUNNING) +--+-+-+--++-+-+-+-+--+ | Trial name | status | loc | iter | total time (s) | ts | reward | episode_reward_max | episode_reward_min | episode_len_mean | |--+-+-+--++-+-+-+-+--| | PPO_StatelessCartPole_c9203_00000 | RUNNING | 127.0.0.1:8716 | 6 | 432.628 | 24000 | 37.2569 | 126 | 9 | 37.2569 | +--+-+-+--++-+-+-+-+--+ == Status == Current time: 2021-11-29 18:37:22 (running for 00:08:45.24) Memory usage on this node: 10.1/11.9 GiB Using FIFO scheduling algorithm. Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.7 GiB heap, 0.0/0.85 GiB objects Result logdir: C: Users Stefan ray_results PPO Number of trials: 1/1 (1 RUNNING) +--+-+-+--++-+-+-+-+--+ | Trial name | status | loc | iter | total time (s) | ts | reward | episode_reward_max | episode_reward_min | episode_len_mean | |--+-+-+--++-+-+-+-+--| | PPO_StatelessCartPole_c9203_00000 | RUNNING | 127.0.0.1:8716 | 6 | 432.628 | 24000 | 37.2569 | 126 | 9 | 37.2569 | +--+-+-+--++-+-+-+-+--+ == Status == Current time: 2021-11-29 18:37:27 (running for 00:08:50.33) Memory usage on this node: 10.1/11.9 GiB Using FIFO scheduling algorithm. Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.7 GiB heap, 0.0/0.85 GiB objects Result logdir: C: Users Stefan ray_results PPO Number of trials: 1/1 (1 RUNNING) +--+-+-+--++-+-+-+-+--+ | Trial name | status | loc | iter | total time (s) | ts | reward | episode_reward_max | episode_reward_min | episode_len_mean | |--+-+-+--++-+-+-+-+--| | PPO_StatelessCartPole_c9203_00000 | RUNNING | 127.0.0.1:8716 | 6 | 432.628 | 24000 | 37.2569 | 126 | 9 | 37.2569 | +--+-+-+--++-+-+-+-+--+ == Status == Current time: 2021-11-29 18:37:32 (running for 00:08:55.58) Memory usage on this node: 10.1/11.9 GiB Using FIFO scheduling algorithm. Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.7 GiB heap, 0.0/0.85 GiB objects Result logdir: C: Users Stefan ray_results PPO Number of trials: 1/1 (1 RUNNING) +--+-+-+--++-+-+-+-+--+ | Trial name | status | loc | iter | total time (s) | ts | reward | episode_reward_max | episode_reward_min | episode_len_mean | |--+-+-+--++-+-+-+-+--| | PPO_StatelessCartPole_c9203_00000 | RUNNING | 127.0.0.1:8716 | 6 | 432.628 | 24000 | 37.2569 | 126 | 9 | 37.2569 | +--+-+-+--++-+-+-+-+--+ == Status == Current time: 2021-11-29 18:37:37 (running for 00:09:00.70) Memory usage on this node: 10.1/11.9 GiB Using FIFO scheduling algorithm. Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.7 GiB heap, 0.0/0.85 GiB objects Result logdir: C: Users Stefan ray_results PPO Number of trials: 1/1 (1 RUNNING) +--+-+-+--++-+-+-+-+--+ | Trial name | status | loc | iter | total time (s) | ts | reward | episode_reward_max | episode_reward_min | episode_len_mean | |--+-+-+--++-+-+-+-+--| | PPO_StatelessCartPole_c9203_00000 | RUNNING | 127.0.0.1:8716 | 6 | 432.628 | 24000 | 37.2569 | 126 | 9 | 37.2569 | +--+-+-+--++-+-+-+-+--+ == Status == Current time: 2021-11-29 18:37:43 (running for 00:09:06.19) Memory usage on this node: 10.1/11.9 GiB Using FIFO scheduling algorithm. Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.7 GiB heap, 0.0/0.85 GiB objects Result logdir: C: Users Stefan ray_results PPO Number of trials: 1/1 (1 RUNNING) +--+-+-+--++-+-+-+-+--+ | Trial name | status | loc | iter | total time (s) | ts | reward | episode_reward_max | episode_reward_min | episode_len_mean | |--+-+-+--++-+-+-+-+--| | PPO_StatelessCartPole_c9203_00000 | RUNNING | 127.0.0.1:8716 | 6 | 432.628 | 24000 | 37.2569 | 126 | 9 | 37.2569 | +--+-+-+--++-+-+-+-+--+ Result for PPO_StatelessCartPole_c9203_00000: agent_timesteps_total: 28000 custom_metrics: {} date: 2021-11-29_18-37-46 done: false episode_len_mean: 44.25 episode_media: {} episode_reward_max: 144.0 episode_reward_mean: 44.25 episode_reward_min: 11.0 episodes_this_iter: 91 episodes_total: 863 experiment_id: d11020ea6a8d4dee8d7930b4a3aca15c hostname: nb-stschn info: learner: default_policy: custom_metrics: {} learner_stats: cur_kl_coeff: 0.10000000149011612 cur_lr: 4.999999873689376e-05 entropy: 0.5985927581787109 entropy_coeff: 0.0 kl: 0.010536722838878632 model: {} policy_loss: -0.001510055735707283 total_loss: 205.55392456054688 vf_explained_var: 0.22275586426258087 vf_loss: 205.5543670654297 num_agent_steps_sampled: 28000 num_agent_steps_trained: 28000 num_steps_sampled: 28000 num_steps_trained: 28000 num_steps_trained_this_iter: 0 iterations_since_restore: 7 node_ip: 127.0.0.1 num_healthy_workers: 2 off_policy_estimator: {} perf: cpu_util_percent: 99.61747572815533 ram_util_percent: 84.57572815533977 pid: 8716 policy_reward_max: {} policy_reward_mean: {} policy_reward_min: {} sampler_perf: mean_action_processing_ms: 0.16072242692933073 mean_env_render_ms: 0.0 mean_env_wait_ms: 0.15409101867972258 mean_inference_ms: 3.6422761196753273 mean_raw_obs_processing_ms: 0.2559284180972224 time_since_restore: 510.75397849082947 time_this_iter_s: 78.12605476379395 time_total_s: 510.75397849082947 timers: learn_throughput: 62.325 learn_time_ms: 64179.487 load_throughput: 3101886.163 load_time_ms: 1.29 sample_throughput: 63.181 sample_time_ms: 63310.218 update_time_ms: 7.838 timestamp: 1638207466 timesteps_since_restore: 0 timesteps_this_iter: 0 timesteps_total: 28000 training_iteration: 7 trial_id: c9203_00000 == Status == Current time: 2021-11-29 18:37:49 (running for 00:09:12.14) Memory usage on this node: 10.1/11.9 GiB Using FIFO scheduling algorithm. Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.7 GiB heap, 0.0/0.85 GiB objects Result logdir: C: Users Stefan ray_results PPO Number of trials: 1/1 (1 RUNNING) +--+-+-+--++-+-+-+-+--+ | Trial name | status | loc | iter | total time (s) | ts | reward | episode_reward_max | episode_reward_min | episode_len_mean | |--+-+-+--++-+-+-+-+--| | PPO_StatelessCartPole_c9203_00000 | RUNNING | 127.0.0.1:8716 | 7 | 510.754 | 28000 | 44.25 | 144 | 11 | 44.25 | +--+-+-+--++-+-+-+-+--+ == Status == Current time: 2021-11-29 18:37:54 (running for 00:09:17.28) Memory usage on this node: 10.1/11.9 GiB Using FIFO scheduling algorithm. Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.7 GiB heap, 0.0/0.85 GiB objects Result logdir: C: Users Stefan ray_results PPO Number of trials: 1/1 (1 RUNNING) +--+-+-+--++-+-+-+-+--+ | Trial name | status | loc | iter | total time (s) | ts | reward | episode_reward_max | episode_reward_min | episode_len_mean | |--+-+-+--++-+-+-+-+--| | PPO_StatelessCartPole_c9203_00000 | RUNNING | 127.0.0.1:8716 | 7 | 510.754 | 28000 | 44.25 | 144 | 11 | 44.25 | +--+-+-+--++-+-+-+-+--+ == Status == Current time: 2021-11-29 18:37:59 (running for 00:09:22.42) Memory usage on this node: 10.1/11.9 GiB Using FIFO scheduling algorithm. Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.7 GiB heap, 0.0/0.85 GiB objects Result logdir: C: Users Stefan ray_results PPO Number of trials: 1/1 (1 RUNNING) +--+-+-+--++-+-+-+-+--+ | Trial name | status | loc | iter | total time (s) | ts | reward | episode_reward_max | episode_reward_min | episode_len_mean | |--+-+-+--++-+-+-+-+--| | PPO_StatelessCartPole_c9203_00000 | RUNNING | 127.0.0.1:8716 | 7 | 510.754 | 28000 | 44.25 | 144 | 11 | 44.25 | +--+-+-+--++-+-+-+-+--+ == Status == Current time: 2021-11-29 18:38:04 (running for 00:09:27.67) Memory usage on this node: 10.3/11.9 GiB Using FIFO scheduling algorithm. Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.7 GiB heap, 0.0/0.85 GiB objects Result logdir: C: Users Stefan ray_results PPO Number of trials: 1/1 (1 RUNNING) +--+-+-+--++-+-+-+-+--+ | Trial name | status | loc | iter | total time (s) | ts | reward | episode_reward_max | episode_reward_min | episode_len_mean | |--+-+-+--++-+-+-+-+--| | PPO_StatelessCartPole_c9203_00000 | RUNNING | 127.0.0.1:8716 | 7 | 510.754 | 28000 | 44.25 | 144 | 11 | 44.25 | +--+-+-+--++-+-+-+-+--+ == Status == Current time: 2021-11-29 18:38:09 (running for 00:09:32.87) Memory usage on this node: 10.3/11.9 GiB Using FIFO scheduling algorithm. Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.7 GiB heap, 0.0/0.85 GiB objects Result logdir: C: Users Stefan ray_results PPO Number of trials: 1/1 (1 RUNNING) +--+-+-+--++-+-+-+-+--+ | Trial name | status | loc | iter | total time (s) | ts | reward | episode_reward_max | episode_reward_min | episode_len_mean | |--+-+-+--++-+-+-+-+--| | PPO_StatelessCartPole_c9203_00000 | RUNNING | 127.0.0.1:8716 | 7 | 510.754 | 28000 | 44.25 | 144 | 11 | 44.25 | +--+-+-+--++-+-+-+-+--+ == Status == Current time: 2021-11-29 18:38:15 (running for 00:09:38.21) Memory usage on this node: 10.3/11.9 GiB Using FIFO scheduling algorithm. Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.7 GiB heap, 0.0/0.85 GiB objects Result logdir: C: Users Stefan ray_results PPO Number of trials: 1/1 (1 RUNNING) +--+-+-+--++-+-+-+-+--+ | Trial name | status | loc | iter | total time (s) | ts | reward | episode_reward_max | episode_reward_min | episode_len_mean | |--+-+-+--++-+-+-+-+--| | PPO_StatelessCartPole_c9203_00000 | RUNNING | 127.0.0.1:8716 | 7 | 510.754 | 28000 | 44.25 | 144 | 11 | 44.25 | +--+-+-+--++-+-+-+-+--+ == Status == Current time: 2021-11-29 18:38:20 (running for 00:09:43.35) Memory usage on this node: 10.4/11.9 GiB Using FIFO scheduling algorithm. Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.7 GiB heap, 0.0/0.85 GiB objects Result logdir: C: Users Stefan ray_results PPO Number of trials: 1/1 (1 RUNNING) +--+-+-+--++-+-+-+-+--+ | Trial name | status | loc | iter | total time (s) | ts | reward | episode_reward_max | episode_reward_min | episode_len_mean | |--+-+-+--++-+-+-+-+--| | PPO_StatelessCartPole_c9203_00000 | RUNNING | 127.0.0.1:8716 | 7 | 510.754 | 28000 | 44.25 | 144 | 11 | 44.25 | +--+-+-+--++-+-+-+-+--+ == Status == Current time: 2021-11-29 18:38:25 (running for 00:09:48.71) Memory usage on this node: 10.3/11.9 GiB Using FIFO scheduling algorithm. Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.7 GiB heap, 0.0/0.85 GiB objects Result logdir: C: Users Stefan ray_results PPO Number of trials: 1/1 (1 RUNNING) +--+-+-+--++-+-+-+-+--+ | Trial name | status | loc | iter | total time (s) | ts | reward | episode_reward_max | episode_reward_min | episode_len_mean | |--+-+-+--++-+-+-+-+--| | PPO_StatelessCartPole_c9203_00000 | RUNNING | 127.0.0.1:8716 | 7 | 510.754 | 28000 | 44.25 | 144 | 11 | 44.25 | +--+-+-+--++-+-+-+-+--+ == Status == Current time: 2021-11-29 18:38:31 (running for 00:09:53.95) Memory usage on this node: 10.3/11.9 GiB Using FIFO scheduling algorithm. Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.7 GiB heap, 0.0/0.85 GiB objects Result logdir: C: Users Stefan ray_results PPO Number of trials: 1/1 (1 RUNNING) +--+-+-+--++-+-+-+-+--+ | Trial name | status | loc | iter | total time (s) | ts | reward | episode_reward_max | episode_reward_min | episode_len_mean | |--+-+-+--++-+-+-+-+--| | PPO_StatelessCartPole_c9203_00000 | RUNNING | 127.0.0.1:8716 | 7 | 510.754 | 28000 | 44.25 | 144 | 11 | 44.25 | +--+-+-+--++-+-+-+-+--+ == Status == Current time: 2021-11-29 18:38:36 (running for 00:09:59.60) Memory usage on this node: 10.3/11.9 GiB Using FIFO scheduling algorithm. Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.7 GiB heap, 0.0/0.85 GiB objects Result logdir: C: Users Stefan ray_results PPO Number of trials: 1/1 (1 RUNNING) +--+-+-+--++-+-+-+-+--+ | Trial name | status | loc | iter | total time (s) | ts | reward | episode_reward_max | episode_reward_min | episode_len_mean | |--+-+-+--++-+-+-+-+--| | PPO_StatelessCartPole_c9203_00000 | RUNNING | 127.0.0.1:8716 | 7 | 510.754 | 28000 | 44.25 | 144 | 11 | 44.25 | +--+-+-+--++-+-+-+-+--+ == Status == Current time: 2021-11-29 18:38:41 (running for 00:10:04.69) Memory usage on this node: 10.3/11.9 GiB Using FIFO scheduling algorithm. Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.7 GiB heap, 0.0/0.85 GiB objects Result logdir: C: Users Stefan ray_results PPO Number of trials: 1/1 (1 RUNNING) +--+-+-+--++-+-+-+-+--+ | Trial name | status | loc | iter | total time (s) | ts | reward | episode_reward_max | episode_reward_min | episode_len_mean | |--+-+-+--++-+-+-+-+--| | PPO_StatelessCartPole_c9203_00000 | RUNNING | 127.0.0.1:8716 | 7 | 510.754 | 28000 | 44.25 | 144 | 11 | 44.25 | +--+-+-+--++-+-+-+-+--+ == Status == Current time: 2021-11-29 18:38:47 (running for 00:10:10.01) Memory usage on this node: 10.3/11.9 GiB Using FIFO scheduling algorithm. Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.7 GiB heap, 0.0/0.85 GiB objects Result logdir: C: Users Stefan ray_results PPO Number of trials: 1/1 (1 RUNNING) +--+-+-+--++-+-+-+-+--+ | Trial name | status | loc | iter | total time (s) | ts | reward | episode_reward_max | episode_reward_min | episode_len_mean | |--+-+-+--++-+-+-+-+--| | PPO_StatelessCartPole_c9203_00000 | RUNNING | 127.0.0.1:8716 | 7 | 510.754 | 28000 | 44.25 | 144 | 11 | 44.25 | +--+-+-+--++-+-+-+-+--+ == Status == Current time: 2021-11-29 18:38:52 (running for 00:10:15.16) Memory usage on this node: 10.3/11.9 GiB Using FIFO scheduling algorithm. Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.7 GiB heap, 0.0/0.85 GiB objects Result logdir: C: Users Stefan ray_results PPO Number of trials: 1/1 (1 RUNNING) +--+-+-+--++-+-+-+-+--+ | Trial name | status | loc | iter | total time (s) | ts | reward | episode_reward_max | episode_reward_min | episode_len_mean | |--+-+-+--++-+-+-+-+--| | PPO_StatelessCartPole_c9203_00000 | RUNNING | 127.0.0.1:8716 | 7 | 510.754 | 28000 | 44.25 | 144 | 11 | 44.25 | +--+-+-+--++-+-+-+-+--+ == Status == Current time: 2021-11-29 18:38:57 (running for 00:10:20.50) Memory usage on this node: 10.3/11.9 GiB Using FIFO scheduling algorithm. Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.7 GiB heap, 0.0/0.85 GiB objects Result logdir: C: Users Stefan ray_results PPO Number of trials: 1/1 (1 RUNNING) +--+-+-+--++-+-+-+-+--+ | Trial name | status | loc | iter | total time (s) | ts | reward | episode_reward_max | episode_reward_min | episode_len_mean | |--+-+-+--++-+-+-+-+--| | PPO_StatelessCartPole_c9203_00000 | RUNNING | 127.0.0.1:8716 | 7 | 510.754 | 28000 | 44.25 | 144 | 11 | 44.25 | +--+-+-+--++-+-+-+-+--+ == Status == Current time: 2021-11-29 18:39:03 (running for 00:10:26.00) Memory usage on this node: 10.4/11.9 GiB Using FIFO scheduling algorithm. Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.7 GiB heap, 0.0/0.85 GiB objects Result logdir: C: Users Stefan ray_results PPO Number of trials: 1/1 (1 RUNNING) +--+-+-+--++-+-+-+-+--+ | Trial name | status | loc | iter | total time (s) | ts | reward | episode_reward_max | episode_reward_min | episode_len_mean | |--+-+-+--++-+-+-+-+--| | PPO_StatelessCartPole_c9203_00000 | RUNNING | 127.0.0.1:8716 | 7 | 510.754 | 28000 | 44.25 | 144 | 11 | 44.25 | +--+-+-+--++-+-+-+-+--+ Result for PPO_StatelessCartPole_c9203_00000: agent_timesteps_total: 32000 custom_metrics: {} date: 2021-11-29_18-39-05 done: false episode_len_mean: 40.07 episode_media: {} episode_reward_max: 171.0 episode_reward_mean: 40.07 episode_reward_min: 10.0 episodes_this_iter: 97 episodes_total: 960 experiment_id: d11020ea6a8d4dee8d7930b4a3aca15c hostname: nb-stschn info: learner: default_policy: custom_metrics: {} learner_stats: cur_kl_coeff: 0.10000000149011612 cur_lr: 4.999999873689376e-05 entropy: 0.5840765237808228 entropy_coeff: 0.0 kl: 0.01586488075554371 model: {} policy_loss: -0.0019629900343716145 total_loss: 223.22601318359375 vf_explained_var: 0.22599440813064575 vf_loss: 223.2263946533203 num_agent_steps_sampled: 32000 num_agent_steps_trained: 32000 num_steps_sampled: 32000 num_steps_trained: 32000 num_steps_trained_this_iter: 0 iterations_since_restore: 8 node_ip: 127.0.0.1 num_healthy_workers: 2 off_policy_estimator: {} perf: cpu_util_percent: 99.88846153846154 ram_util_percent: 86.47211538461536 pid: 8716 policy_reward_max: {} policy_reward_mean: {} policy_reward_min: {} sampler_perf: mean_action_processing_ms: 0.15995308287025492 mean_env_render_ms: 0.0 mean_env_wait_ms: 0.1561062337534058 mean_inference_ms: 3.6696105211636922 mean_raw_obs_processing_ms: 0.26280132905810133 time_since_restore: 589.6512229442596 time_this_iter_s: 78.89724445343018 time_total_s: 589.6512229442596 timers: learn_throughput: 61.65 learn_time_ms: 64882.695 load_throughput: 2659936.344 load_time_ms: 1.504 sample_throughput: 61.494 sample_time_ms: 65046.761 update_time_ms: 7.432 timestamp: 1638207545 timesteps_since_restore: 0 timesteps_this_iter: 0 timesteps_total: 32000 training_iteration: 8 trial_id: c9203_00000 == Status == Current time: 2021-11-29 18:39:08 (running for 00:10:31.19) Memory usage on this node: 10.3/11.9 GiB Using FIFO scheduling algorithm. Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.7 GiB heap, 0.0/0.85 GiB objects Result logdir: C: Users Stefan ray_results PPO Number of trials: 1/1 (1 RUNNING) +--+-+-+--++-+-+-+-+--+ | Trial name | status | loc | iter | total time (s) | ts | reward | episode_reward_max | episode_reward_min | episode_len_mean | |--+-+-+--++-+-+-+-+--| | PPO_StatelessCartPole_c9203_00000 | RUNNING | 127.0.0.1:8716 | 8 | 589.651 | 32000 | 40.07 | 171 | 10 | 40.07 | +--+-+-+--++-+-+-+-+--+ == Status == Current time: 2021-11-29 18:39:13 (running for 00:10:36.35) Memory usage on this node: 10.3/11.9 GiB Using FIFO scheduling algorithm. Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.7 GiB heap, 0.0/0.85 GiB objects Result logdir: C: Users Stefan ray_results PPO Number of trials: 1/1 (1 RUNNING) +--+-+-+--++-+-+-+-+--+ | Trial name | status | loc | iter | total time (s) | ts | reward | episode_reward_max | episode_reward_min | episode_len_mean | |--+-+-+--++-+-+-+-+--| | PPO_StatelessCartPole_c9203_00000 | RUNNING | 127.0.0.1:8716 | 8 | 589.651 | 32000 | 40.07 | 171 | 10 | 40.07 | +--+-+-+--++-+-+-+-+--+ == Status == Current time: 2021-11-29 18:39:18 (running for 00:10:41.72) Memory usage on this node: 10.3/11.9 GiB Using FIFO scheduling algorithm. Resources requested: 3.0/3 CPUs, 0/0 GPUs, 0.0/1.7 GiB heap, 0.0/0.85 GiB objects Result logdir: C: Users Stefan ray_results PPO Number of trials: 1/1 (1 RUNNING) +--+-+-+--++-+-+-+-+--+ | Trial name | status | loc | iter | total time (s) | ts | reward | episode_reward_max | episode_reward_min | episode_len_mean | |--+-+-+--++-+-+-+-+--| | PPO_StatelessCartPole_c9203_00000 | RUNNING | 127.0.0.1:8716 | 8 | 589.651 | 32000 | 40.07 | 171 | 10 | 40.07 | +--+-+-+--++-+-+-+-+--+ . . results3b.default_metric = &quot;episode_reward_mean&quot; results3b.default_mode = &quot;max&quot; # print the mean episode reward = episode length --&gt; higher = better results3b.best_result[&quot;episode_reward_mean&quot;] . plot_rewards(results3b) . LSTM with Stacked Observations . Using the StackedStatelessCartPole from above. . config3b2 = ppo.DEFAULT_CONFIG.copy() config3b2[&quot;env&quot;] = &quot;StackedStatelessCartPole&quot; config3b2[&quot;model&quot;] = { &quot;use_lstm&quot;: True, } results3b2 = ray.tune.run(&quot;PPO&quot;, config=config3b2, stop=stop) print(&quot;Option 3b2: Training finished successfully&quot;) . results3b2.default_metric = &quot;episode_reward_mean&quot; results3b2.default_mode = &quot;max&quot; # print the mean episode reward = episode length --&gt; higher = better results3b2.best_result[&quot;episode_reward_mean&quot;] . plot_rewards(results3b2) . Option 3c: Use Attention for Processing the Sequence . Self-attention is a recent and popular alternative to RNNs for processing sequence data. Currently, the transformer architecture using self-attention is state of the art for natural language processing (NLP) tasks. . A similar, yet slightly modified architecture using attention is also useful for RL (see related paper). Again, enabling attention in RLlib simply requires setting the corresponding flag in the model config: . config3c = ppo.DEFAULT_CONFIG.copy() config3c[&quot;env&quot;] = &quot;StatelessCartPole&quot; config3c[&quot;model&quot;] = { # Attention net wrapping (for tf) can already use the native keras # model versions. For torch, this will have no effect. &quot;_use_default_native_models&quot;: True, &quot;use_attention&quot;: True, # &quot;max_seq_len&quot;: 10, # &quot;attention_num_transformer_units&quot;: 1, # &quot;attention_dim&quot;: 32, # &quot;attention_memory_inference&quot;: 10, # &quot;attention_memory_training&quot;: 10, # &quot;attention_num_heads&quot;: 1, # &quot;attention_head_dim&quot;: 32, # &quot;attention_position_wise_mlp_dim&quot;: 32, } results3c = ray.tune.run(&quot;PPO&quot;, config=config3c, stop=stop) print(&quot;Option 3c: Training finished successfully&quot;) . results3c.default_metric = &quot;episode_reward_mean&quot; results3c.default_mode = &quot;max&quot; # print the mean episode reward = episode length --&gt; higher = better results3c.best_result[&quot;episode_reward_mean&quot;] . plot_rewards(results3c) . Attention with Stacked Observations . config3c2 = ppo.DEFAULT_CONFIG.copy() config3c2[&quot;env&quot;] = &quot;StackedStatelessCartPole&quot; config3c2[&quot;model&quot;] = { &quot;_use_default_native_models&quot;: True, &quot;use_attention&quot;: True, } results3c2 = ray.tune.run(&quot;PPO&quot;, config=config3c2, stop=stop) print(&quot;Option 3b2: Training finished successfully&quot;) . results3c2.default_metric = &quot;episode_reward_mean&quot; results3c2.default_mode = &quot;max&quot; # print the mean episode reward = episode length --&gt; higher = better results3c2.best_result[&quot;episode_reward_mean&quot;] . plot_rewards(results3c2) .",
            "url": "https://stefanbschneider.github.io/blog/rl-self-attention",
            "relUrl": "/rl-self-attention",
            "date": " • Nov 16, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Easily editing videos with Descript",
            "content": "After the Covid outbreak, many conferences and similar events were moved online or into a hybrid format. For me as researcher, this means that I now only rarely present my research outcomes live in person. Instead, I record more and more videos of my conferences presentations to share online as part of online conferences. Some of them are also on YouTube. . I noticed that I tend to use filler words like “uhm”, “äh”, “like”, etc, which are even more annoying in recorded and publishded videos than in live, in-person presentations. Of course, I practice my talks before recording, but it is difficult to get rid of bloopers and filler words completely without editing. . Since I am not used to video editing, manually searching for and removing filler words in long presentations would have meant hours of work. Hence, I searched for some tool to support this editing process and found Descript1, which does exactly what I needed: It can record presentations, converts speech to text, automatically detects and removes filler words, improves audio, and more. To edit and remove parts of the videos, simply delete the corresponding auto-generated text like you would in Word. . Demonstration . Here is a short demonstration of how it works: . Of course, it is not perfect yet and sometimes words are recognized incorrectly. But overall, I was very impressed by how well it usually works, plus there are frequent updates and improvements. It has certainly saved me hours of editing and significantly improved my videos. Since it took me a while to find the tool, I wanted to spread the word in this blog post, hopefully saving others some time. . Product Video . For more available features, check out the product video/advertisement, which I find very entertaining: . Try It Yourself . You can try Descript1 for free for some hours (but with watermark, I think). After that, there is a discounted plan for students/non-profit1, which might be useful, or more powerful plans for creators, professionals, or enterprises1 . . 1. Affiliate link: Descript offers a free trial and discounted plans for non-profit. If you buy any plan via my link, e.g., for more features, I will get a small commission. This does not affect pricing for you.↩ .",
            "url": "https://stefanbschneider.github.io/blog/descript",
            "relUrl": "/descript",
            "date": " • Nov 2, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Cell Selection with Deep Reinforcement Learning",
            "content": ". In the last year, I have worked a lot on mobility management and cell selection in mobile networks, which is very relevant for upcoming 5G networks and beyond. In this blog post, I try to give a brief, high-level overview of my research, describing what cell selection is, why it is important, yet challenging. I also outline how I solved the cell selection problem with modern self-learning techniques and visualize the outcomes. . The whole blog post is meant for people who are interested but do not have a scientific background or extensive knowledge in the area. The corresponding research paper is still work in progress. I will post a preprint later, which contains a lot more details than this blog post. . Either way, if something is unclear, or you have open questions, feel free to ask them in the comment section below. For that, you need a (free) GitHub account. Alternatively, you can also contact me directly via Twitter or email. . What is (multi-)cell selection and why is it important? . We use mobile networks every day, where our phones connect to cell towers nearby to send and receive data. While there are many influencing factors, we generally have a better signal and higher data rates when we are closer to the connected cell tower. The farther we are away, the worse the signal gets. When we are far away from the connected cell tower and in between multiple cells (called “cell edge”), the received signal is typically rather bad, which results in low and unreliable data rates. However, high and reliable data rates are crucial for current and upcoming services like video streaming, augmented or virtual reality, and cloud gaming. . . One option to increase the received data rate, particularly at the cell edge, was introduced in 4G/LTE and is called coordinated multipoint (CoMP). Among other features, CoMP allows users to connect to and receive data from multiple cells simultaneously. Compared to previous mobile networks (1G, 2G, and 3G), which only allowed connecting to a single cell, joint transmission from multiple cells with CoMP leads to significantly higher data rates. This is illustrated in the figure above for user 5. . In 5G and beyond, joint transmission from multiple cells with CoMP will become increasingly important as users want higher and higher data rates, and the trend is towards many, small and partially overlapping cells. Since users move around these cells and cell sizes might be quite small, we need to frequently adjust which users are connected to which cells, which is called “cell selection”. In CoMP with joint transmissions, users can connect to and receive data from multiple cells, thus I refer to it as “multi-cell selection”. . What is the challenge? . Ok, so users need to connect to cells and, with CoMP, they may connect to multiple cells. What’s the problem? Why not just connect each user to the closest cell? Indeed, hat is a common approach for single-cell selection, but it does not leverage the idea of multiple connections with CoMP. For CoMP, why not simply connect each user to all cells in range or to a specific number of cells? Again, that is a fair idea, but it may lead to strong competition between users. Each user connected to a cell requires some resources, e.g., in terms of time and frequency, often called “resource blocks”. Since each cell has a limited amount of these resource blocks, connecting more users to a cell means that fewer resource blocks are available per user, i.e., these users compete for resources. In the figure above, users 1-4 are all connected to cell A and compete for resources. How each cell allocates resources to connected users (and how much) is typically unknown and may be defined in a vendor-specific, proprietary allocation scheme. . Hence, the challenge of multi-cell selection is to dynamically select a set of cells that each user should be connected to. This by connecting to these cells, the user should receive data at a sufficient rate but avoid unnecessary competition with other users. As users move around, the selected set of connected cells should be adjusted dynamically. The goal of dynamic multi-cell selection is to ensure good service quality for all users. . How to solve it? . As discussed above, simple rules, e.g., for connecting to the closest cell or to all cells in range, often do not work very well or just in some scenarios. Instead, I propose three more flexible and powerful approach for multi-cell selection using deep reinforcement learning. The main idea is that these approaches do not follow a hand-written algorithm or fixed rules but learn themselves how to optimize service quality by testing different actions and receiving rewards for good service quality. In doing so, these approaches learn themselves how to optimize multi-cell selection and adapt to any given scenario. I propose three approaches: A centralized approach, called DeepCoMP and two distributed approaches, called DD-CoMP and D3-CoMP. The implemented prototypes (in Python) are publicly available on GitHub. . Centralized cell selection with DeepCoMP . As a centralized approach, DeepCoMP assumes global knowledge and control of all users. This means, DeepCoMP needs to collect information from all users in a central location, e.g., about users’ current connections, signal strength, etc. Similarly, DeepCoMP represents a single, centralized entity that selects cells for all users simultaneously. This is illustrated in the figure below. . . Distributed cell selection with DD-CoMP and D3-CoMP . In addition to the centralized DeepCoMP approach, I also propose two distributed approaches, called DD-CoMP and D3-CoMP. These approaches use multi-agent reinforcement learning, where multiple “agents” select cells independently in parallel. Particularly, I assume one agent per user, which observes the users current connections, signal strength, etc. and selects suitable cells. Because each agent only cares about its own user, it is much simpler to implement and use than the centralized DeepCoMP approach. This is visualized in the figure below. . . In reinforcement learning, such agents are initially trained, where they test and learn from many different actions, and then apply their learned policy (in the “inference” phase). In DD-CoMP training experience from all users is combined to learn cell selection more efficiently. In D3-CoMP each agent trains independently from all other agents and only uses experience from its own user. This allows to reduce communication between agents. . Outcome . I implemented prototypes of DeepCoMP, DD-CoMP, and D3-CoMP, which are available on GitHub. When evaluating these approaches, I found that they indeed learn multi-cell selection well after some training. To illustrate, DeepCoMP without any training only makes random actions, which are not useful and lead to many disconnected users (shown as circles; red means bad quality): . . After some training (here: 2 million train steps), it learns to select cells in a highly optimized fashion, ensuring that all users are connected and have much better quality. It connects users to multiple cells with CoMP where it is useful but not overly aggressive to avoid unnecessary competition: . . Further details and systematic evaluation results are in the paper, which I will post later. . What next? . In future work, my proposed approaches could be further improved to learn more efficiently. I could also extend the problem to not only dynamically select cells for each user but also control things like sending power. . If you are interested, you can find more information here: . Paper Preprint (will be uploaded soon) | Code on GitHub | My talk at Ray Summit 2021 | Other blog posts on understandable research | .",
            "url": "https://stefanbschneider.github.io/blog/deepcomp",
            "relUrl": "/deepcomp",
            "date": " • May 2, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Using PyTorch Inside a Django App",
            "content": "In this blog post, I build a simple image classification app using a pre-trained DenseNet 121 model in PyTorch. I deploy this image classification model inside a Django web app on Heroku. . . This is very much related to the PyTorch guide on deployment with Flask. Here, I show an alternative using Django, which is not as light-weight but contains more features built-in than Flask. For more information on differences between Django and Flask, see this website. . Deployed image classification app on Heroku | GitHub repository with final code | Related blog posts: Getting started with PyTorch | Building and deploying Django apps on Heroku | Adding Google Analytics to a Django App | . | . Note that my deployed app may take several seconds to load because I use Heroku&#39;s fee dynos, which automatically power off when they are unused. . Initial Setup: Install Django and PyTorch . Requirements: Python 3, GitHub and Heroku account. Install Django and PyTorch: . pip install django trochvision . Create a Django project pytorch_django and an app image_classification: . django-admin startproject pytorch_django cd pytorch_django python manage.py startapp image_classification . Inside settings.py, add &#39;image_classification.apps.ImageClassificationConfig&#39; to the INSTALLED_APPS list. . To verify, that there are no errors yet, start the Django dev server: . python manage.py runserver . Go to localhost:8000: . . PyTorch Image Classification . To classify uploaded images, I use a DenseNet neural network that is pretrained on the ImageNet dataset. Since the web app is very simple and does not have any other functionality, I simply implement the image classification inside the Django image_classification/views.py module. . Note: This code is taken from a PyTorch tutorial and is under MIT license. . First, I load the pretrained DenseNet, switch to evaluation/inference mode (since I do not need any further training), and load the mapping for predicted indices to human-readable labels for ImageNet. The JSON-file containing the mapping is available here and should be saved in the Django static directory under as defined in STATICFILES_DIRS (settings.py). . import io import os import json from torchvision import models from torchvision import transforms from PIL import Image from django.conf import settings # load pretrained DenseNet and go straight to evaluation mode for inference # load as global variable here, to avoid expensive reloads with each request model = models.densenet121(pretrained=True) model.eval() # load mapping of ImageNet index to human-readable label (from staticfiles directory) # run &quot;python manage.py collectstatic&quot; to ensure all static files are copied to the STATICFILES_DIRS json_path = os.path.join(settings.STATIC_ROOT, &quot;imagenet_class_index.json&quot;) imagenet_mapping = json.load(open(json_path)) . . Important: It is important to load the pretrained model once as global variable and not inside the view function, which would reload the model on each request (expensive and slow!). . Tip: Loading the static JSON file via settings.STATIC_ROOT should work both in development and production deployment but requires running python manage.py collectstatic first. . Then, I need a function to transform an uploaded image (passed in bytes) into the required format for DenseNet, which is a 224 x 224 image with 3 RGB channels. The following code does this transformation and also normalizes the image, returning the corresponding tensor: . def transform_image(image_bytes): &quot;&quot;&quot; Transform image into required DenseNet format: 224x224 with 3 RGB channels and normalized. Return the corresponding tensor. &quot;&quot;&quot; my_transforms = transforms.Compose([transforms.Resize(255), transforms.CenterCrop(224), transforms.ToTensor(), transforms.Normalize( [0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]) image = Image.open(io.BytesIO(image_bytes)) return my_transforms(image).unsqueeze(0) . Finally, this function can be used inside the prediction function, where the transformed tensor of the uploaded image is passed through the pretrained DenseNet model in a forward pass. Since I only do inference here not training, I do not need a backward pass for backpropagation. The model predicts the index of the corresponding ImageNet class, which is just an integer. To display a more useful label, I retrieve the corresponding human-readable label from the imagenet_mapping dict that I created at the beginning from the downloaded JSON file: . def get_prediction(image_bytes): &quot;&quot;&quot;For given image bytes, predict the label using the pretrained DenseNet&quot;&quot;&quot; tensor = transform_image(image_bytes) outputs = model.forward(tensor) _, y_hat = outputs.max(1) predicted_idx = str(y_hat.item()) class_name, human_label = imagenet_mapping[predicted_idx] return human_label . Django URL setup . Having the PyTorch classification logic implemented in image_classification/views.py, I now need to integrate it into the Django app and really use it in a Django view and template. For that, I first make some adjustments in the URLs by creating a separate image_classification/urls.py for the URLs of the image classification app: . from django.urls import path, include from django.conf import settings from django.conf.urls.static import static from . import views app_name = &#39;image_classification&#39; urlpatterns = [ # two paths: with or without given image path(&#39;&#39;, views.index, name=&#39;index&#39;), ] + static(settings.MEDIA_URL, document_root=settings.MEDIA_ROOT) . When visiting the main page of the web app, the requests are now directed to an index view, which I need to implement next and which will make use of the previous PyTorch classification logic. Before, I still need link these URLs to the project&#39;s URLs in pytorch_django/urls.py such that they become effective: . urlpatterns = [ path(&#39;&#39;, include(&#39;image_classification.urls&#39;)), path(&#39;admin/&#39;, admin.site.urls), ] . Django Image Upload, Classification, and Display . Now, I implement the index view, which accepts an uploaded image, processes it, and passes it to the PyTorch classification logic implemented above. I also need a simple Django template to render the web interface, where users can upload an image and submit it for classification. After classification, the template needs to show the predicted label. . Form . For submitting uploaded images, I use a very simply Django form with an ImageField in image_classification/forms.py: . from django import forms class ImageUploadForm(forms.Form): image = forms.ImageField() . View . I use this form inside my index view to accept uploaded images. (index is how I called it in my image_classification/urls.py but it could be any other name.) Here, I just want to display the uploaded image and pass it to the PyTorch model for classification. I do not want to (even temporarily) store it to the file system/disk. Hence, inside the view (image_classification/views.py), I get the image from the form, get its byte representation (for PyTorch) and create an image URI for displaying the image in the template later (see StackOverflow): . import base64 from django.shortcuts import render from .forms import ImageUploadForm def index(request): image_uri = None predicted_label = None if request.method == &#39;POST&#39;: # in case of POST: get the uploaded image from the form and process it form = ImageUploadForm(request.POST, request.FILES) if form.is_valid(): # retrieve the uploaded image and convert it to bytes (for PyTorch) image = form.cleaned_data[&#39;image&#39;] image_bytes = image.file.read() # convert and pass the image as base64 string to avoid storing it to DB or filesystem encoded_img = base64.b64encode(image_bytes).decode(&#39;ascii&#39;) image_uri = &#39;data:%s;base64,%s&#39; % (&#39;image/jpeg&#39;, encoded_img) # get predicted label with previously implemented PyTorch function try: predicted_label = get_prediction(image_bytes) except RuntimeError as re: print(re) else: # in case of GET: simply show the empty form for uploading images form = ImageUploadForm() # pass the form, image URI, and predicted label to the template to be rendered context = { &#39;form&#39;: form, &#39;image_uri&#39;: image_uri, &#39;predicted_label&#39;: predicted_label, } return render(request, &#39;image_classification/index.html&#39;, context) . Template . The index view above calls Django&#39;s render function on a template image_classification/index.html, which I need to create now (inside the image_classification/templates directory). The template needs to show the form for uploading images and, after submitting and image, the uploaded image and its predicted label. . . &lt;h1&gt;Image Classification App&lt;/h1&gt; &lt;p&gt;A simple Django web app with a pretrained PyTorch DenseNet model will try to classify the selected image according to ImageNet labels. Uploaded images are not saved.&lt;/p&gt; &lt;p&gt;&lt;small&gt;Further information: &lt;a href=&quot;&quot; target=&quot;_blank&quot;&gt;Blog Post&lt;/a&gt;, &lt;a href=&quot;https://github.com/stefanbschneider/pytorch-django&quot; target=&quot;_blank&quot;&gt;GitHub&lt;/a&gt;&lt;/small&gt; &lt;/p&gt; &lt;form method=&quot;post&quot; enctype=&quot;multipart/form-data&quot; style=&quot;margin-top: 50px; margin-bottom: 30px;&quot;&gt; {% csrf_token %} {{ form }} &lt;button type=&quot;submit&quot; id=&quot;btnUpload&quot; class=&quot;btn btn-primary&quot;&gt;Upload&lt;/button&gt; &lt;/form&gt; {% if image_uri is not None %} {% if predicted_label is not None %} &lt;div class=&quot;alert alert-primary&quot; role=&quot;alert&quot;&gt; Predicted label: &lt;b&gt;{{ predicted_label }}&lt;/b&gt; &lt;/div&gt; {% else %} &lt;div class=&quot;alert alert-danger&quot; role=&quot;alert&quot;&gt; Prediction error. No label predicted. &lt;/div&gt; {% endif %} &lt;img src=&quot;{{ image_uri }}&quot; class=&quot;img-fluid&quot; alt=&quot;Uploaded image&quot; style=&quot;max-width: min(500px, 100%); height: auto; margin-top: 30px;&quot;&gt; {% endif %} . . The uploaded image uses the saved and passed image URI from before and does not save or load any image from disk, which is important for privacy. . This template relies on some Bootstrap styling (see my corresponding blog post), but it is of course possible to omit that. . Testing the App Locally . Running the app locally should now work without errors and show a simple page with the image upload form: . . After uploading an image, the app shows the image and its classification below: . . Here, it correctly classifies the image as a (tiger) cat. . Deployment on Heroku . For (production) deployment of this simple web app on Heroku, a few extra steps are necessary. Also refer to my dedicated blog post on this topic for details. . File Structure . For some reason, the default directory structure always breaks my Heroku deployment. It works, when removing the parent pytorch_django directory like this: . # original structure when generating the project and app pytorch_django image_classification ... pytorch_django ... manage.py README.md # after removing the parent directory image_classification ... pytorch_django ... manage.py README.md . Setup and Production Settings . After creating the app on Heroku and enabling automatic deploys from the corresponding GitHub repo, set the following config variables (in Heroku: Settings &gt; Config Vars): . DJANGO_SETTINGS_MODULE: pytorch_django.prod_settings DJANGO_SECRET_KEY: &lt;randomly-generated-secret-key&gt; . This indicates that Heroku should use a separate prod_settings.py rather than the settings.py used for development. This prod_settings.py simply overwrites and disables debug mode, sets the production secret key, and allowed hosts. It also makes use of the django_heroku package for further settings. . import django_heroku # default: use settings from main settings.py if not overwritten from .settings import * DEBUG = False SECRET_KEY = os.getenv(&#39;DJANGO_SECRET_KEY&#39;, SECRET_KEY) # adjust this to the URL of your Heroku app ALLOWED_HOSTS = [&#39;pytorch-django.herokuapp.com&#39;] # Activate Django-Heroku. django_heroku.settings(locals()) . Procfile and Requirements . Also, add a Procfile in the project root that indicates how to prepare the release and deployment on Heroku using gunicorn: . release: python manage.py migrate --no-input web: gunicorn pytorch_django.wsgi . The paths depend on the project name and directory structure. . Also specify the requirements that need to be installed for the app in requirements.txt: . -f https://download.pytorch.org/whl/torch_stable.html django==3.2 whitenoise==5.2.0 gunicorn==20.0.4 django-heroku==0.3.1 # cpu version of torch and torchvision for heroku to reduce slug size torch==1.8.1+cpu torchvision==0.9.1+cpu . For deployment on Heroku, it&#39;s important to use the CPU version of PyTorch since the slug size is otherwise too large (above 500 MB), which leads to a build error (see StackOverflow). The free Herku dynos only support CPU anyways. . Static Files . For serving static files (here, the JSON containing the ImageNet label mapping), configure STATIC_ROOT, STATIC_URL, and STATICFILES_DIR in settings.py: . STATIC_URL = &#39;/static/&#39; # path to where static files are copied for deployment (eg, for heroku) STATIC_ROOT = os.path.join(BASE_DIR, &#39;staticfiles&#39;) # location of static files in local development: https://learndjango.com/tutorials/django-favicon-tutorial STATICFILES_DIRS = [os.path.join(BASE_DIR, &#39;static&#39;),] . For production, use whitenoise as described here. Make sure to add the staticfiles directory to GitHub as it will not be created automatically by Django. . The STATIC_ROOT is used inside views.py (see above) to load the JSON file for mapping. To copy all static files from their STATICFILES_DIRS to STATIC_ROOT, run . python manage.py collectstatic . This is only required once locally. Heroku executes it on each deploy automatically. . Testing the Deployed App . Check the Heroku activity/logs to see if the build and deployment are successful. After successful deployment, access the app at its URL. Mine is at https://pytorch-django.herokuapp.com/. . What Next? . Outcomes of this blog post: . Deployed app | GitHub code | . Other blog posts: . Posts related to Django | Posts related to PyTorch | Posts related to Heroku | . External links: . PyTorch tutorial on image classification with DenseNet and Flask | PyTorch DenseNet information | ImageNet dataset | .",
            "url": "https://stefanbschneider.github.io/blog/pytorch-django",
            "relUrl": "/pytorch-django",
            "date": " • Apr 11, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "Scaling Deep Reinforcement Learning to a Private Cluster",
            "content": ". This guide is still work in progress and currently incomplete! Thanks already to the Ray team (particularly Alex) for [their great support](https://discuss.ray.io/t/getting-started-with-rllib-on-a-private-cluster/683). In this blog post, I use reinforcement learning (RL) to solve a custom optimization task (here, related to coordination in mobile networks). To this end, I use the scalable RL framework RLlib, which is part of Ray, and a custom environment, which implements the OpenAI Gym interface. As RL algorithm, I use proximal policy optimization (PPO), which is implemented in RLlib and configured in my environment. . I first show how to train PPO on my environment when running locally. Then, to speed up training, I execute training on a private/on-premise multi-node cluster. . While it is simple in principle, it took me a while to go from running RLlib and my custom environment locally to getting it to work on a private cluster. I’m hoping this guide is useful for anyone in a similar situation. In this blog post, I focus on the general workflow but use my specific environment as an example. I will cover details about my RL approach and environment in a future blog post. . Training an RL Agent Locally . Setup . Installation requires Python 3.8+ and should work on Linux, Windows, and Mac. Inside a virtualenv, install RLlib with . pip install ray[rllib] . Then install the custom environment. Here, DeepCoMP as described in the readme: . pip install deepcomp . Test the installation with deepcomp -h, which should show the available CLI options. . Training . Once installation is complete, train a centralized RL agent with PPO in an example scenario. Note, that training will take a while (around 15min on my laptop), so running the command inside a detachable GNU screen or tmux session makes sense. . deepcomp --agent central --train-steps 100000 --env medium --slow-ues 3 . This trains a centralized PPO agent for 100k training steps running on a single core. To use more cores, set the corresponding value via CLI argument --workers. The additional arguments --env and --slow-ues configure my custom DeepCoMP environment (more about that in another blog post). During training, updates should be printed on screen and progress can be monitored with TensorBoard. To start TensorBoard, run (in a separate terminal): . tensorboard --logdir results/PPO/ . Here, the TensorBoard files are in results/PPO/, but this depends on the environment. Once started, TensorBoard can be accessed at localhost:6006. . . Results . In the case of my environment, results are saved in the results directory on the project root (where deepcomp is installed) by default. To specify a custom result path, use the --result-dir CLI argument, which accepts relative paths. . Files in folders prefixed with PPO contain neural network weights, configuration, log, and progress files generated by RLlib. They are useful for analyzing training progress or when loading a trained agent for inference (--test arg) or continued training (--continue). Additionally, folders test and videos are generated by DeepCoMP and contain easy-to-parse testing/evaluation results and rendered videos, depending on the DeepCoMP CLI args (--eval and --video). . Of course, this is just an example. Results are saved differently for each problem and environment. . Scaling to Training in a Private Cluster . The nice thing about RLlib is that it can seamlessly scale from running locally to a large cluster. . Preparations . While there are virtually no code changes required in the environment, some preparation steps were necessary for me to get RLlib to work on our private/on-premise cluster. . Cluster Configuration . The Ray cluster configuration is saved in a YAML file. My configuration file is here. . The most relevant fields concern information about the private cluster: . provider: type: local head_ip: &lt;head-machine-ip-or-address&gt; worker_ips: - &lt;worker1-ip&gt; - &lt;worker2-ip&gt; . Here, type: local indicates that the cluster is local/private/on premise. The head IP or address points to the head node, i.e., the machine that should coordinate the cluster. To execute commands and train my RL agent, I will later attach to the head node, start training and TensorBoard, and finally retrieve results. The workers are other machines in the cluster on which the training is executed. . Depending on the number of workers listed under worker_ips, also set min_workers and max_workers to the same value. . For authentication when logging into the workers and distributing computation across them, also configure auth: . auth: ssh_user: stefan # Optional if an ssh private key is necessary to ssh to the cluster # This is the SSH key of the local laptop, not of the head node ssh_private_key: ~/.ssh/id_rsa . Installation . To run code on the workers, install ray[rllib] and the custom environment deepcomp on each worker machine of the cluster. All nodes in the cluster must have the same Python and same ray version (check with --version inside the virutalenv). . . Maybe this can be avoided, eg, by using Docker images that are pulled automatically? SSH Access . The head node needs ssh access to all worker nodes. Ensure the head node’s public SSH key is registered as authorized key (in ssh/authorized_keys) in all worker nodes. The head node’s private key path should be configured in the cluster.yaml. . . In fact, the private key configured in `cluster.yaml` is the private key of the local laptop that controls the cluster. Not the head node. ray command . The ray command needs to be available on all cluster nodes. If the ray command is not available on the cluster, trying to start the cluster will crash with the error Command &#39;ray&#39; not found ... Failed to setup head node.. . If ray is installed in a virtual environment, the easiest option is to automatically source the virtualenv on each login. Particularly, adding the following line to .bashrc will source the virtualenv: . source path/to/venv/bin/activate . Where path/to/venv needs to point to the virtualenv. The change is in effect after log out and back in. . Then ray --version should run without errors. . Connect to Ray cluster . To ensure that running ray connects to the same cluster and the same Redis DB, use ray.init(address=&#39;auto&#39;). Without argument address=&#39;auto&#39;, execution on the cluster does not work. . However, for me, adding address=&#39;auto&#39; breaks local execution. Hence, I added an optional CLI argument --cluster to my custom deepcomp environment, which adds address=&#39;auto&#39; for running the environment on a cluster without code changes. . Starting the Ray Cluster . On the local machine . Start cluster: . # start the cluster (non-blocking) ray up cluster.yaml # forward the cluster dashboard to the local machine (this is a blocking command) ray dashboard cluster.yaml . View dashboard: http://localhost:8265 . . This currently doesn&#39;t work for me. It only shows the head node, not the workers. Connect to cluster and run command for training. Note, you can attach but not detach. Thus, better to run this in a screen/tmux session. . ray attach cluster.yaml deepcomp --agent central --train-steps 100000 --env medium --slow-ues 3 --cluster --workers XY . Once training completed, detach/close terminal with Ctrl+D. . Monitoring Training Progress . Training updates should be printed inside the attached terminal | On the cluster’s head node, htop should show ray::RolloutWorker running. | On the cluster’s worker nodes, htop should show ray::PPO()::train() (or similar) to indicate the training is running. | Monitor progress with Tensorboard running tensorboard --host 0.0.0.0 --logdir results/PPO/ on the cluster’s head node. Then access on &lt;head-node-ip&gt;:6006. | . . This currently doesn&#39;t work for me. It seems like the program is only running on the head nodes, not at all on the workers. Retrieving Training &amp; Testing Results . From the local laptop, use ray rsync-down to copy the result files from the cluster’s head node to the local laptop: . # ray rsync-down &lt;cluster-config&gt; &lt;source&gt; &lt;target&gt; ray rsync-down cluster.yaml ~/DeepCoMP/results . . Will be copied to local directory into results. . Terminating the Cluster . From the local laptop: . ray down cluster.yaml . Debugging . If the process above does not work, the logs may contain helpful information for debugging the problem. To print the logs, run on the cluster’s head node: . cat /tmp/ray/session_latest/logs/monitor.* . To print a status overview of the cluster: . ray status --address &lt;address:port&gt; . Where &lt;address:port&gt; belongs to the cluster and is displayed when starting it with ray up cluster.yaml (after To connect to this Ray runtime from another node, run ...). . Common Errors . Command &#39;ray&#39; not found when trying to start the cluster The ray command is not available on the head node after SSH. One solution is to source the virtualenv with ray in the .bashrc or to install ray system-wide. | . | Repeatedly autoscaler +4m36s) Adding 1 nodes of type ray-legacy-head-node-type. when training on the cluster ?? | . | When trying to run code on the cluster after ray attach cluster.yaml: (raylet) OSError: [Errno 98] Address already in use ?? Is the redis server already running; something wrong with the cluster ?? | Stopping and restarting the cluster seems to fix the problem: Detach, then from the laptop stop the cluster: ray down cluster.yaml, then start it again ray up cluster.yaml | . | All load seems to be just on the cluster head and nothing is distributed to the workers (when observing with htop) ?? | . | In the logs: ERROR monitor.py:264 -- Monitor: Cleanup exception. Trying again... | 1 random worker nodes will not be shut down. (due to --keep-min-workers) | . | . What Next? . Ray cluster documentation | DeepCoMP GitHub repository | . Note to self: Todos . Test with multiple nodes on the cluster. Is there a real speedup? | Test running on cluster without installing env (and ray?) on workers | Some basic tests and proper CI (check example command from readme); update Readme with cluster instructions and link to blog; publish new release | Let Ray team know to distribute the blog post: https://discuss.ray.io/t/use-of-ray-logo-in-blog/797 | Measure time to complete training when training on 1 core; 20 cores on 1 machine; 20 cores each on multiple machines 15min on 1 core | 5 min on 20 core | 4 min on 40 core | . | .",
            "url": "https://stefanbschneider.github.io/blog/rllib-private-cluster",
            "relUrl": "/rllib-private-cluster",
            "date": " • Feb 15, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "Using Bootstrap to Style a Django App",
            "content": "Django allows building simple (and complex) web apps quickly, using Django Templates for rendering. By default, forms, buttons, and other elements are not styled and look quite ugly: . . Using Bootstrap and django-crispy-forms, the rendered templates can easily be improved to look much nicer, without having to adjust styling manually. For example: . . As an example, I extend my Django “Hello World” App (described in a previous post) by adding a simple form and rendering it with Bootstrap and django-crispy-forms. All it does is asking for the user’s name and a date and then displaying &lt;username&gt; says &quot;Hello World!&quot; on &lt;date&gt; and a counter of how often the button has been clicked. Still, the small example illustrates how to use Bootstrap and django-crispy-forms. Especially with many or large forms, django-crispy-forms becomes useful to reduce repetitive boilerplate. . Installing django-crispy-forms . Install the dependency and also add it to requirements.txt. . pip install django-crispy-forms . Add crispy_forms inside settings.py: . INSTALLED_APPS = ( ... &#39;crispy_forms&#39;, ) . Enable the Bootstrap 4 template pack, adding the following line to settings.py: . CRISPY_TEMPLATE_PACK = &#39;bootstrap4&#39; . So far, there is no official &#39;bootstrap5&#39; template pack, but the &#39;bootstrap4&#39; pack also seems to work with Bootstrap 5, which I use later. . Building a crispy Form . Creating a Django Form . As an example, I create a new form that allows users to specify their name and an arbitrary date that will be displayed in the “Hello World” app. For that, I create helloworld/forms.py with the following content: . import datetime from django import forms class HelloWorldForm(forms.Form): &quot;&quot;&quot;Form asking for the user&#39;s name and an arbitrary date, both used inside the displayed &#39;Hello World&#39; text.&quot;&quot;&quot; username = forms.CharField(label=&#39;Your Name&#39;, max_length=100) date = forms.DateField(label=&#39;An arbitrary date&#39;, initial=datetime.date.today, widget=forms.widgets.DateInput(attrs={&#39;type&#39;: &#39;date&#39;}), help_text=&#39;The entered name and date will be displayed temporarily but publicly in the &#39; &#39;generated &quot;Hello World&quot; message. It will not be stored.&#39;) . Most field arguments are optional but provide additional information for django-crispy-forms to display in the Bootstrap form. . I then use this form inside views.py for my index view (the only view of the “Hello World” app so far): . def index(request): # retriever counter model instance from DB or create it if it doesn&#39;t exist yet counter, created = Counter.objects.get_or_create(name=&#39;hello-world-button&#39;) # increment counter when a POST request arrives (from the button click) # see previous blog post: https://stefanbschneider.github.io/blog/django-db if request.method == &#39;POST&#39;: counter.value += 1 counter.save() # and get the values filled in form form = HelloWorldForm(request.POST) if form.is_valid(): username = form.cleaned_data[&#39;username&#39;] date = form.cleaned_data[&#39;date&#39;] else: form = HelloWorldForm() username = &#39;Nobody&#39; date = datetime.date.today() context = { &#39;clicks&#39;: counter.value, &#39;form&#39;: form, &#39;username&#39;: username, &#39;date&#39;: date, } return render(request, &#39;helloworld/index.html&#39;, context) . Finally, show the form in the index.html template: . &lt;div class=&quot;alert alert-success&quot; role=&quot;alert&quot;&gt; {{ username }} says: &quot;Hello World!&quot; on {{ date }}. (Button clicked {{ clicks }}x in total.) &lt;/div&gt; &lt;form action=&quot;{% url &#39;helloworld:index&#39; %}&quot; method=&quot;post&quot;&gt; {% csrf_token %} {{ form }} &lt;button type=&quot;submit&quot; class=&quot;btn btn-primary&quot;&gt;Greet the world!&lt;/button&gt; &lt;/form&gt; . Now, when running the development server, the app shows the new form: . . The “Hello World” message should display the entered username and date as well as the total click count. However, the form does not yet use Bootstrap and is still quite ugly! . Making the Form Pretty . To make the form look nicer, I first include Bootstrap. For using Bootstrap, simply include the Bootstrap CCS and JavaScript inside the head of the Django app’s main/base template. The Bootstrap website has the latest instructions. . For my “Hello World” Django app, I simply add the following lines inside helloworld/templates/helloworld/index.html: . &lt;head&gt; &lt;title&gt;Hello World&lt;/title&gt; {# Bootstrap #} &lt;link href=&quot;https://cdn.jsdelivr.net/npm/bootstrap@5.0.0-beta1/dist/css/bootstrap.min.css&quot; rel=&quot;stylesheet&quot; integrity=&quot;sha384-giJF6kkoqNQ00vy+HMDP7azOuL0xtbfIcaT9wjKHr8RbDVddVHyTfAAsrekwKmP1&quot; crossorigin=&quot;anonymous&quot;&gt; &lt;script src=&quot;https://cdn.jsdelivr.net/npm/bootstrap@5.0.0-beta1/dist/js/bootstrap.bundle.min.js&quot; integrity=&quot;sha384-ygbV9kiqUc6oa4msXn9868pTtWMgiQaeYH7/t7LECLbyPA2x65Kgf80OJFdroafW&quot; crossorigin=&quot;anonymous&quot;&gt;&lt;/script&gt; &lt;!-- other includes --&gt; &lt;/head&gt; &lt;!-- body --&gt; . This loads the new Bootstrap 5 from the JSDeliver CDN network so it can be used within the Django app templates. Now, the alert and button should already look nicer, but the form fields will still look ugly. . To also render the form fields with Bootstrap, I use django-crispy-forms. All it takes, is loading crispy and passing the form to crispy inside index.html: . {% load crispy_forms_tags %} &lt;div class=&quot;alert alert-success&quot; role=&quot;alert&quot;&gt; {{ username }} says: &quot;Hello World!&quot; on {{ date }}. (Button clicked {{ clicks }}x in total.) &lt;/div&gt; &lt;form action=&quot;{% url &#39;helloworld:index&#39; %}&quot; method=&quot;post&quot;&gt; {% csrf_token %} {{ form | crispy }} &lt;button type=&quot;submit&quot; class=&quot;btn btn-primary&quot;&gt;Greet the world!&lt;/button&gt; &lt;/form&gt; . Now, the Django app should be rendered with Bootstrap and already look much nicer: . . The nice thing is that crispy will handle all the overhead of styling each form field with bootstrap, which is particularly useful when having many large forms inside a Django app. . What Next? . Django “Hello World” GitHub Repository | Deployed “Hello World” App on Heroku | . Small example apps I built with Django and deployed on Heroku, using Bootstrap: . Quotify: An example app showing inspirational quotes. [Code] [App] | Ideally: Organize &amp; Grow Your Ideas. [Code] [App] | FeelYa: The app that gets you! [Code] [App] | . Other blog posts: . Adding Google Analytics to a Django App | Building a Django App and Deploying It on Heroku | Adding a Database to a Django App | Other blog posts related to Django | .",
            "url": "https://stefanbschneider.github.io/blog/django-bootstrap",
            "relUrl": "/django-bootstrap",
            "date": " • Feb 3, 2021"
        }
        
    
  
    
        ,"post6": {
            "title": "Lessons Learned from Leading My First Project",
            "content": "Today is the last day of my first bigger project, RealVNF, which I have been leading for 27 months (Nov. 2018 - Jan. 2021). Early 2018, I applied for the project through the Software Campus and received a €100k grant for hiring student research assistants, traveling, equipment, etc. I also had the opportunity to participate in several leadership trainings organized by companies across Germany. The project itself was research-focused and a collaboration with researchers from Huawei Germany (more info the website). . I found that leading the project was surprisingly challenging but, in the end, definitely successful and a great experience. This blog post reflects on the last 27 months and summarizes seven lessons learned from a leadership/project management perspective. . . 7 Lessons Learned . Put Effort Into Recruiting the Right People . Recruiting the right people for the project was very important. Most of the people I hired at the beginning stayed until the end of the project. . Initially, I posted the job openings broadly online and received many applications. I also directly asked talented people I knew to apply. To avoid interviewing dozens of applicants, I introduced a small programming exercise that applicants had to submit. It was doable in maybe 1-3 hours using basic Python and tools like GitHub. I invited applicants who finished this first task to an interview, asking them to read a selected research paper up front and prepare to summarize it during the interview. . Both tasks meant initial extra work for both me and the applicants but helped to select the most suitable candidates for the project. In the end, I got to work with motivated and talented people, who made the project a success. . Talk to Potential Users Early . When preparing the initial project plan, of course, I discussed it with people in my group at university. Still, it was only when talking to the researchers at Huawei, I understood that they had a very different perspective on the problem we were trying to solve. We spent the first months discussing and aligning our understanding of the problem and possible solution approaches. These discussions were very valuable to ensure that the work in the project was actually relevant and helpful in practice. As a leading company in networking, Huawei is a potential user of our developed coordination schemes and thus was an ideal project partner. . Especially in academia, it is easy to get excited about an idea without properly thinking (or even understanding) its relevance in practice. Whether in academia or industry, I believe it is very important to talk to potential users/customers early and understand their needs before designing a solution. This is related to the concept of design thinking. . Start Small, Scale Quickly . When starting the project, we had tons of interesting ideas and thought of a variety of relevant problem aspects that we wanted to address. Creating an approach from scratch that incorporates all ideas and considers all problem aspects would have been prohibitively complex. Instead, it was important to quickly identify the most important aspects and most promising ideas and simplify everything else. This helped to get started quickly. . Once the simple approach worked, we could extend it to integrate more ideas and address more problem aspects. It is also helpful to have a working prototype at all times: First for the smallest and simplest case, then for more and more complex scenarios. This allows running evaluations for each stage and quantifying progress. Trying to solve everything at once can be overwhelming and makes it difficult to understand the root cause of problems or bugs. Building on something that works already is much simpler, faster, and less error prone. Versioning everything allows going back to previous, working versions if something breaks in between. . Of course, it makes sense to think of a realistic roadmap and architecture at the beginning to have a clear direction and ensure that initial solutions can easily be extended later. . Focus on Just a Few Topics at Once . At the beginning, I was wondering whether I should assign all students working on the project to their own topic or to let them all work on the same topic. The former would allow more parallelization and independent work, thus less dependencies and blocking each other. Still, I found that it was much easier focusing all work on just one or two topics (of course, still with different tasks). Not only did it help drive progress faster, it also allowed more productive meetings and discussions between team members. . While this worked well for my project with 3-4 people, it will likely be different for much larger projects. . Planning is Everything. The Plan is Nothing. . Especially in a research-focused project like RealVNF, there is a lot of uncertainty, e.g., about experiment outcomes, where the outcome of one task affects the next one. This makes mid-/long-term planning very difficult. . I found it useful to have a high-level “storyline” in mind with intermediate milestones, but to frequently adjust it to new insights or results. When interpreting results and adjusting the plan, we always discussed and decided next steps in the group. It is important for motivation to not only understand what a task is about but also why it is relevant. In general, frequent and clear communication within the team was crucial to quickly adapt to new outcomes and avoid wasting time. . Clear, Frequent, and Open Communication Within the Team . Frequent communication with and within the team is crucial. Still, I did not want to waste everyone’s time with unnecessary meetings. For us, weekly group meetings worked quite well. I structured each meeting into status updates, discussion items, and tasks for the next week and tried to limit them to at most an hour. Upcoming tasks need to be communicated very clearly (goal, scope, time) to avoid misunderstandings. Sending out short notes after each meeting helped to keep everyone on the same page. . For an overview of past, current, and upcoming tasks, we also used GitHub issues and project boards (Kanban boards). Additionally, we defined high-level monthly goals/milestones to keep the project a bit more focused. To keep the discussion going during the week and quickly adapt to new experiment outcomes or new insights, we kept in touch via Slack constantly. . We also did individual feedback discussions every now and then, which were really useful. In retrospective, I would do one-to-one meetings more regularly to better understand each team member’s needs and interests. It is important to encourage open communication at all times such that team members say if they have too many or too few tasks. Both is demotivating. At the same time, it can be difficult to estimate how quickly someone will progress with a task. . Finally, while communication and tooling still worked well when working remotely during COVID-19, social meetings were no longer possible. Such social meetings were nice to hang out informally and improve team spirit. . Embrace Bureaucracy . I suppose any bigger project comes with bureaucracy. In my case, I had to communicate with the funding partner and the university’s administration. Sometimes, I was surprised by the amount of rules, formulas, and processes for hiring people, traveling, and buying equipment - even with the money available (i.e., after securing the grant). . As a small anecdote, I tried to hire student research assistants for the project to start with the beginning of the project in November. Unfortunately, this was not possible and I was told that new contracts could not start at the end of a year. I had to wait until January for hiring the students. In January, I was asked why I did not follow my original budget plan and did not spend any money on staff during the first two months (November and December)… . Still, while some rules may be a bit bizarre, all people in the administration were friendly and helpful. I understood that I had to identify the right people to ask and keep asking persistently to eventually understand the rules and get what I needed. . Thank You . Leading this first project was a great experience and an amazing opportunity. I want to thank everyone directly or indirectly involved in the project! Particularly, . thank you to my colleagues at Paderborn University, the project partners at Huawei, and all student research assistants. It was a pleasure to work with you! | thank you to Software Campus, which provided the framework and opportunity for this project! | thank you to the German Ministry for Education and Research (BMBF) for providing funding! This also means thank you to everyone paying taxes in Germany :) | . . Further Information . RealVNF Project Website and Outcomes | RealVNF Official Project Description | RealVNF GitHub Repositories | Software Campus | All Blog Posts on Leadership | .",
            "url": "https://stefanbschneider.github.io/blog/my-first-project",
            "relUrl": "/my-first-project",
            "date": " • Jan 31, 2021"
        }
        
    
  
    
        ,"post7": {
            "title": "Adding a Database to a Django App",
            "content": "Most Django apps store and work with data, e.g., entered by users. Django supports multiple relational databases for persistent storage of such data out of the box. In this blog post, I show how to a database to a Django app. For local development, I use the default SQLite database and for production deployment on Heroku, I use PostgreSQL® 1. . I use the Django “Hello World” App as simple example. I add a button that users can click to say “Hello World!” and count how often it was clicked. The number of clicks is stored in a database and displayed in the app. . The whole process is very simple as Django does most of the work in the background: Creating the database, making the queries, etc. . Requirements . A Django app. For example, the Django “Hello World” App. GitHub repository. This blog post corresponds to release v1.2. | Initial blog post on building and deploying the Django app on Heroku | . | For automated deployment, a free Heroku and GitHub account. | . Defining a Model . Django defines types of data as models. Instances of these models are then saved in a database, where the attributes correspond to the database table’s columns and the instances to the rows. . In this example, I just want to save the value of a counter. I define a counter by a name (in case I want to distinguish multiple counters) and a value in helloworld/models.py: . from django.db import models class Counter(models.Model): name = models.CharField(max_length=100) value = models.IntegerField(default=0) def __str__(self): return f&quot;{self.name}: {self.value}&quot; . Saving the Model in a Database . When developing locally, Django automatically creates a SQLite db.sqlite3. To save the new model in the database and create or adjust the corresponding database tables, Django offers simple CLI commands: . python manage.py makemigrations python manage.py migrate . The first command generates the code for making the necessary adjustments to the database. This code is stored in helloworld/migrations and should be committed to git. The second command then applies the adjustments to the database. . Using the Model in the Django App . To integrate the model into the “Hello World” app, I first create a view that handles the interaction with the model. In helloworld/views.py: . from django.shortcuts import render from .models import Counter def index(request): # retriever counter model instance from DB or create it if it doesn&#39;t exist yet counter, created = Counter.objects.get_or_create(name=&#39;hello-world-button&#39;) # increment counter when a POST request arrives (from the button click) if request.method == &#39;POST&#39;: counter.value += 1 counter.save() context = { &#39;clicks&#39;: counter.value, } return render(request, &#39;helloworld/index.html&#39;, context) . This view replaces the previous TemplateView in the app’s URL settings (helloworld/urls.py): . from django.urls import path from . import views app_name = &#39;helloworld&#39; urlpatterns = [ # path(&#39;&#39;, TemplateView.as_view(template_name=&#39;helloworld/index.html&#39;), name=&#39;index&#39;), path(&#39;&#39;, views.index, name=&#39;index&#39;), ] . Finally, I need to adjust the template to add a button and display the number of times it was clicked (helloworld/templates/helloworld/index.html): . Hello World! ({{ clicks }}x) &lt;form action=&quot;{% url &#39;helloworld:index&#39; %}&quot; method=&quot;post&quot;&gt; {% csrf_token %} &lt;button type=&quot;submit&quot; name=&quot;hello-world-button&quot;&gt;Greet the world!&lt;/button&gt; &lt;/form&gt; . When running the app, it should now look like this: . . Clicking the button should reload the page and show an increased click count (here, “2x”). The click count should persist even when the app server is restarted or the app is updated. . Of course, this is super slow and just an example for illustrating persistent storage in Django. In production, this should happen asynchronously with some frontend framework. . PostgreSQL® DB on Heroku . The SQLite database used for local development is not suitable for production deployment. To enable persistent storage for my “Hello World” app deployed on Heroku, I use Heroku’s PostgreSQL® database, which is free for such small use cases. . For deployment, Heroku uses by default a free PostgreSQL® database. This database is automatically created when initially setting up the Django app on Heroku. The available databases on Heroku are listed in the Heroku data dashboard. . . If a database is already listed for the Django app, then persistent storage should work out of the box for the app once it is deployed. . What Next? . Django “Hello World” GitHub Repository | Deployed “Hello World” App on Heroku | . Small example apps I built with Django and deployed on Heroku, using persistent storage: . Quotify: An example app showing inspirational quotes. [Code] [App] | Ideally: Organize &amp; Grow Your Ideas. [Code] [App] | FeelYa: The app that gets you! [Code] [App] | . Other blog posts: . Adding Google Analytics to a Django App | Building a Django App and Deploying It on Heroku | Using Bootstrap to Style a Django App | Other blog posts related to Django | Other blog posts related to Heroku | . Postgres, PostgreSQL and the Slonik Logo are trademarks or registered trademarks of the PostgreSQL Community Association of Canada, and used with their permission. &#8617; . |",
            "url": "https://stefanbschneider.github.io/blog/django-db",
            "relUrl": "/django-db",
            "date": " • Jan 29, 2021"
        }
        
    
  
    
        ,"post8": {
            "title": "Adding Google Analytics to a Django App",
            "content": "Google Analytics is the most popular solution to monitor and analyze web traffic. It provides detailed information about users visiting your website or web app - both historically and in real-time. . Here, use the new Google Analytics GA4 to create a &quot;Google Analytics property&quot; for monitoring analyzing the traffic on a Django web app. As simple example app, I build on the previously described Django &quot;Hello World&quot; app. I use WebsitePolicies1 to create a free cookie banner. . The final Django app is deployed on Heroku and available here. The final code is in this this GitHub repository. . Requirements . A Django web app. This post builds on the Django &quot;Hello World&quot; app described in this post. Release v1.1.0 belongs to this blog post. | . | A Google account you can use for Google Analytics. It&#39;s free. | . Creating the Google Analytics GA4 Property . Go to the Google Analytics dashboard and log in if requested. To monitor a new Django app, create a new Google Analytics property by navigating to Admin &gt; + Create Property. . . Fill in the fields, selecting a property name, region, currency, and information regarding the monitored Django app. . . Once the new property is created, select Data Streams &gt; Web to set up a new data stream for the Django web app. Paste the URL of the Django app: . . Adding Google Analytics to the Django App . Get the code snippet from the created Google Analytics data stream from the created data stream site under Tagging Instructions &gt; Add new on-page tag &gt; Global Site Tag (gtag.js). The snippet should look like this (of course with the real tag ID instead of YOURTAGID): . &lt;!-- Global site tag (gtag.js) - Google Analytics --&gt; &lt;script async src=&quot;https://www.googletagmanager.com/gtag/js?id=YOURTAGID&quot;&gt;&lt;/script&gt; &lt;script&gt; window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag(&#39;js&#39;, new Date()); gtag(&#39;config&#39;, &#39;YOURTAGID&#39;); &lt;/script&gt; . Copy the code snippet and paste it inside the Django app&#39;s main/base template. In case of my &quot;Hello World&quot; app, there is just a single template, which looks like this: . &lt;!DOCTYPE html&gt; &lt;html lang=&quot;en&quot;&gt; &lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;title&gt;Hello World&lt;/title&gt; &lt;!-- Global site tag (gtag.js) - Google Analytics --&gt; &lt;script async src=&quot;https://www.googletagmanager.com/gtag/js?id=MYTAGID&quot;&gt;&lt;/script&gt; &lt;script&gt; window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag(&#39;js&#39;, new Date()); gtag(&#39;config&#39;, &#39;MYTAGID&#39;); &lt;/script&gt; &lt;/head&gt; &lt;body&gt; Hello World! &lt;/body&gt; &lt;/html&gt; . Save, commit, push, and deploy. Google Analytics will likely only work on the deployed app in production. . To validate that it works, open the Django app in Chrome and open developer tools. Opening Application &gt; Cookies should show that the Django app now has cookies used for Google Analytics: . . Opening the Google Analytics realtime dashboard should show that there is a visitor (this may take a few seconds): . . Note: Firefox may block Google Analytics or send &quot;Do-Not-Track&quot; Signals such that visiting the Django app is not recognized or shown in the realtime dashboard. For testing, turn off tracking protection or use Chrome. . Adding a Cookie Banner . Using cookies for Google Analytics legally requires informing users of the Django app of these cookies. If the app does not yet have a cookie banner, it&#39;s now time to create one. . I use WebsitePolicies1 for generating suitable cookie banners. The service is simple and free for non-commercial apps. Simply fill in the questions, and the service generates an HTML snippet to copy and paste into the Django app&#39;s base template (similar to the GA4 tag). . The updated web page should show a cookie banner similar to this: . . What Next? . Create a cookie banner and privacy policy on WebsitePolicies1 | Migrating an existing Google Analytics universal property to the new GA4 (Guide by Google) | Django Hello World GitHub Repository | . Other blog posts: . Building a Django App and Deploying It on Heroku | Adding a Database to a Django App | Using Bootstrap to Style a Django App | Other blog posts related to Django | . 1. Affiliate link: WebsitePolicies offers free cookie banners and policies for non-commercial apps. If you buy an advanced option via my link, e.g., for commercial purposes, I will get a small commission. This does not affect pricing for you.↩ .",
            "url": "https://stefanbschneider.github.io/blog/django-google-analytics",
            "relUrl": "/django-google-analytics",
            "date": " • Jan 21, 2021"
        }
        
    
  
    
        ,"post9": {
            "title": "Building a Django App and Deploying It on Heroku",
            "content": "In this guide, I show how to setup a simple Django web app that says &quot;Hello World!&quot; and how to deploy it on Heroku through GitHub. The process is simple enough, but I still kept running into errors when deploying new Django apps, so I thought to finally just write it down step-by-step. . Requirements . For that, Heroku and GitHub accounts are required. Both are free. . All code should be in a GitHub repository. For reference, this link my GitHub repository. The steps described here belong to release v1.0.0. . Django &quot;Hello World&quot; App . Initial Setup . Install Django: . pip install django . Create a new Django project (inside the GitHub repository): . django-admin startproject myproject . A project can consist of multiple apps. Create a new Django helloworld app inside the myproject project: . cd myproject python manage.py startapp helloworld . Run the dev server: . python manage.py runserver . Go to http://localhost:8000/ in the browser. This should confirm the successful initial setup: . . Create the &quot;Hello World&quot; App . Link the helloworld app to the project by adding . &#39;helloworld.apps.HelloworldConfig&#39;, . to the INSTALLED_APPS inside myproject/settings.py. . Create a new HTML-template index.html inside helloworld/templates/helloworld (also create these folders): . &lt;!DOCTYPE html&gt; &lt;html lang=&quot;en&quot;&gt; &lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;title&gt;Hello World&lt;/title&gt; &lt;/head&gt; &lt;body&gt; Hello World! &lt;/body&gt; &lt;/html&gt; . Create helloworld/urls.py to configure a URL path showing the new template: . from django.urls import path from django.views.generic import TemplateView urlpatterns = [ path(&#39;&#39;, TemplateView.as_view(template_name=&#39;helloworld/index.html&#39;), name=&#39;index&#39;), ] . Finally, link to the helloworld URLs from the project&#39;s myproject/urls.py: . from django.contrib import admin from django.urls import path, include urlpatterns = [ path(&#39;&#39;, include(&#39;helloworld.urls&#39;)), path(&#39;admin/&#39;, admin.site.urls), ] . Now, http://localhost:8000/ should show the dummy &quot;hello world&quot; message: . . Deploying the App on Heroku . Creating a New App on Heroku . To create a new app on Heroku, I log into the Heroku dashboard and select New &gt; Create new app. For the new app, any unique name is ok (I chose django-hello-world-app). . . Note: The name of your Heroku app will also determine its URL, which is &lt;app-name&gt;.herokuapp.com. It can still be changed in the settings later. Next, I select my GitHub repository as deployment method inside Heroku (this may require authorization using GitHub credentials): . . Finally, I have to set some configuration variables (= environmental variables) inside Heroku: Settings &gt; Config Vars &gt; Reveal Config Vars. . Set DJANGO_SETTINGS_MODULE to myproject.prod_settings, which is are the settings for production deployment, created later. | Set DJANGO_SECRET_KEY to a randomly generated secret key that is used for deployment. This key must not be commited to the GitHub repository. | . . Preparing Deployment . Before the actual deployment on Heroku, a few additional steps are required. . Currently, the generated folder structure should look like this: . myproject/ helloworld/ myproject/ manage.py . For deployment on Heroku, move everything into the top-level folder such that manage.py is in the project root and there is no more top-level myproject directory: . helloworld/ myproject/ manage.py . The Procfile indicates how to deploy and serve the web app, here with gunicorn: . release: python manage.py migrate --no-input web: gunicorn myproject.wsgi . Accordingly, the dependencies of this &quot;hello world&quot; app are (saved in requirements.txt): . django django-heroku gunicorn . Heroku needs django-heroku for proper deployment (see Heroku docs). . Specify the root path for static files by appending to the myproject/settings.py (also import os): . # Static files (CSS, JavaScript, Images) # https://docs.djangoproject.com/en/3.1/howto/static-files/ STATIC_URL = &#39;/static/&#39; # path to where static files are copied for deployment (eg, for heroku) STATIC_ROOT = os.path.join(BASE_DIR, &#39;staticfiles&#39;) # location of static files in local development: https://learndjango.com/tutorials/django-favicon-tutorial # not needed yet, only once adding static files (eg, images) # STATICFILES_DIRS = [os.path.join(BASE_DIR, &#39;static&#39;),] . Create myproject/prod_settings.py with additional settings for production deployment: . &quot;&quot;&quot; Production Settings &quot;&quot;&quot; # default: use settings from main settings.py if not overwritten from .settings import * import django_heroku DEBUG = False SECRET_KEY = os.getenv(&#39;DJANGO_SECRET_KEY&#39;, SECRET_KEY) # adjust to the URL of your Heroku app ALLOWED_HOSTS = [&#39;django-hello-world-app.herokuapp.com&#39;] # Activate Django-Heroku. django_heroku.settings(locals()) . Commit and push everything to the repository&#39;s main branch. . Automated Deployment via GitHub . To test the deployment, go to the Heroku dashboard Deploy &gt; Manual deploy, select the main branch and deploy. After the build and deployment succeeds, click Open app to open &lt;app-name&gt;.herokuapp.com, which should show &quot;Hello World&quot;. Deployment successful! . Finally, enable automatic deploys at Deploy &gt; Automatic deploys &gt; Enable such that the latest version of the Django app is build and deployed automatically with every push to the main branch. . Note: Heroku&#8217;s free dynos are free but power off if they are unused. So loading a deployed app that hasn&#8217;t been used in a while may take multiple seconds. . What Next? . Small example apps I built with Django and deployed on Heroku: . Quotify: An example app showing inspirational quotes. [Code] [App] | Ideally: Organize &amp; Grow Your Ideas. [Code] [App] | FeelYa: The app that gets you! [Code] [App] | . Other blog posts: . Adding Google Analytics to a Django App | Posts related to Django | Posts related to Heroku | .",
            "url": "https://stefanbschneider.github.io/blog/django-heroku",
            "relUrl": "/django-heroku",
            "date": " • Jan 19, 2021"
        }
        
    
  
    
        ,"post10": {
            "title": "Getting Started with PyTorch",
            "content": ". Note: Code from the official PyTorch 60-min-blitz tutorial. . Loading the CIFAR10 Dataset . import torch import torchvision import torchvision.transforms as transforms . . transform = transforms.Compose( [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]) trainset = torchvision.datasets.CIFAR10(root=&#39;./data&#39;, train=True, download=True, transform=transform) trainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True, num_workers=2) testset = torchvision.datasets.CIFAR10(root=&#39;./data&#39;, train=False, download=True, transform=transform) testloader = torch.utils.data.DataLoader(testset, batch_size=4, shuffle=False, num_workers=2) classes = (&#39;plane&#39;, &#39;car&#39;, &#39;bird&#39;, &#39;cat&#39;, &#39;deer&#39;, &#39;dog&#39;, &#39;frog&#39;, &#39;horse&#39;, &#39;ship&#39;, &#39;truck&#39;) . Files already downloaded and verified Files already downloaded and verified . import matplotlib.pyplot as plt import numpy as np # functions to show an image def imshow(img): img = img / 2 + 0.5 # unnormalize npimg = img.numpy() plt.imshow(np.transpose(npimg, (1, 2, 0))) plt.show() # get some random training images dataiter = iter(trainloader) images, labels = dataiter.next() # show images imshow(torchvision.utils.make_grid(images)) # print labels print(&#39; &#39;.join(&#39;%5s&#39; % classes[labels[j]] for j in range(4))) . . ship frog truck bird . Building a CNN Model with PyTorch . Architecture: . Input: 32x32-pixel images with 3 channels (RGB) &rarr; 3x32x32 images | Convolutions with 3 input channels, 6 output channels, and 5x5 square convolution &rarr; 6x28x28 images | 2x2 max pooling (subsampling) &rarr; 6x14x14 images | 6 input channels (from the previous Conv2d layer), 16 output channels, 5x5 square convolutions &rarr; 16x10x10 images | 2x2 max pooling (subsampling) &rarr; 16x5x5 images | Fully connected linear (=dense) layer with 16x5x5=400 input size and 120 output; ReLU activation | Fully connected layer with 120 input and 84 output; ReLU activation | Fully connected output layer with 84 input and 10 output (for the 10 classes in the CIFAR10 dataset); no/linear activation | . Note that the layers are defined in the constructor and the activations applied in the forward function. . To calculate the output size of a convolutional layer, use this formula: . $ frac{W−K+2P}{S} +1$ with input size $W$ (width and height for square images), convolution size $K$, padding $P$ (default 0), and stride $S$ (default 1). . Further explanation on layer sizes: Medium article by Jake Krajewski . import torch.nn as nn import torch.nn.functional as F class Net(nn.Module): def __init__(self): super(Net, self).__init__() self.conv1 = nn.Conv2d(3, 6, 5) self.pool = nn.MaxPool2d(2, 2) self.conv2 = nn.Conv2d(6, 16, 5) self.fc1 = nn.Linear(16 * 5 * 5, 120) self.fc2 = nn.Linear(120, 84) self.fc3 = nn.Linear(84, 10) def forward(self, x): x = self.pool(F.relu(self.conv1(x))) x = self.pool(F.relu(self.conv2(x))) x = x.view(-1, 16 * 5 * 5) x = F.relu(self.fc1(x)) x = F.relu(self.fc2(x)) x = self.fc3(x) return x net = Net() . Define the loss as cross entropy loss and SGD as optimizer. . import torch.optim as optim criterion = nn.CrossEntropyLoss() optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9) . Training . Over 5 epochs. . for epoch in range(5): # loop over the dataset multiple times running_loss = 0.0 for i, data in enumerate(trainloader, 0): # get the inputs; data is a list of [inputs, labels] inputs, labels = data # zero the parameter gradients optimizer.zero_grad() # forward + backward + optimize outputs = net(inputs) loss = criterion(outputs, labels) loss.backward() optimizer.step() # print statistics running_loss += loss.item() if i % 2000 == 1999: # print every 2000 mini-batches print(&#39;[%d, %5d] loss: %.3f&#39; % (epoch + 1, i + 1, running_loss / 2000)) running_loss = 0.0 print(&#39;Finished Training&#39;) . [1, 2000] loss: 2.200 [1, 4000] loss: 1.837 [1, 6000] loss: 1.695 [1, 8000] loss: 1.587 [1, 10000] loss: 1.534 [1, 12000] loss: 1.469 [2, 2000] loss: 1.391 [2, 4000] loss: 1.377 [2, 6000] loss: 1.361 [2, 8000] loss: 1.332 [2, 10000] loss: 1.306 [2, 12000] loss: 1.297 [3, 2000] loss: 1.227 [3, 4000] loss: 1.220 [3, 6000] loss: 1.202 [3, 8000] loss: 1.217 [3, 10000] loss: 1.180 [3, 12000] loss: 1.187 [4, 2000] loss: 1.093 [4, 4000] loss: 1.094 [4, 6000] loss: 1.141 [4, 8000] loss: 1.109 [4, 10000] loss: 1.127 [4, 12000] loss: 1.125 [5, 2000] loss: 1.033 [5, 4000] loss: 1.047 [5, 6000] loss: 1.039 [5, 8000] loss: 1.072 [5, 10000] loss: 1.039 [5, 12000] loss: 1.061 Finished Training . . Save the trained model locally. . PATH = &#39;./cifar_net.pth&#39; torch.save(net.state_dict(), PATH) . Testing the Trained Model . dataiter = iter(testloader) images, labels = dataiter.next() # print images imshow(torchvision.utils.make_grid(images)) print(&#39;GroundTruth: &#39;, &#39; &#39;.join(&#39;%5s&#39; % classes[labels[j]] for j in range(4))) . GroundTruth: cat ship ship plane . net = Net() net.load_state_dict(torch.load(PATH)) # make predictions outputs = net(images) _, predicted = torch.max(outputs, 1) print(&#39;Predicted: &#39;, &#39; &#39;.join(&#39;%5s&#39; % classes[predicted[j]] for j in range(4))) . Predicted: dog car car plane . correct = 0 total = 0 with torch.no_grad(): for data in testloader: images, labels = data outputs = net(images) _, predicted = torch.max(outputs.data, 1) total += labels.size(0) correct += (predicted == labels).sum().item() print(&#39;Accuracy of the network on the 10000 test images: %d %%&#39; % ( 100 * correct / total)) . Accuracy of the network on the 10000 test images: 60 % . class_correct = list(0. for i in range(10)) class_total = list(0. for i in range(10)) with torch.no_grad(): for data in testloader: images, labels = data outputs = net(images) _, predicted = torch.max(outputs, 1) c = (predicted == labels).squeeze() for i in range(4): label = labels[i] class_correct[label] += c[i].item() class_total[label] += 1 for i in range(10): print(&#39;Accuracy of %5s : %2d %%&#39; % ( classes[i], 100 * class_correct[i] / class_total[i])) . Accuracy of plane : 70 % Accuracy of car : 75 % Accuracy of bird : 44 % Accuracy of cat : 33 % Accuracy of deer : 57 % Accuracy of dog : 56 % Accuracy of frog : 74 % Accuracy of horse : 61 % Accuracy of ship : 59 % Accuracy of truck : 71 % . What Next? . Using PyTorch Inside a Django App | Other blog posts related to PyTorch | Official PyTorch Tutorials | .",
            "url": "https://stefanbschneider.github.io/blog/pytorch-getting-started",
            "relUrl": "/pytorch-getting-started",
            "date": " • Dec 30, 2020"
        }
        
    
  
    
        ,"post11": {
            "title": "Fastpages Notebook Blog Post",
            "content": ". Note: This post was generated by fastpages and kept for reference. . About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://stefanbschneider.github.io/blog/test",
            "relUrl": "/test",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post12": {
            "title": "An Example Markdown Post",
            "content": ". This post was generated by fastpages and kept for reference. Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://stefanbschneider.github.io/blog/test-markdown-post",
            "relUrl": "/test-markdown-post",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About",
          "content": "I am a computer science PhD candidate and passionate about learning new things. I use this blog to take notes for myself - and possibly help others. . Contact details and more information about myself are on my website. .",
          "url": "https://stefanbschneider.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://stefanbschneider.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}