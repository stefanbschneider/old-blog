{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Why and When Self-Attention Matters in Reinforcement Learning\n",
    "> \"Using RLlib and AttentionNet to master environments with stateless observations, here, stateless CartPole.\"\n",
    "\n",
    "- hide: true\n",
    "- toc: true\n",
    "- branch: master\n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [python, ray, rllib, tensorflow, machine learning, reinforcement learning, attention]\n",
    "- image: images/cartpole.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "TODOs:\n",
    "* Image and short explanation of (self-)attention.\n",
    "* Proper description of different options\n",
    "\n",
    "\n",
    "In reinforcement learning (RL), the RL agent typically selects a suitable action based on the last observation.\n",
    "Since many practical environments are stateful, this state should be taken into account when selecting an action.\n",
    "As an example, consider the popular [OpenAI Gym CartPole environment](https://gym.openai.com/envs/CartPole-v1/),\n",
    "where the task is to move a cart left or right in order to balance a pole on the cart as long as possible.\n",
    "\n",
    "![OpenAI Gym CartPole-v1 Environment](attention/cartpole.gif \"OpenAI Gym CartPole-v1 Environment\")\n",
    "\n",
    "Whether the cart should be moved left or right clearly depends on how the pole is currently moving,\n",
    "i.e., in which direction it is swinging and with which velocity.\n",
    "In this example, the pole's movement and velocity are an important part of the state,\n",
    "which should determine the selected action (left or right).\n",
    "\n",
    "There are different options how to deal with this state:\n",
    "\n",
    "* Simplest case: Ignore it. If the RL agent only observes the raw pixels or current position of the pole,\n",
    "but not the pole movement and velocity (i.e., the full state), it is very hard to learn a useful policy.\n",
    "* Observation with state, i.e., explicitly containing pole movement and velocity.\n",
    "* Sequence of last observations\n",
    "* Sequence of last observations + attention\n",
    "\n",
    "\n",
    "## Setup\n",
    "\n",
    "Install Ray RLlib and TensorFlow (also works with PyTorch):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install ray[rllib]==1.8.0\n",
    "!pip install tensorflow==2.7.0"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "I am using Python 3.8 on Windows 10.\n",
    "\n",
    "\n",
    "## Solving the Default CartPole with Explicit State"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ray'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-1-824c480e2f37>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[1;32mimport\u001B[0m \u001B[0mray\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      2\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mray\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtune\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mray\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mrllib\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0magents\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mppo\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mray\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mrllib\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mexamples\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0menv\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstateless_cartpole\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mStatelessCartPole\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'ray'"
     ]
    }
   ],
   "source": [
    "import ray\n",
    "import ray.tune\n",
    "from ray.rllib.agents import ppo\n",
    "from ray.rllib.examples.env.stateless_cartpole import StatelessCartPole\n",
    "\n",
    "\n",
    "ray.init()\n",
    "registry.register_env(\"StatelessCartPole\", lambda _: StatelessCartPole())\n",
    "\n",
    "config = ppo.DEFAULT_CONFIG.copy()\n",
    "config[\"env\"] = \"StatelessCartPole\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# train for 2 iterations\n",
    "results = ray.tune.run(\"PPO\", config=config, stop={\"train_iterations\": 2})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}