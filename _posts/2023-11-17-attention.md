---
published: false
toc: true
comments: true
layout: post
title: Understanding Attention and Transformers
description: My notes for understanding the attention mechanism and transformer architecture used by GPT-4 and other LLMs.
image: 
categories: [attention, machine learning, llm]
---

I recently read again the famous paper ["Attention is all you need" by Vaswani et al.](https://arxiv.org/abs/1706.03762), which presents the transformer architecture.
This neural network architecture is at the core of current large language models (LLMs) such as ChatGPT, GPT-4, etc.

Here are my short notes to help me (and hopefully others) understand the underlying attention mechanism and the transformer neural network architecture.
I first look at the attention mechansim, then the transformer architecture, and finally share some pointers to other helpful resources.

{% include info.html text="The figures in this post are reproduced from the paper 'Attention is all you need' by Vaswani et al." %}

# The Attention Mechanism

## Self-Attention & Cross-Attention

## Multi-Head Attention

# The Transformer Architecture

## Encoder

## Decoder

# Helpful Pointers
