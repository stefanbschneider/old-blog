---
published: false
toc: true
comments: true
layout: post
title: Understanding Attention and Transformers
description: My notes for understanding the attention mechanism and transformer architecture used by GPT-4 and other LLMs.
image: 
categories: [attention, machine learning, llm]
---

I recently read again the famous paper "Attention is all you need" by XYZ, which presents the transformer architecture.
This neural network architecture is at the core of current large language models (LLMs) such as ChatGPT, GPT-4, etc.
Here are my short notes to help me (and hopefully others) understand the underlying attention mechanism and the transformer neural network architecture.

TODO
